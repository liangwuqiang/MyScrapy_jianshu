2019-10-29 17:28:09 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: MyScrapy_jianshu)
2019-10-29 17:28:09 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.17763-SP0
2019-10-29 17:28:09 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'MyScrapy_jianshu', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'MyScrapy_jianshu.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['MyScrapy_jianshu.spiders']}
2019-10-29 17:28:09 [scrapy.extensions.telnet] INFO: Telnet Password: aaef04f94e8e75f7
2019-10-29 17:28:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-29 17:28:09 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-29 17:28:09 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-29 17:28:09 [scrapy.middleware] INFO: Enabled item pipelines:
['MyScrapy_jianshu.pipelines.MyImagesPipeline',
 'MyScrapy_jianshu.pipelines.MyscrapyJianshuPipeline']
2019-10-29 17:28:09 [scrapy.core.engine] INFO: Spider opened
2019-10-29 17:28:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-29 17:28:09 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-29 17:28:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/robots.txt> (referer: None)
2019-10-29 17:28:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/p/a167aa9cb61c> (referer: None)
2019-10-29 17:28:11 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://upload-images.jianshu.io/robots.txt> (referer: None)
2019-10-29 17:28:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://upload-images.jianshu.io/upload_images/8516750-a79ef30e81b8de31.PNG> (referer: None)
2019-10-29 17:28:11 [scrapy.pipelines.files] DEBUG: File (downloaded): Downloaded file from <GET https://upload-images.jianshu.io/upload_images/8516750-a79ef30e81b8de31.PNG> referred in <None>
2019-10-29 17:28:11 [PIL.PngImagePlugin] DEBUG: STREAM b'IHDR' 16 13
2019-10-29 17:28:11 [PIL.PngImagePlugin] DEBUG: STREAM b'sRGB' 41 1
2019-10-29 17:28:11 [PIL.PngImagePlugin] DEBUG: STREAM b'gAMA' 54 4
2019-10-29 17:28:11 [PIL.PngImagePlugin] DEBUG: STREAM b'pHYs' 70 9
2019-10-29 17:28:11 [PIL.PngImagePlugin] DEBUG: STREAM b'IDAT' 91 6649
2019-10-29 17:28:11 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.jianshu.com/p/a167aa9cb61c>
{'image_urls': ['https://upload-images.jianshu.io/upload_images/8516750-a79ef30e81b8de31.PNG']}
2019-10-29 17:28:11 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-29 17:28:11 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1246,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 46809,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 3,
 'downloader/response_status_count/404': 1,
 'elapsed_time_seconds': 1.457896,
 'file_count': 1,
 'file_status_count/downloaded': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 29, 9, 28, 11, 426599),
 'item_scraped_count': 1,
 'log_count/DEBUG': 11,
 'log_count/INFO': 10,
 'response_received_count': 4,
 'robotstxt/request_count': 2,
 'robotstxt/response_count': 2,
 'robotstxt/response_status_count/200': 1,
 'robotstxt/response_status_count/404': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 10, 29, 9, 28, 9, 968703)}
2019-10-29 17:28:11 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-30 08:21:22 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: MyScrapy_jianshu)
2019-10-30 08:21:22 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.17763-SP0
2019-10-30 08:21:22 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'MyScrapy_jianshu', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'MyScrapy_jianshu.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['MyScrapy_jianshu.spiders']}
2019-10-30 08:21:22 [scrapy.extensions.telnet] INFO: Telnet Password: 3e1b406c921212c1
2019-10-30 08:21:22 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-30 08:21:23 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-30 08:21:23 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-30 08:21:23 [scrapy.middleware] INFO: Enabled item pipelines:
['MyScrapy_jianshu.pipelines.MyImagesPipeline',
 'MyScrapy_jianshu.pipelines.MyscrapyJianshuPipeline']
2019-10-30 08:21:23 [scrapy.core.engine] INFO: Spider opened
2019-10-30 08:21:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-30 08:21:23 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-30 08:21:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/robots.txt> (referer: None)
2019-10-30 08:21:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/p/a167aa9cb61c> (referer: None)
2019-10-30 08:21:34 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/8516750-a79ef30e81b8de31.PNG> referred in <None>
2019-10-30 08:21:34 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.jianshu.com/p/a167aa9cb61c>
{'image_urls': ['https://upload-images.jianshu.io/upload_images/8516750-a79ef30e81b8de31.PNG']}
2019-10-30 08:21:34 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-30 08:21:34 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 600,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 37915,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 11.71528,
 'file_count': 1,
 'file_status_count/uptodate': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 30, 0, 21, 34, 773887),
 'item_scraped_count': 1,
 'log_count/DEBUG': 4,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 10, 30, 0, 21, 23, 58607)}
2019-10-30 08:21:34 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-30 08:24:08 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: MyScrapy_jianshu)
2019-10-30 08:24:08 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.17763-SP0
2019-10-30 08:24:08 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'MyScrapy_jianshu', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'MyScrapy_jianshu.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['MyScrapy_jianshu.spiders']}
2019-10-30 08:24:08 [scrapy.extensions.telnet] INFO: Telnet Password: c0f1ed20cd6d25ac
2019-10-30 08:24:08 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-30 08:24:08 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-30 08:24:08 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-30 08:24:08 [scrapy.middleware] INFO: Enabled item pipelines:
['MyScrapy_jianshu.pipelines.MyImagesPipeline',
 'MyScrapy_jianshu.pipelines.MyscrapyJianshuPipeline']
2019-10-30 08:24:08 [scrapy.core.engine] INFO: Spider opened
2019-10-30 08:24:08 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-30 08:24:08 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-30 08:24:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/robots.txt> (referer: None)
2019-10-30 08:24:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/p/28fb20245e05> (referer: None)
2019-10-30 08:24:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.jianshu.com/p/28fb20245e05>
{'image_urls': []}
2019-10-30 08:24:20 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-30 08:24:20 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 600,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 35011,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 11.680432,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 30, 0, 24, 20, 536100),
 'item_scraped_count': 1,
 'log_count/DEBUG': 3,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 10, 30, 0, 24, 8, 855668)}
2019-10-30 08:24:20 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-30 08:36:10 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: MyScrapy_jianshu)
2019-10-30 08:36:10 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.17763-SP0
2019-10-30 08:36:10 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'MyScrapy_jianshu', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'MyScrapy_jianshu.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['MyScrapy_jianshu.spiders']}
2019-10-30 08:36:10 [scrapy.extensions.telnet] INFO: Telnet Password: a25231d42b8768fb
2019-10-30 08:36:10 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-30 08:36:10 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-30 08:36:10 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-30 08:36:10 [scrapy.middleware] INFO: Enabled item pipelines:
['MyScrapy_jianshu.pipelines.MyImagesPipeline',
 'MyScrapy_jianshu.pipelines.MyscrapyJianshuPipeline']
2019-10-30 08:36:10 [scrapy.core.engine] INFO: Spider opened
2019-10-30 08:36:10 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-30 08:36:10 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-30 08:36:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/robots.txt> (referer: None)
2019-10-30 08:36:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/p/33b1c5d0fbca> (referer: None)
2019-10-30 08:36:22 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/14462374-814903e825c89a31.png> referred in <None>
2019-10-30 08:36:22 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/14462374-802c0f257bdb15a0.png> referred in <None>
2019-10-30 08:36:22 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/14462374-6666c418e834eaca.png> referred in <None>
2019-10-30 08:36:22 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.jianshu.com/p/33b1c5d0fbca>
{'image_urls': ['https://upload-images.jianshu.io/upload_images/14462374-814903e825c89a31.png',
                'https://upload-images.jianshu.io/upload_images/14462374-802c0f257bdb15a0.png',
                'https://upload-images.jianshu.io/upload_images/14462374-6666c418e834eaca.png']}
2019-10-30 08:36:22 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-30 08:36:22 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 600,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 34143,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 11.651685,
 'file_count': 3,
 'file_status_count/uptodate': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 30, 0, 36, 22, 348294),
 'item_scraped_count': 1,
 'log_count/DEBUG': 6,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 10, 30, 0, 36, 10, 696609)}
2019-10-30 08:36:22 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-30 08:39:43 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: MyScrapy_jianshu)
2019-10-30 08:39:43 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.17763-SP0
2019-10-30 08:39:43 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'MyScrapy_jianshu', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'MyScrapy_jianshu.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['MyScrapy_jianshu.spiders']}
2019-10-30 08:39:43 [scrapy.extensions.telnet] INFO: Telnet Password: a87680af63edfe49
2019-10-30 08:39:43 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-30 08:39:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-30 08:39:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-30 08:39:44 [scrapy.middleware] INFO: Enabled item pipelines:
['MyScrapy_jianshu.pipelines.MyImagesPipeline',
 'MyScrapy_jianshu.pipelines.MyscrapyJianshuPipeline']
2019-10-30 08:39:44 [scrapy.core.engine] INFO: Spider opened
2019-10-30 08:39:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-30 08:39:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-30 08:40:05 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.jianshu.com/robots.txt> (failed 1 times): TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。.
2019-10-30 08:40:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/robots.txt> (referer: None)
2019-10-30 08:40:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/p/6bc5a4641629> (referer: None)
2019-10-30 08:40:14 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7> referred in <None>
2019-10-30 08:40:14 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2> referred in <None>
2019-10-30 08:40:14 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852> referred in <None>
2019-10-30 08:40:14 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2> referred in <None>
2019-10-30 08:40:14 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f> referred in <None>
2019-10-30 08:40:14 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602> referred in <None>
2019-10-30 08:40:14 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-23021209da207b71> referred in <None>
2019-10-30 08:40:14 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c> referred in <None>
2019-10-30 08:40:14 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c> referred in <None>
2019-10-30 08:40:14 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96> referred in <None>
2019-10-30 08:40:14 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93> referred in <None>
2019-10-30 08:40:14 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3> referred in <None>
2019-10-30 08:40:14 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198> referred in <None>
2019-10-30 08:40:14 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.jianshu.com/p/6bc5a4641629>
{'image_urls': ['https://upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7',
                'https://upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2',
                'https://upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852',
                'https://upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2',
                'https://upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f',
                'https://upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602',
                'https://upload-images.jianshu.io/upload_images/9489193-23021209da207b71',
                'https://upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c',
                'https://upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c',
                'https://upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96',
                'https://upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93',
                'https://upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3',
                'https://upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198']}
2019-10-30 08:40:14 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-30 08:40:14 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 1,
 'downloader/request_bytes': 898,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 39391,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 30.767599,
 'file_count': 13,
 'file_status_count/uptodate': 13,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 30, 0, 40, 14, 805640),
 'item_scraped_count': 1,
 'log_count/DEBUG': 17,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'retry/count': 1,
 'retry/reason_count/twisted.internet.error.TCPTimedOutError': 1,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 10, 30, 0, 39, 44, 38041)}
2019-10-30 08:40:14 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-30 09:27:05 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: MyScrapy_jianshu)
2019-10-30 09:27:05 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.17763-SP0
2019-10-30 09:27:05 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'MyScrapy_jianshu', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'MyScrapy_jianshu.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['MyScrapy_jianshu.spiders']}
2019-10-30 09:27:05 [scrapy.extensions.telnet] INFO: Telnet Password: 7259aca91359736d
2019-10-30 09:27:05 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-30 09:27:06 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-30 09:27:06 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-30 09:27:06 [scrapy.middleware] INFO: Enabled item pipelines:
['MyScrapy_jianshu.pipelines.MyImagesPipeline',
 'MyScrapy_jianshu.pipelines.MyscrapyJianshuPipeline']
2019-10-30 09:27:06 [scrapy.core.engine] INFO: Spider opened
2019-10-30 09:27:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-30 09:27:06 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-30 09:27:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/robots.txt> (referer: None)
2019-10-30 09:27:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/p/6bc5a4641629> (referer: None)
2019-10-30 09:27:06 [scrapy.core.scraper] ERROR: Error processing {'content': '<section class="ouvJEz"><h1 '
            'class="_1RuRku">爬虫框架Scrapy的安装与基本使用</h1><div class="rEsl9f"><div '
            'class="s-dsoj"><span class="_3tCVn5"><i aria-label="ic-diamond" '
            'class="anticon"><svg width="1em" height="1em" fill="currentColor" '
            'aria-hidden="true" focusable="false" class=""><use '
            'xlink:href="#ic-diamond"></use></svg></i><span>1</span></span><time '
            'datetime="2018-07-20T04:25:56.000Z">2018.07.20 '
            '12:25:56</time><span>字数 2216</span><span>阅读 '
            '38611</span></div></div><article '
            'class="_2rhmJa"><blockquote><p><b>概括：上一节学习了pyspider框架，这一节我们来看一下Scrapy的强大之处。</b><b>他应该是目前python使用的最广泛的爬虫框架。</b></p></blockquote><p><b>一、简单实例，了解基本。</b></p><p><b>1、安装Scrapy框架</b></p><p>这里如果直接pip3 '
            'install scrapy可能会出错。</p><p>所以你可以先安装lxml：pip3 install '
            'lxml(已安装请忽略)。</p><p>安装pyOpenSSL：在官网下载wheel文件。</p><p>安装Twisted：在官网下载wheel文件。</p><p>安装PyWin32：在官网下载wheel文件。</p><p>下载地址：https://www.lfd.uci.edu/~gohlke/pythonlibs/</p><p>配置环境变量：将scrapy所在目录添加到系统环境变量即可。</p><p>ctrl+f搜索即可。</p><p>最后安装scrapy，pip3 '
            'install '
            'scrapy</p><p><b>2、创建一个scrapy项目</b></p><p>新创建一个目录，按住shift-右键-在此处打开命令窗口</p><p>输入：scrapy '
            'startproject '
            'tutorial即可创建一个tutorial文件夹</p><p>文件夹目录如下：</p><blockquote>\n'
            '<p>|-tutorial</p>\n'
            '<p>|-scrapy.cfg</p>\n'
            '<p>\xa0 |-__init__.py</p>\n'
            '<p>\xa0 |-items.py</p>\n'
            '<p>\xa0 |-middlewares.py</p>\n'
            '<p>\xa0 |-pipelines.py</p>\n'
            '<p>\xa0 |-settings.py</p>\n'
            '<p>\xa0 |-spiders</p>\n'
            '<p>\xa0 \xa0 |-__init__.py</p>\n'
            '</blockquote><p><b>文件的功能：</b></p><p>scrapy.cfg：配置文件</p><p>spiders：存放你Spider文件，也就是你爬取的py文件</p><p>items.py：相当于一个容器，和字典较像</p><p>middlewares.py：定义Downloader '
            'Middlewares(下载器中间件)和Spider '
            'Middlewares(蜘蛛中间件)的实现</p><p>pipelines.py:定义Item '
            'Pipeline的实现，实现数据的清洗，储存，验证。</p><p>settings.py：全局配置</p><p><b>3、创建一个spider（自己定义的爬虫文件）</b></p><p>例如以爬取猫眼热映口碑榜为例子来了解一下：</p><p>在spiders文件夹下创建一个maoyan.py文件，你也可以按住shift-右键-在此处打开命令窗口，输入：scrapy '
            'genspider 文件名 '
            '要爬取的网址。</p><p>自己创建的需要自己写，使用命令创建的包含最基本的东西。</p><p>我们来看一下使用命令创建的有什么。</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 573px; max-height: '
            '264px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '46.07%;"></div>\n'
            '<div class="image-view" data-width="573" data-height="264"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7" '
            'data-original-width="573" data-original-height="264" '
            'data-original-format="image/png" '
            'data-original-filesize="10489"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>介绍一下这些是干嘛的：</p><p>name：是项目的名字</p><p>allowed_domains：是允许爬取的域名，比如一些网站有相关链接，域名就和本网站不同，这些就会忽略。</p><p>atart_urls：是Spider爬取的网站，定义初始的请求url，可以多个。</p><p>parse方法：是Spider的一个方法，在请求start_url后，之后的方法，这个方法是对网页的解析，与提取自己想要的东西。</p><p>response参数：是请求网页后返回的内容，也就是你需要解析的网页。</p><p>还有其他参数有兴趣可以去查查。</p><p><b>4、定义Item</b></p><p>item是保存爬取数据的容器，使用的方法和字典差不多。</p><p>我们打开items.py，之后我们想要提取的信息有：</p><p>index(排名)、title(电影名)、star(主演)、releasetime(上映时间)、score(评分)</p><p>于是我们将items.py文件修改成这样。</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '226px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '29.82%;"></div>\n'
            '<div class="image-view" data-width="758" data-height="226"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2" '
            'data-original-width="758" data-original-height="226" '
            'data-original-format="image/png" '
            'data-original-filesize="12301"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>即可。</p><p><b>5、再次打开spider来提取我们想要的信息</b></p><p>修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '399px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '36.94%;"></div>\n'
            '<div class="image-view" data-width="1080" data-height="399"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852" '
            'data-original-width="1080" data-original-height="399" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="53873"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>好了，一个简单的爬虫就写完了。</p><p><b>6、运行</b></p><p>在该文件夹下，按住shift-右键-在此处打开命令窗口，输入：scrapy '
            'crawl maoyan(项目的名字)</p><p>即可看到：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '580px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '53.7%;"></div>\n'
            '<div class="image-view" data-width="1080" data-height="580"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2" '
            'data-original-width="1080" data-original-height="580" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="107723"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p><b>7、保存</b></p><p>我们只运行了代码，看看有没有报错，并没有保存。</p><p>如果我们想保存为csv、xml、json格式，可以直接使用命令：</p><p>在该文件夹下，按住shift-右键-在此处打开命令窗口，输入：</p><p>scrapy '
            'crawl maoyan -o maoyan.csv</p><p>scrapy crawl maoyan -o '
            'maoyan.xml</p><p>scrapy crawl maoyan -o '
            "maoyan.json</p><p>选择其中一个即可。当然如果想要保存为其他格式也是可以的，这里只说常见的。这里选择json格式，运行后会发现，在文件夹下多出来一个maoyan.json的文件。打开之后发现，中文都是一串乱码，这里需要修改编码方式，当然也可以在配置里修改</p><p>（在settings.py文件中添加FEED_EXPORT_ENCODING='UTF8'即可），</p><p>如果想直接在命令行中修改：</p><p>scrapy "
            'crawl maoyan -o maoyan.json -s '
            'FEED_EXPORT_ENCODING=UTF8</p><p>即可。</p><p>这里自己试试效果吧。</p><p>当然我们保存也可以在运行的时候自动保存，不需要自己写命令。后面介绍（我们还有还多文件没有用到呦）。</p><p><b>二、scrapy如何解析？</b></p><p>之前写过一篇文章：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483766&amp;idx=1&amp;sn=86d61115ebb7a4083e17a54f1acffdf1&amp;chksm=fb74c947cc03405122d74b1172a9a96ef35af65753666f3f7900a9318290c4326736c2d6e69e&amp;scene=21#wechat_redirect" '
            'target="_blank" '
            'rel="nofollow">三大解析库的使用</a></p><p>但是scrapy也提供了自己的解析方式（Selector），和上面的也很相似，我们来看一下：</p><p><b>1、css</b></p><p>首先需要导入模块：from '
            'scrapy import '
            "Selector</p><p>例如有这样一段html代码：</p><p>html='&lt;html&gt;&lt;head&gt;&lt;title&gt;Demo&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;div "
            "class='cla'&gt;This is "
            'Demo&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</p><p><b>1.1、首先需要构建一个Selector对象</b></p><p>sel '
            '= Selector(html)</p><p>text = '
            "sel.css('.cla::text').extract_first()</p><p>.cla表示选中上面的div节点，::text表示获取文本，这里和以前的有所不同。</p><p>extract_first()表示返回第一个元素，因为上述 "
            'sel.css(\'.cla::text\')返回的是一个列表，你也可以写成sel.css(\'.cla::text\')[0]来获取第一个元素，但是如果为空，就会报出超出最大索引的错误，不建议这样写，而使用extract_first()就不会报错，同时如果写成extract_first(\'123\')这样，如果为空就返回123</p><p><b>1.2、</b>有了选取第一个，就有选取所有：extract()表示选取所有，如果返回的是多个值，就可以是这样写。</p><p><b>1.3、</b>获取属性就是sel.css(\'.cla::sttr(\'class\')\').extract_first()表示获取class</p><p><b>1.4、</b>获取指定属性的文本：sel.css(\'div[class="cla"]::text\')</p><p><b>1.5、</b>其他写法和css的写法如出一辙。</p><p><b>1.6、</b>在scrapy中为我们提供了一个简便的写法，在上述的简单实例中，我们知道了response为请求网页的返回值。</p><p>我们可以直接写成：response.css()来解析，提取我们想要的信息。同样，下面要说的XPath也可以直接写成：</p><p>response.xpath()来解析。</p><p><b>2、Xpath</b></p><p>Xpath的使用可以看上面的文章：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483766&amp;idx=1&amp;sn=86d61115ebb7a4083e17a54f1acffdf1&amp;chksm=fb74c947cc03405122d74b1172a9a96ef35af65753666f3f7900a9318290c4326736c2d6e69e&amp;scene=21#wechat_redirect" '
            'target="_blank" '
            'rel="nofollow">三大解析库的使用</a></p><p>注意：获取的还是列表，所以还是要加上extract_first()或者extract()</p><p><b>3、正则匹配(这里用response操作)</b></p><p>例如：response.css(\'a::text\').re(\'写正则\')</p><p>这里如果response.css(\'a::text\')匹配的是多个对象，那么加上正则也是匹配符合要求的多个对象。</p><p>这里如果想要匹配第一个对象，可以把re()修改成re_first()即可。</p><p><b>注意：response不可以直接调用re(),response.xpath(\'.\').re()可以相当于达到直接使用正则的效果</b>。</p><p>正则的使用：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483727&amp;idx=1&amp;sn=1fc19af2291177ac3c1a832c47392a3f&amp;chksm=fb74c97ecc0340681bc54c97ceb7bba413da3f1ca6fc3a107bbdabe7eb9be144c261a6de347d&amp;scene=21#wechat_redirect" '
            'target="_blank" rel="nofollow">万能的正则表达式</a></p><p><b>三、Dowmloader '
            'Middleware的使用</b></p><p>本身scrapy就提供了很多Dowmloader '
            "Middleware，但是有时候我们要修改，</p><p>比如修改User-Agent，使用代理ip等。</p><p>以修改User-Agent为例（设置代理ip大同小异）：</p><p>第一种方法，可以在settings.py中直接添加USER-AGENT='xxx'</p><p>但是我们想要添加多个User-Agent，每次随机获取一个可以利用Dowmloader "
            'Middleware来设置。</p><p>第一步将settings中的USER-AGENT=\'xxx\'修改成USER-AGENT=["xxx","xxxxx","xxxxxxx"]</p><p>第二步在middlewares.py中添加：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '287px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '27.02%;"></div>\n'
            '<div class="image-view" data-width="1062" data-height="287"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f" '
            'data-original-width="1062" data-original-height="287" '
            'data-original-format="image/png" '
            'data-original-filesize="19133"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>from_crawler():通过参数crawler可以拿到配置的信息，我们的User-Agent在配置文件里，所以我们需要获取到。</p><p>方法名不可以修改。</p><p>第三步在settings.py中添加：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '110px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '12.04%;"></div>\n'
            '<div class="image-view" data-width="914" data-height="110"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602" '
            'data-original-width="914" data-original-height="110" '
            'data-original-format="image/png" '
            'data-original-filesize="5150"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>将scrapy自带的UserAgentmiddleware的键值设置为None,</p><p>自定义的设置为400，这个键值越小表示优先调用的意思。</p><p><b>四、Item '
            'Pipeline的使用。</b></p><p><b>1、进行数据的清洗</b></p><p>在一的实例中我们把评分小于等于8.5分的score修改为（不好看！），我们认为是不好看的电影，我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 678px; max-height: '
            '306px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '45.129999999999995%;"></div>\n'
            '<div class="image-view" data-width="678" data-height="306"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-23021209da207b71" '
            'data-original-width="678" data-original-height="306" '
            'data-original-format="image/png" '
            'data-original-filesize="7034"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 580px; max-height: '
            '78px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '13.450000000000001%;"></div>\n'
            '<div class="image-view" data-width="580" data-height="78"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c" '
            'data-original-width="580" data-original-height="78" '
            'data-original-format="image/png" '
            'data-original-filesize="5871"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>我们执行一下：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '460px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '44.7%;"></div>\n'
            '<div class="image-view" data-width="1029" data-height="460"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c" '
            'data-original-width="1029" data-original-height="460" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="91210"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p><b>2、储存</b></p><p><b>2.1储存为json格式</b></p><p>我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '564px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '65.58%;"></div>\n'
            '<div class="image-view" data-width="860" data-height="564"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96" '
            'data-original-width="860" data-original-height="564" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="76827"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 616px; max-height: '
            '100px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '16.23%;"></div>\n'
            '<div class="image-view" data-width="616" data-height="100"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93" '
            'data-original-width="616" data-original-height="100" '
            'data-original-format="image/png" '
            'data-original-filesize="7382"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>表示先执行TextPipeline方法，再执行JsonPipeline方法，先清洗，再储存。</p><p><b>2.2储存在mysql数据库</b></p><p>首先在mysql数据库中创建一个数据库maoyanreying，创建一个表maoyan。</p><p>我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '634px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '62.46000000000001%;"></div>\n'
            '<div class="image-view" data-width="1015" data-height="634"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3" '
            'data-original-width="1015" data-original-height="634" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="102252"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 566px; max-height: '
            '218px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '38.519999999999996%;"></div>\n'
            '<div class="image-view" data-width="566" data-height="218"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198" '
            'data-original-width="566" data-original-height="218" '
            'data-original-format="image/png" '
            'data-original-filesize="5142"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>即可</p><p>完。</p></article><div></div><div class="_19DgIp" '
            'style="margin-top:24px;margin-bottom:24px"></div></section>',
 'image_srcs': ['//upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7',
                '//upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2',
                '//upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852',
                '//upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2',
                '//upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f',
                '//upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602',
                '//upload-images.jianshu.io/upload_images/9489193-23021209da207b71',
                '//upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c',
                '//upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c',
                '//upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96',
                '//upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93',
                '//upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3',
                '//upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198'],
 'image_urls': ['https://upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7',
                'https://upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2',
                'https://upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852',
                'https://upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2',
                'https://upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f',
                'https://upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602',
                'https://upload-images.jianshu.io/upload_images/9489193-23021209da207b71',
                'https://upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c',
                'https://upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c',
                'https://upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96',
                'https://upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93',
                'https://upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3',
                'https://upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198'],
 'title': '爬虫框架Scrapy的安装与基本使用',
 'url': 'https://www.jianshu.com/p/6bc5a4641629'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\PycharmProjects\MyScrapy_jianshu\MyScrapy_jianshu\pipelines.py", line 37, in process_item
    with codecs.open(filename, 'w', 'utf-8') as f:
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\codecs.py", line 898, in open
    file = builtins.open(filename, mode, buffering)
PermissionError: [Errno 13] Permission denied: 'outputs/爬虫框架Scrapy的安装与基本使用.html'
2019-10-30 09:27:06 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-30 09:27:06 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 600,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 39363,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.589072,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 30, 1, 27, 6, 692479),
 'log_count/DEBUG': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 10, 30, 1, 27, 6, 103407)}
2019-10-30 09:27:06 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-30 09:28:56 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: MyScrapy_jianshu)
2019-10-30 09:28:56 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.17763-SP0
2019-10-30 09:28:56 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'MyScrapy_jianshu', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'MyScrapy_jianshu.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['MyScrapy_jianshu.spiders']}
2019-10-30 09:28:56 [scrapy.extensions.telnet] INFO: Telnet Password: 55e234333f8e5950
2019-10-30 09:28:56 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-30 09:28:56 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-30 09:28:56 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-30 09:28:56 [scrapy.middleware] INFO: Enabled item pipelines:
['MyScrapy_jianshu.pipelines.MyImagesPipeline',
 'MyScrapy_jianshu.pipelines.MyscrapyJianshuPipeline']
2019-10-30 09:28:56 [scrapy.core.engine] INFO: Spider opened
2019-10-30 09:28:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-30 09:28:56 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-30 09:28:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/robots.txt> (referer: None)
2019-10-30 09:28:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/p/6bc5a4641629> (referer: None)
2019-10-30 09:28:57 [scrapy.core.scraper] ERROR: Error processing {'content': '<section class="ouvJEz"><h1 '
            'class="_1RuRku">爬虫框架Scrapy的安装与基本使用</h1><div class="rEsl9f"><div '
            'class="s-dsoj"><span class="_3tCVn5"><i aria-label="ic-diamond" '
            'class="anticon"><svg width="1em" height="1em" fill="currentColor" '
            'aria-hidden="true" focusable="false" class=""><use '
            'xlink:href="#ic-diamond"></use></svg></i><span>1</span></span><time '
            'datetime="2018-07-20T04:25:56.000Z">2018.07.20 '
            '12:25:56</time><span>字数 2216</span><span>阅读 '
            '38612</span></div></div><article '
            'class="_2rhmJa"><blockquote><p><b>概括：上一节学习了pyspider框架，这一节我们来看一下Scrapy的强大之处。</b><b>他应该是目前python使用的最广泛的爬虫框架。</b></p></blockquote><p><b>一、简单实例，了解基本。</b></p><p><b>1、安装Scrapy框架</b></p><p>这里如果直接pip3 '
            'install scrapy可能会出错。</p><p>所以你可以先安装lxml：pip3 install '
            'lxml(已安装请忽略)。</p><p>安装pyOpenSSL：在官网下载wheel文件。</p><p>安装Twisted：在官网下载wheel文件。</p><p>安装PyWin32：在官网下载wheel文件。</p><p>下载地址：https://www.lfd.uci.edu/~gohlke/pythonlibs/</p><p>配置环境变量：将scrapy所在目录添加到系统环境变量即可。</p><p>ctrl+f搜索即可。</p><p>最后安装scrapy，pip3 '
            'install '
            'scrapy</p><p><b>2、创建一个scrapy项目</b></p><p>新创建一个目录，按住shift-右键-在此处打开命令窗口</p><p>输入：scrapy '
            'startproject '
            'tutorial即可创建一个tutorial文件夹</p><p>文件夹目录如下：</p><blockquote>\n'
            '<p>|-tutorial</p>\n'
            '<p>|-scrapy.cfg</p>\n'
            '<p>\xa0 |-__init__.py</p>\n'
            '<p>\xa0 |-items.py</p>\n'
            '<p>\xa0 |-middlewares.py</p>\n'
            '<p>\xa0 |-pipelines.py</p>\n'
            '<p>\xa0 |-settings.py</p>\n'
            '<p>\xa0 |-spiders</p>\n'
            '<p>\xa0 \xa0 |-__init__.py</p>\n'
            '</blockquote><p><b>文件的功能：</b></p><p>scrapy.cfg：配置文件</p><p>spiders：存放你Spider文件，也就是你爬取的py文件</p><p>items.py：相当于一个容器，和字典较像</p><p>middlewares.py：定义Downloader '
            'Middlewares(下载器中间件)和Spider '
            'Middlewares(蜘蛛中间件)的实现</p><p>pipelines.py:定义Item '
            'Pipeline的实现，实现数据的清洗，储存，验证。</p><p>settings.py：全局配置</p><p><b>3、创建一个spider（自己定义的爬虫文件）</b></p><p>例如以爬取猫眼热映口碑榜为例子来了解一下：</p><p>在spiders文件夹下创建一个maoyan.py文件，你也可以按住shift-右键-在此处打开命令窗口，输入：scrapy '
            'genspider 文件名 '
            '要爬取的网址。</p><p>自己创建的需要自己写，使用命令创建的包含最基本的东西。</p><p>我们来看一下使用命令创建的有什么。</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 573px; max-height: '
            '264px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '46.07%;"></div>\n'
            '<div class="image-view" data-width="573" data-height="264"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7" '
            'data-original-width="573" data-original-height="264" '
            'data-original-format="image/png" '
            'data-original-filesize="10489"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>介绍一下这些是干嘛的：</p><p>name：是项目的名字</p><p>allowed_domains：是允许爬取的域名，比如一些网站有相关链接，域名就和本网站不同，这些就会忽略。</p><p>atart_urls：是Spider爬取的网站，定义初始的请求url，可以多个。</p><p>parse方法：是Spider的一个方法，在请求start_url后，之后的方法，这个方法是对网页的解析，与提取自己想要的东西。</p><p>response参数：是请求网页后返回的内容，也就是你需要解析的网页。</p><p>还有其他参数有兴趣可以去查查。</p><p><b>4、定义Item</b></p><p>item是保存爬取数据的容器，使用的方法和字典差不多。</p><p>我们打开items.py，之后我们想要提取的信息有：</p><p>index(排名)、title(电影名)、star(主演)、releasetime(上映时间)、score(评分)</p><p>于是我们将items.py文件修改成这样。</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '226px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '29.82%;"></div>\n'
            '<div class="image-view" data-width="758" data-height="226"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2" '
            'data-original-width="758" data-original-height="226" '
            'data-original-format="image/png" '
            'data-original-filesize="12301"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>即可。</p><p><b>5、再次打开spider来提取我们想要的信息</b></p><p>修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '399px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '36.94%;"></div>\n'
            '<div class="image-view" data-width="1080" data-height="399"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852" '
            'data-original-width="1080" data-original-height="399" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="53873"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>好了，一个简单的爬虫就写完了。</p><p><b>6、运行</b></p><p>在该文件夹下，按住shift-右键-在此处打开命令窗口，输入：scrapy '
            'crawl maoyan(项目的名字)</p><p>即可看到：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '580px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '53.7%;"></div>\n'
            '<div class="image-view" data-width="1080" data-height="580"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2" '
            'data-original-width="1080" data-original-height="580" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="107723"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p><b>7、保存</b></p><p>我们只运行了代码，看看有没有报错，并没有保存。</p><p>如果我们想保存为csv、xml、json格式，可以直接使用命令：</p><p>在该文件夹下，按住shift-右键-在此处打开命令窗口，输入：</p><p>scrapy '
            'crawl maoyan -o maoyan.csv</p><p>scrapy crawl maoyan -o '
            'maoyan.xml</p><p>scrapy crawl maoyan -o '
            "maoyan.json</p><p>选择其中一个即可。当然如果想要保存为其他格式也是可以的，这里只说常见的。这里选择json格式，运行后会发现，在文件夹下多出来一个maoyan.json的文件。打开之后发现，中文都是一串乱码，这里需要修改编码方式，当然也可以在配置里修改</p><p>（在settings.py文件中添加FEED_EXPORT_ENCODING='UTF8'即可），</p><p>如果想直接在命令行中修改：</p><p>scrapy "
            'crawl maoyan -o maoyan.json -s '
            'FEED_EXPORT_ENCODING=UTF8</p><p>即可。</p><p>这里自己试试效果吧。</p><p>当然我们保存也可以在运行的时候自动保存，不需要自己写命令。后面介绍（我们还有还多文件没有用到呦）。</p><p><b>二、scrapy如何解析？</b></p><p>之前写过一篇文章：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483766&amp;idx=1&amp;sn=86d61115ebb7a4083e17a54f1acffdf1&amp;chksm=fb74c947cc03405122d74b1172a9a96ef35af65753666f3f7900a9318290c4326736c2d6e69e&amp;scene=21#wechat_redirect" '
            'target="_blank" '
            'rel="nofollow">三大解析库的使用</a></p><p>但是scrapy也提供了自己的解析方式（Selector），和上面的也很相似，我们来看一下：</p><p><b>1、css</b></p><p>首先需要导入模块：from '
            'scrapy import '
            "Selector</p><p>例如有这样一段html代码：</p><p>html='&lt;html&gt;&lt;head&gt;&lt;title&gt;Demo&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;div "
            "class='cla'&gt;This is "
            'Demo&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</p><p><b>1.1、首先需要构建一个Selector对象</b></p><p>sel '
            '= Selector(html)</p><p>text = '
            "sel.css('.cla::text').extract_first()</p><p>.cla表示选中上面的div节点，::text表示获取文本，这里和以前的有所不同。</p><p>extract_first()表示返回第一个元素，因为上述 "
            'sel.css(\'.cla::text\')返回的是一个列表，你也可以写成sel.css(\'.cla::text\')[0]来获取第一个元素，但是如果为空，就会报出超出最大索引的错误，不建议这样写，而使用extract_first()就不会报错，同时如果写成extract_first(\'123\')这样，如果为空就返回123</p><p><b>1.2、</b>有了选取第一个，就有选取所有：extract()表示选取所有，如果返回的是多个值，就可以是这样写。</p><p><b>1.3、</b>获取属性就是sel.css(\'.cla::sttr(\'class\')\').extract_first()表示获取class</p><p><b>1.4、</b>获取指定属性的文本：sel.css(\'div[class="cla"]::text\')</p><p><b>1.5、</b>其他写法和css的写法如出一辙。</p><p><b>1.6、</b>在scrapy中为我们提供了一个简便的写法，在上述的简单实例中，我们知道了response为请求网页的返回值。</p><p>我们可以直接写成：response.css()来解析，提取我们想要的信息。同样，下面要说的XPath也可以直接写成：</p><p>response.xpath()来解析。</p><p><b>2、Xpath</b></p><p>Xpath的使用可以看上面的文章：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483766&amp;idx=1&amp;sn=86d61115ebb7a4083e17a54f1acffdf1&amp;chksm=fb74c947cc03405122d74b1172a9a96ef35af65753666f3f7900a9318290c4326736c2d6e69e&amp;scene=21#wechat_redirect" '
            'target="_blank" '
            'rel="nofollow">三大解析库的使用</a></p><p>注意：获取的还是列表，所以还是要加上extract_first()或者extract()</p><p><b>3、正则匹配(这里用response操作)</b></p><p>例如：response.css(\'a::text\').re(\'写正则\')</p><p>这里如果response.css(\'a::text\')匹配的是多个对象，那么加上正则也是匹配符合要求的多个对象。</p><p>这里如果想要匹配第一个对象，可以把re()修改成re_first()即可。</p><p><b>注意：response不可以直接调用re(),response.xpath(\'.\').re()可以相当于达到直接使用正则的效果</b>。</p><p>正则的使用：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483727&amp;idx=1&amp;sn=1fc19af2291177ac3c1a832c47392a3f&amp;chksm=fb74c97ecc0340681bc54c97ceb7bba413da3f1ca6fc3a107bbdabe7eb9be144c261a6de347d&amp;scene=21#wechat_redirect" '
            'target="_blank" rel="nofollow">万能的正则表达式</a></p><p><b>三、Dowmloader '
            'Middleware的使用</b></p><p>本身scrapy就提供了很多Dowmloader '
            "Middleware，但是有时候我们要修改，</p><p>比如修改User-Agent，使用代理ip等。</p><p>以修改User-Agent为例（设置代理ip大同小异）：</p><p>第一种方法，可以在settings.py中直接添加USER-AGENT='xxx'</p><p>但是我们想要添加多个User-Agent，每次随机获取一个可以利用Dowmloader "
            'Middleware来设置。</p><p>第一步将settings中的USER-AGENT=\'xxx\'修改成USER-AGENT=["xxx","xxxxx","xxxxxxx"]</p><p>第二步在middlewares.py中添加：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '287px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '27.02%;"></div>\n'
            '<div class="image-view" data-width="1062" data-height="287"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f" '
            'data-original-width="1062" data-original-height="287" '
            'data-original-format="image/png" '
            'data-original-filesize="19133"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>from_crawler():通过参数crawler可以拿到配置的信息，我们的User-Agent在配置文件里，所以我们需要获取到。</p><p>方法名不可以修改。</p><p>第三步在settings.py中添加：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '110px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '12.04%;"></div>\n'
            '<div class="image-view" data-width="914" data-height="110"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602" '
            'data-original-width="914" data-original-height="110" '
            'data-original-format="image/png" '
            'data-original-filesize="5150"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>将scrapy自带的UserAgentmiddleware的键值设置为None,</p><p>自定义的设置为400，这个键值越小表示优先调用的意思。</p><p><b>四、Item '
            'Pipeline的使用。</b></p><p><b>1、进行数据的清洗</b></p><p>在一的实例中我们把评分小于等于8.5分的score修改为（不好看！），我们认为是不好看的电影，我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 678px; max-height: '
            '306px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '45.129999999999995%;"></div>\n'
            '<div class="image-view" data-width="678" data-height="306"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-23021209da207b71" '
            'data-original-width="678" data-original-height="306" '
            'data-original-format="image/png" '
            'data-original-filesize="7034"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 580px; max-height: '
            '78px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '13.450000000000001%;"></div>\n'
            '<div class="image-view" data-width="580" data-height="78"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c" '
            'data-original-width="580" data-original-height="78" '
            'data-original-format="image/png" '
            'data-original-filesize="5871"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>我们执行一下：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '460px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '44.7%;"></div>\n'
            '<div class="image-view" data-width="1029" data-height="460"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c" '
            'data-original-width="1029" data-original-height="460" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="91210"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p><b>2、储存</b></p><p><b>2.1储存为json格式</b></p><p>我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '564px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '65.58%;"></div>\n'
            '<div class="image-view" data-width="860" data-height="564"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96" '
            'data-original-width="860" data-original-height="564" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="76827"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 616px; max-height: '
            '100px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '16.23%;"></div>\n'
            '<div class="image-view" data-width="616" data-height="100"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93" '
            'data-original-width="616" data-original-height="100" '
            'data-original-format="image/png" '
            'data-original-filesize="7382"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>表示先执行TextPipeline方法，再执行JsonPipeline方法，先清洗，再储存。</p><p><b>2.2储存在mysql数据库</b></p><p>首先在mysql数据库中创建一个数据库maoyanreying，创建一个表maoyan。</p><p>我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '634px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '62.46000000000001%;"></div>\n'
            '<div class="image-view" data-width="1015" data-height="634"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3" '
            'data-original-width="1015" data-original-height="634" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="102252"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 566px; max-height: '
            '218px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '38.519999999999996%;"></div>\n'
            '<div class="image-view" data-width="566" data-height="218"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198" '
            'data-original-width="566" data-original-height="218" '
            'data-original-format="image/png" '
            'data-original-filesize="5142"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>即可</p><p>完。</p></article><div></div><div class="_19DgIp" '
            'style="margin-top:24px;margin-bottom:24px"></div></section>',
 'image_srcs': ['//upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7',
                '//upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2',
                '//upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852',
                '//upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2',
                '//upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f',
                '//upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602',
                '//upload-images.jianshu.io/upload_images/9489193-23021209da207b71',
                '//upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c',
                '//upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c',
                '//upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96',
                '//upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93',
                '//upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3',
                '//upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198'],
 'image_urls': ['https://upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7',
                'https://upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2',
                'https://upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852',
                'https://upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2',
                'https://upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f',
                'https://upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602',
                'https://upload-images.jianshu.io/upload_images/9489193-23021209da207b71',
                'https://upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c',
                'https://upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c',
                'https://upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96',
                'https://upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93',
                'https://upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3',
                'https://upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198'],
 'title': '爬虫框架Scrapy的安装与基本使用',
 'url': 'https://www.jianshu.com/p/6bc5a4641629'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\PycharmProjects\MyScrapy_jianshu\MyScrapy_jianshu\pipelines.py", line 37, in process_item
    with codecs.open(filename, 'w', 'utf-8') as f:
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\codecs.py", line 898, in open
    file = builtins.open(filename, mode, buffering)
PermissionError: [Errno 13] Permission denied: 'outputs/爬虫框架Scrapy的安装与基本使用.html'
2019-10-30 09:28:57 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-30 09:28:57 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 600,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 39367,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.74166,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 30, 1, 28, 57, 232880),
 'log_count/DEBUG': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 10, 30, 1, 28, 56, 491220)}
2019-10-30 09:28:57 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-30 09:40:12 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: MyScrapy_jianshu)
2019-10-30 09:40:12 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.17763-SP0
2019-10-30 09:40:12 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'MyScrapy_jianshu', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'MyScrapy_jianshu.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['MyScrapy_jianshu.spiders']}
2019-10-30 09:40:12 [scrapy.extensions.telnet] INFO: Telnet Password: 58ff7d2fc217ddd0
2019-10-30 09:40:12 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-30 09:40:12 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-30 09:40:12 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-30 09:40:12 [scrapy.middleware] INFO: Enabled item pipelines:
['MyScrapy_jianshu.pipelines.MyImagesPipeline',
 'MyScrapy_jianshu.pipelines.MyscrapyJianshuPipeline']
2019-10-30 09:40:12 [scrapy.core.engine] INFO: Spider opened
2019-10-30 09:40:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-30 09:40:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-30 09:40:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/robots.txt> (referer: None)
2019-10-30 09:40:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/p/6bc5a4641629> (referer: None)
2019-10-30 09:40:13 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7> referred in <None>
2019-10-30 09:40:13 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2> referred in <None>
2019-10-30 09:40:13 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852> referred in <None>
2019-10-30 09:40:13 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2> referred in <None>
2019-10-30 09:40:13 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f> referred in <None>
2019-10-30 09:40:13 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602> referred in <None>
2019-10-30 09:40:13 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-23021209da207b71> referred in <None>
2019-10-30 09:40:13 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c> referred in <None>
2019-10-30 09:40:13 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c> referred in <None>
2019-10-30 09:40:13 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96> referred in <None>
2019-10-30 09:40:13 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93> referred in <None>
2019-10-30 09:40:13 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3> referred in <None>
2019-10-30 09:40:13 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198> referred in <None>
2019-10-30 09:40:13 [scrapy.core.scraper] ERROR: Error processing {'content': '<section class="ouvJEz"><h1 '
            'class="_1RuRku">爬虫框架Scrapy的安装与基本使用</h1><div class="rEsl9f"><div '
            'class="s-dsoj"><span class="_3tCVn5"><i aria-label="ic-diamond" '
            'class="anticon"><svg width="1em" height="1em" fill="currentColor" '
            'aria-hidden="true" focusable="false" class=""><use '
            'xlink:href="#ic-diamond"></use></svg></i><span>1</span></span><time '
            'datetime="2018-07-20T04:25:56.000Z">2018.07.20 '
            '12:25:56</time><span>字数 2216</span><span>阅读 '
            '38618</span></div></div><article '
            'class="_2rhmJa"><blockquote><p><b>概括：上一节学习了pyspider框架，这一节我们来看一下Scrapy的强大之处。</b><b>他应该是目前python使用的最广泛的爬虫框架。</b></p></blockquote><p><b>一、简单实例，了解基本。</b></p><p><b>1、安装Scrapy框架</b></p><p>这里如果直接pip3 '
            'install scrapy可能会出错。</p><p>所以你可以先安装lxml：pip3 install '
            'lxml(已安装请忽略)。</p><p>安装pyOpenSSL：在官网下载wheel文件。</p><p>安装Twisted：在官网下载wheel文件。</p><p>安装PyWin32：在官网下载wheel文件。</p><p>下载地址：https://www.lfd.uci.edu/~gohlke/pythonlibs/</p><p>配置环境变量：将scrapy所在目录添加到系统环境变量即可。</p><p>ctrl+f搜索即可。</p><p>最后安装scrapy，pip3 '
            'install '
            'scrapy</p><p><b>2、创建一个scrapy项目</b></p><p>新创建一个目录，按住shift-右键-在此处打开命令窗口</p><p>输入：scrapy '
            'startproject '
            'tutorial即可创建一个tutorial文件夹</p><p>文件夹目录如下：</p><blockquote>\n'
            '<p>|-tutorial</p>\n'
            '<p>|-scrapy.cfg</p>\n'
            '<p>\xa0 |-__init__.py</p>\n'
            '<p>\xa0 |-items.py</p>\n'
            '<p>\xa0 |-middlewares.py</p>\n'
            '<p>\xa0 |-pipelines.py</p>\n'
            '<p>\xa0 |-settings.py</p>\n'
            '<p>\xa0 |-spiders</p>\n'
            '<p>\xa0 \xa0 |-__init__.py</p>\n'
            '</blockquote><p><b>文件的功能：</b></p><p>scrapy.cfg：配置文件</p><p>spiders：存放你Spider文件，也就是你爬取的py文件</p><p>items.py：相当于一个容器，和字典较像</p><p>middlewares.py：定义Downloader '
            'Middlewares(下载器中间件)和Spider '
            'Middlewares(蜘蛛中间件)的实现</p><p>pipelines.py:定义Item '
            'Pipeline的实现，实现数据的清洗，储存，验证。</p><p>settings.py：全局配置</p><p><b>3、创建一个spider（自己定义的爬虫文件）</b></p><p>例如以爬取猫眼热映口碑榜为例子来了解一下：</p><p>在spiders文件夹下创建一个maoyan.py文件，你也可以按住shift-右键-在此处打开命令窗口，输入：scrapy '
            'genspider 文件名 '
            '要爬取的网址。</p><p>自己创建的需要自己写，使用命令创建的包含最基本的东西。</p><p>我们来看一下使用命令创建的有什么。</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 573px; max-height: '
            '264px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '46.07%;"></div>\n'
            '<div class="image-view" data-width="573" data-height="264"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7" '
            'data-original-width="573" data-original-height="264" '
            'data-original-format="image/png" '
            'data-original-filesize="10489"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>介绍一下这些是干嘛的：</p><p>name：是项目的名字</p><p>allowed_domains：是允许爬取的域名，比如一些网站有相关链接，域名就和本网站不同，这些就会忽略。</p><p>atart_urls：是Spider爬取的网站，定义初始的请求url，可以多个。</p><p>parse方法：是Spider的一个方法，在请求start_url后，之后的方法，这个方法是对网页的解析，与提取自己想要的东西。</p><p>response参数：是请求网页后返回的内容，也就是你需要解析的网页。</p><p>还有其他参数有兴趣可以去查查。</p><p><b>4、定义Item</b></p><p>item是保存爬取数据的容器，使用的方法和字典差不多。</p><p>我们打开items.py，之后我们想要提取的信息有：</p><p>index(排名)、title(电影名)、star(主演)、releasetime(上映时间)、score(评分)</p><p>于是我们将items.py文件修改成这样。</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '226px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '29.82%;"></div>\n'
            '<div class="image-view" data-width="758" data-height="226"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2" '
            'data-original-width="758" data-original-height="226" '
            'data-original-format="image/png" '
            'data-original-filesize="12301"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>即可。</p><p><b>5、再次打开spider来提取我们想要的信息</b></p><p>修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '399px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '36.94%;"></div>\n'
            '<div class="image-view" data-width="1080" data-height="399"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852" '
            'data-original-width="1080" data-original-height="399" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="53873"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>好了，一个简单的爬虫就写完了。</p><p><b>6、运行</b></p><p>在该文件夹下，按住shift-右键-在此处打开命令窗口，输入：scrapy '
            'crawl maoyan(项目的名字)</p><p>即可看到：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '580px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '53.7%;"></div>\n'
            '<div class="image-view" data-width="1080" data-height="580"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2" '
            'data-original-width="1080" data-original-height="580" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="107723"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p><b>7、保存</b></p><p>我们只运行了代码，看看有没有报错，并没有保存。</p><p>如果我们想保存为csv、xml、json格式，可以直接使用命令：</p><p>在该文件夹下，按住shift-右键-在此处打开命令窗口，输入：</p><p>scrapy '
            'crawl maoyan -o maoyan.csv</p><p>scrapy crawl maoyan -o '
            'maoyan.xml</p><p>scrapy crawl maoyan -o '
            "maoyan.json</p><p>选择其中一个即可。当然如果想要保存为其他格式也是可以的，这里只说常见的。这里选择json格式，运行后会发现，在文件夹下多出来一个maoyan.json的文件。打开之后发现，中文都是一串乱码，这里需要修改编码方式，当然也可以在配置里修改</p><p>（在settings.py文件中添加FEED_EXPORT_ENCODING='UTF8'即可），</p><p>如果想直接在命令行中修改：</p><p>scrapy "
            'crawl maoyan -o maoyan.json -s '
            'FEED_EXPORT_ENCODING=UTF8</p><p>即可。</p><p>这里自己试试效果吧。</p><p>当然我们保存也可以在运行的时候自动保存，不需要自己写命令。后面介绍（我们还有还多文件没有用到呦）。</p><p><b>二、scrapy如何解析？</b></p><p>之前写过一篇文章：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483766&amp;idx=1&amp;sn=86d61115ebb7a4083e17a54f1acffdf1&amp;chksm=fb74c947cc03405122d74b1172a9a96ef35af65753666f3f7900a9318290c4326736c2d6e69e&amp;scene=21#wechat_redirect" '
            'target="_blank" '
            'rel="nofollow">三大解析库的使用</a></p><p>但是scrapy也提供了自己的解析方式（Selector），和上面的也很相似，我们来看一下：</p><p><b>1、css</b></p><p>首先需要导入模块：from '
            'scrapy import '
            "Selector</p><p>例如有这样一段html代码：</p><p>html='&lt;html&gt;&lt;head&gt;&lt;title&gt;Demo&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;div "
            "class='cla'&gt;This is "
            'Demo&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</p><p><b>1.1、首先需要构建一个Selector对象</b></p><p>sel '
            '= Selector(html)</p><p>text = '
            "sel.css('.cla::text').extract_first()</p><p>.cla表示选中上面的div节点，::text表示获取文本，这里和以前的有所不同。</p><p>extract_first()表示返回第一个元素，因为上述 "
            'sel.css(\'.cla::text\')返回的是一个列表，你也可以写成sel.css(\'.cla::text\')[0]来获取第一个元素，但是如果为空，就会报出超出最大索引的错误，不建议这样写，而使用extract_first()就不会报错，同时如果写成extract_first(\'123\')这样，如果为空就返回123</p><p><b>1.2、</b>有了选取第一个，就有选取所有：extract()表示选取所有，如果返回的是多个值，就可以是这样写。</p><p><b>1.3、</b>获取属性就是sel.css(\'.cla::sttr(\'class\')\').extract_first()表示获取class</p><p><b>1.4、</b>获取指定属性的文本：sel.css(\'div[class="cla"]::text\')</p><p><b>1.5、</b>其他写法和css的写法如出一辙。</p><p><b>1.6、</b>在scrapy中为我们提供了一个简便的写法，在上述的简单实例中，我们知道了response为请求网页的返回值。</p><p>我们可以直接写成：response.css()来解析，提取我们想要的信息。同样，下面要说的XPath也可以直接写成：</p><p>response.xpath()来解析。</p><p><b>2、Xpath</b></p><p>Xpath的使用可以看上面的文章：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483766&amp;idx=1&amp;sn=86d61115ebb7a4083e17a54f1acffdf1&amp;chksm=fb74c947cc03405122d74b1172a9a96ef35af65753666f3f7900a9318290c4326736c2d6e69e&amp;scene=21#wechat_redirect" '
            'target="_blank" '
            'rel="nofollow">三大解析库的使用</a></p><p>注意：获取的还是列表，所以还是要加上extract_first()或者extract()</p><p><b>3、正则匹配(这里用response操作)</b></p><p>例如：response.css(\'a::text\').re(\'写正则\')</p><p>这里如果response.css(\'a::text\')匹配的是多个对象，那么加上正则也是匹配符合要求的多个对象。</p><p>这里如果想要匹配第一个对象，可以把re()修改成re_first()即可。</p><p><b>注意：response不可以直接调用re(),response.xpath(\'.\').re()可以相当于达到直接使用正则的效果</b>。</p><p>正则的使用：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483727&amp;idx=1&amp;sn=1fc19af2291177ac3c1a832c47392a3f&amp;chksm=fb74c97ecc0340681bc54c97ceb7bba413da3f1ca6fc3a107bbdabe7eb9be144c261a6de347d&amp;scene=21#wechat_redirect" '
            'target="_blank" rel="nofollow">万能的正则表达式</a></p><p><b>三、Dowmloader '
            'Middleware的使用</b></p><p>本身scrapy就提供了很多Dowmloader '
            "Middleware，但是有时候我们要修改，</p><p>比如修改User-Agent，使用代理ip等。</p><p>以修改User-Agent为例（设置代理ip大同小异）：</p><p>第一种方法，可以在settings.py中直接添加USER-AGENT='xxx'</p><p>但是我们想要添加多个User-Agent，每次随机获取一个可以利用Dowmloader "
            'Middleware来设置。</p><p>第一步将settings中的USER-AGENT=\'xxx\'修改成USER-AGENT=["xxx","xxxxx","xxxxxxx"]</p><p>第二步在middlewares.py中添加：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '287px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '27.02%;"></div>\n'
            '<div class="image-view" data-width="1062" data-height="287"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f" '
            'data-original-width="1062" data-original-height="287" '
            'data-original-format="image/png" '
            'data-original-filesize="19133"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>from_crawler():通过参数crawler可以拿到配置的信息，我们的User-Agent在配置文件里，所以我们需要获取到。</p><p>方法名不可以修改。</p><p>第三步在settings.py中添加：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '110px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '12.04%;"></div>\n'
            '<div class="image-view" data-width="914" data-height="110"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602" '
            'data-original-width="914" data-original-height="110" '
            'data-original-format="image/png" '
            'data-original-filesize="5150"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>将scrapy自带的UserAgentmiddleware的键值设置为None,</p><p>自定义的设置为400，这个键值越小表示优先调用的意思。</p><p><b>四、Item '
            'Pipeline的使用。</b></p><p><b>1、进行数据的清洗</b></p><p>在一的实例中我们把评分小于等于8.5分的score修改为（不好看！），我们认为是不好看的电影，我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 678px; max-height: '
            '306px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '45.129999999999995%;"></div>\n'
            '<div class="image-view" data-width="678" data-height="306"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-23021209da207b71" '
            'data-original-width="678" data-original-height="306" '
            'data-original-format="image/png" '
            'data-original-filesize="7034"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 580px; max-height: '
            '78px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '13.450000000000001%;"></div>\n'
            '<div class="image-view" data-width="580" data-height="78"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c" '
            'data-original-width="580" data-original-height="78" '
            'data-original-format="image/png" '
            'data-original-filesize="5871"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>我们执行一下：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '460px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '44.7%;"></div>\n'
            '<div class="image-view" data-width="1029" data-height="460"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c" '
            'data-original-width="1029" data-original-height="460" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="91210"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p><b>2、储存</b></p><p><b>2.1储存为json格式</b></p><p>我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '564px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '65.58%;"></div>\n'
            '<div class="image-view" data-width="860" data-height="564"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96" '
            'data-original-width="860" data-original-height="564" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="76827"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 616px; max-height: '
            '100px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '16.23%;"></div>\n'
            '<div class="image-view" data-width="616" data-height="100"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93" '
            'data-original-width="616" data-original-height="100" '
            'data-original-format="image/png" '
            'data-original-filesize="7382"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>表示先执行TextPipeline方法，再执行JsonPipeline方法，先清洗，再储存。</p><p><b>2.2储存在mysql数据库</b></p><p>首先在mysql数据库中创建一个数据库maoyanreying，创建一个表maoyan。</p><p>我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '634px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '62.46000000000001%;"></div>\n'
            '<div class="image-view" data-width="1015" data-height="634"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3" '
            'data-original-width="1015" data-original-height="634" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="102252"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 566px; max-height: '
            '218px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '38.519999999999996%;"></div>\n'
            '<div class="image-view" data-width="566" data-height="218"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198" '
            'data-original-width="566" data-original-height="218" '
            'data-original-format="image/png" '
            'data-original-filesize="5142"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>即可</p><p>完。</p></article><div></div><div class="_19DgIp" '
            'style="margin-top:24px;margin-bottom:24px"></div></section>',
 'image_srcs': ['//upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7',
                '//upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2',
                '//upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852',
                '//upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2',
                '//upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f',
                '//upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602',
                '//upload-images.jianshu.io/upload_images/9489193-23021209da207b71',
                '//upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c',
                '//upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c',
                '//upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96',
                '//upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93',
                '//upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3',
                '//upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198'],
 'image_urls': ['https://upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7',
                'https://upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2',
                'https://upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852',
                'https://upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2',
                'https://upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f',
                'https://upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602',
                'https://upload-images.jianshu.io/upload_images/9489193-23021209da207b71',
                'https://upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c',
                'https://upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c',
                'https://upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96',
                'https://upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93',
                'https://upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3',
                'https://upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198'],
 'title': '爬虫框架Scrapy的安装与基本使用',
 'url': 'https://www.jianshu.com/p/6bc5a4641629'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\PycharmProjects\MyScrapy_jianshu\MyScrapy_jianshu\pipelines.py", line 36, in process_item
    with codecs.open(filename, 'w', 'utf-8') as f:
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\codecs.py", line 898, in open
    file = builtins.open(filename, mode, buffering)
PermissionError: [Errno 13] Permission denied: 'outputs/爬虫框架Scrapy的安装与基本使用.html'
2019-10-30 09:40:13 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-30 09:40:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 600,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 39390,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.753324,
 'file_count': 13,
 'file_status_count/uptodate': 13,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 30, 1, 40, 13, 571794),
 'log_count/DEBUG': 15,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 10, 30, 1, 40, 12, 818470)}
2019-10-30 09:40:13 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-30 10:52:28 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: MyScrapy_jianshu)
2019-10-30 10:52:28 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.17763-SP0
2019-10-30 10:52:28 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'MyScrapy_jianshu', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'MyScrapy_jianshu.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['MyScrapy_jianshu.spiders']}
2019-10-30 10:52:28 [scrapy.extensions.telnet] INFO: Telnet Password: e0afec666ac88de4
2019-10-30 10:52:28 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-30 10:52:28 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-30 10:52:28 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-30 10:52:28 [scrapy.middleware] INFO: Enabled item pipelines:
['MyScrapy_jianshu.pipelines.MyscrapyJianshuPipeline',
 'MyScrapy_jianshu.pipelines.MyImagesPipeline']
2019-10-30 10:52:28 [scrapy.core.engine] INFO: Spider opened
2019-10-30 10:52:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-30 10:52:28 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-30 10:52:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/robots.txt> (referer: None)
2019-10-30 10:52:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/p/6bc5a4641629> (referer: None)
2019-10-30 10:52:29 [scrapy.core.scraper] ERROR: Error processing {'content': '<section class="ouvJEz"><h1 '
            'class="_1RuRku">爬虫框架Scrapy的安装与基本使用</h1><div class="rEsl9f"><div '
            'class="s-dsoj"><span class="_3tCVn5"><i aria-label="ic-diamond" '
            'class="anticon"><svg width="1em" height="1em" fill="currentColor" '
            'aria-hidden="true" focusable="false" class=""><use '
            'xlink:href="#ic-diamond"></use></svg></i><span>1</span></span><time '
            'datetime="2018-07-20T04:25:56.000Z">2018.07.20 '
            '12:25:56</time><span>字数 2216</span><span>阅读 '
            '38650</span></div></div><article '
            'class="_2rhmJa"><blockquote><p><b>概括：上一节学习了pyspider框架，这一节我们来看一下Scrapy的强大之处。</b><b>他应该是目前python使用的最广泛的爬虫框架。</b></p></blockquote><p><b>一、简单实例，了解基本。</b></p><p><b>1、安装Scrapy框架</b></p><p>这里如果直接pip3 '
            'install scrapy可能会出错。</p><p>所以你可以先安装lxml：pip3 install '
            'lxml(已安装请忽略)。</p><p>安装pyOpenSSL：在官网下载wheel文件。</p><p>安装Twisted：在官网下载wheel文件。</p><p>安装PyWin32：在官网下载wheel文件。</p><p>下载地址：https://www.lfd.uci.edu/~gohlke/pythonlibs/</p><p>配置环境变量：将scrapy所在目录添加到系统环境变量即可。</p><p>ctrl+f搜索即可。</p><p>最后安装scrapy，pip3 '
            'install '
            'scrapy</p><p><b>2、创建一个scrapy项目</b></p><p>新创建一个目录，按住shift-右键-在此处打开命令窗口</p><p>输入：scrapy '
            'startproject '
            'tutorial即可创建一个tutorial文件夹</p><p>文件夹目录如下：</p><blockquote>\n'
            '<p>|-tutorial</p>\n'
            '<p>|-scrapy.cfg</p>\n'
            '<p>\xa0 |-__init__.py</p>\n'
            '<p>\xa0 |-items.py</p>\n'
            '<p>\xa0 |-middlewares.py</p>\n'
            '<p>\xa0 |-pipelines.py</p>\n'
            '<p>\xa0 |-settings.py</p>\n'
            '<p>\xa0 |-spiders</p>\n'
            '<p>\xa0 \xa0 |-__init__.py</p>\n'
            '</blockquote><p><b>文件的功能：</b></p><p>scrapy.cfg：配置文件</p><p>spiders：存放你Spider文件，也就是你爬取的py文件</p><p>items.py：相当于一个容器，和字典较像</p><p>middlewares.py：定义Downloader '
            'Middlewares(下载器中间件)和Spider '
            'Middlewares(蜘蛛中间件)的实现</p><p>pipelines.py:定义Item '
            'Pipeline的实现，实现数据的清洗，储存，验证。</p><p>settings.py：全局配置</p><p><b>3、创建一个spider（自己定义的爬虫文件）</b></p><p>例如以爬取猫眼热映口碑榜为例子来了解一下：</p><p>在spiders文件夹下创建一个maoyan.py文件，你也可以按住shift-右键-在此处打开命令窗口，输入：scrapy '
            'genspider 文件名 '
            '要爬取的网址。</p><p>自己创建的需要自己写，使用命令创建的包含最基本的东西。</p><p>我们来看一下使用命令创建的有什么。</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 573px; max-height: '
            '264px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '46.07%;"></div>\n'
            '<div class="image-view" data-width="573" data-height="264"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7" '
            'data-original-width="573" data-original-height="264" '
            'data-original-format="image/png" '
            'data-original-filesize="10489"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>介绍一下这些是干嘛的：</p><p>name：是项目的名字</p><p>allowed_domains：是允许爬取的域名，比如一些网站有相关链接，域名就和本网站不同，这些就会忽略。</p><p>atart_urls：是Spider爬取的网站，定义初始的请求url，可以多个。</p><p>parse方法：是Spider的一个方法，在请求start_url后，之后的方法，这个方法是对网页的解析，与提取自己想要的东西。</p><p>response参数：是请求网页后返回的内容，也就是你需要解析的网页。</p><p>还有其他参数有兴趣可以去查查。</p><p><b>4、定义Item</b></p><p>item是保存爬取数据的容器，使用的方法和字典差不多。</p><p>我们打开items.py，之后我们想要提取的信息有：</p><p>index(排名)、title(电影名)、star(主演)、releasetime(上映时间)、score(评分)</p><p>于是我们将items.py文件修改成这样。</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '226px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '29.82%;"></div>\n'
            '<div class="image-view" data-width="758" data-height="226"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2" '
            'data-original-width="758" data-original-height="226" '
            'data-original-format="image/png" '
            'data-original-filesize="12301"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>即可。</p><p><b>5、再次打开spider来提取我们想要的信息</b></p><p>修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '399px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '36.94%;"></div>\n'
            '<div class="image-view" data-width="1080" data-height="399"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852" '
            'data-original-width="1080" data-original-height="399" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="53873"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>好了，一个简单的爬虫就写完了。</p><p><b>6、运行</b></p><p>在该文件夹下，按住shift-右键-在此处打开命令窗口，输入：scrapy '
            'crawl maoyan(项目的名字)</p><p>即可看到：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '580px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '53.7%;"></div>\n'
            '<div class="image-view" data-width="1080" data-height="580"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2" '
            'data-original-width="1080" data-original-height="580" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="107723"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p><b>7、保存</b></p><p>我们只运行了代码，看看有没有报错，并没有保存。</p><p>如果我们想保存为csv、xml、json格式，可以直接使用命令：</p><p>在该文件夹下，按住shift-右键-在此处打开命令窗口，输入：</p><p>scrapy '
            'crawl maoyan -o maoyan.csv</p><p>scrapy crawl maoyan -o '
            'maoyan.xml</p><p>scrapy crawl maoyan -o '
            "maoyan.json</p><p>选择其中一个即可。当然如果想要保存为其他格式也是可以的，这里只说常见的。这里选择json格式，运行后会发现，在文件夹下多出来一个maoyan.json的文件。打开之后发现，中文都是一串乱码，这里需要修改编码方式，当然也可以在配置里修改</p><p>（在settings.py文件中添加FEED_EXPORT_ENCODING='UTF8'即可），</p><p>如果想直接在命令行中修改：</p><p>scrapy "
            'crawl maoyan -o maoyan.json -s '
            'FEED_EXPORT_ENCODING=UTF8</p><p>即可。</p><p>这里自己试试效果吧。</p><p>当然我们保存也可以在运行的时候自动保存，不需要自己写命令。后面介绍（我们还有还多文件没有用到呦）。</p><p><b>二、scrapy如何解析？</b></p><p>之前写过一篇文章：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483766&amp;idx=1&amp;sn=86d61115ebb7a4083e17a54f1acffdf1&amp;chksm=fb74c947cc03405122d74b1172a9a96ef35af65753666f3f7900a9318290c4326736c2d6e69e&amp;scene=21#wechat_redirect" '
            'target="_blank" '
            'rel="nofollow">三大解析库的使用</a></p><p>但是scrapy也提供了自己的解析方式（Selector），和上面的也很相似，我们来看一下：</p><p><b>1、css</b></p><p>首先需要导入模块：from '
            'scrapy import '
            "Selector</p><p>例如有这样一段html代码：</p><p>html='&lt;html&gt;&lt;head&gt;&lt;title&gt;Demo&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;div "
            "class='cla'&gt;This is "
            'Demo&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</p><p><b>1.1、首先需要构建一个Selector对象</b></p><p>sel '
            '= Selector(html)</p><p>text = '
            "sel.css('.cla::text').extract_first()</p><p>.cla表示选中上面的div节点，::text表示获取文本，这里和以前的有所不同。</p><p>extract_first()表示返回第一个元素，因为上述 "
            'sel.css(\'.cla::text\')返回的是一个列表，你也可以写成sel.css(\'.cla::text\')[0]来获取第一个元素，但是如果为空，就会报出超出最大索引的错误，不建议这样写，而使用extract_first()就不会报错，同时如果写成extract_first(\'123\')这样，如果为空就返回123</p><p><b>1.2、</b>有了选取第一个，就有选取所有：extract()表示选取所有，如果返回的是多个值，就可以是这样写。</p><p><b>1.3、</b>获取属性就是sel.css(\'.cla::sttr(\'class\')\').extract_first()表示获取class</p><p><b>1.4、</b>获取指定属性的文本：sel.css(\'div[class="cla"]::text\')</p><p><b>1.5、</b>其他写法和css的写法如出一辙。</p><p><b>1.6、</b>在scrapy中为我们提供了一个简便的写法，在上述的简单实例中，我们知道了response为请求网页的返回值。</p><p>我们可以直接写成：response.css()来解析，提取我们想要的信息。同样，下面要说的XPath也可以直接写成：</p><p>response.xpath()来解析。</p><p><b>2、Xpath</b></p><p>Xpath的使用可以看上面的文章：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483766&amp;idx=1&amp;sn=86d61115ebb7a4083e17a54f1acffdf1&amp;chksm=fb74c947cc03405122d74b1172a9a96ef35af65753666f3f7900a9318290c4326736c2d6e69e&amp;scene=21#wechat_redirect" '
            'target="_blank" '
            'rel="nofollow">三大解析库的使用</a></p><p>注意：获取的还是列表，所以还是要加上extract_first()或者extract()</p><p><b>3、正则匹配(这里用response操作)</b></p><p>例如：response.css(\'a::text\').re(\'写正则\')</p><p>这里如果response.css(\'a::text\')匹配的是多个对象，那么加上正则也是匹配符合要求的多个对象。</p><p>这里如果想要匹配第一个对象，可以把re()修改成re_first()即可。</p><p><b>注意：response不可以直接调用re(),response.xpath(\'.\').re()可以相当于达到直接使用正则的效果</b>。</p><p>正则的使用：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483727&amp;idx=1&amp;sn=1fc19af2291177ac3c1a832c47392a3f&amp;chksm=fb74c97ecc0340681bc54c97ceb7bba413da3f1ca6fc3a107bbdabe7eb9be144c261a6de347d&amp;scene=21#wechat_redirect" '
            'target="_blank" rel="nofollow">万能的正则表达式</a></p><p><b>三、Dowmloader '
            'Middleware的使用</b></p><p>本身scrapy就提供了很多Dowmloader '
            "Middleware，但是有时候我们要修改，</p><p>比如修改User-Agent，使用代理ip等。</p><p>以修改User-Agent为例（设置代理ip大同小异）：</p><p>第一种方法，可以在settings.py中直接添加USER-AGENT='xxx'</p><p>但是我们想要添加多个User-Agent，每次随机获取一个可以利用Dowmloader "
            'Middleware来设置。</p><p>第一步将settings中的USER-AGENT=\'xxx\'修改成USER-AGENT=["xxx","xxxxx","xxxxxxx"]</p><p>第二步在middlewares.py中添加：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '287px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '27.02%;"></div>\n'
            '<div class="image-view" data-width="1062" data-height="287"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f" '
            'data-original-width="1062" data-original-height="287" '
            'data-original-format="image/png" '
            'data-original-filesize="19133"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>from_crawler():通过参数crawler可以拿到配置的信息，我们的User-Agent在配置文件里，所以我们需要获取到。</p><p>方法名不可以修改。</p><p>第三步在settings.py中添加：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '110px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '12.04%;"></div>\n'
            '<div class="image-view" data-width="914" data-height="110"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602" '
            'data-original-width="914" data-original-height="110" '
            'data-original-format="image/png" '
            'data-original-filesize="5150"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>将scrapy自带的UserAgentmiddleware的键值设置为None,</p><p>自定义的设置为400，这个键值越小表示优先调用的意思。</p><p><b>四、Item '
            'Pipeline的使用。</b></p><p><b>1、进行数据的清洗</b></p><p>在一的实例中我们把评分小于等于8.5分的score修改为（不好看！），我们认为是不好看的电影，我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 678px; max-height: '
            '306px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '45.129999999999995%;"></div>\n'
            '<div class="image-view" data-width="678" data-height="306"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-23021209da207b71" '
            'data-original-width="678" data-original-height="306" '
            'data-original-format="image/png" '
            'data-original-filesize="7034"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 580px; max-height: '
            '78px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '13.450000000000001%;"></div>\n'
            '<div class="image-view" data-width="580" data-height="78"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c" '
            'data-original-width="580" data-original-height="78" '
            'data-original-format="image/png" '
            'data-original-filesize="5871"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>我们执行一下：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '460px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '44.7%;"></div>\n'
            '<div class="image-view" data-width="1029" data-height="460"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c" '
            'data-original-width="1029" data-original-height="460" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="91210"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p><b>2、储存</b></p><p><b>2.1储存为json格式</b></p><p>我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '564px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '65.58%;"></div>\n'
            '<div class="image-view" data-width="860" data-height="564"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96" '
            'data-original-width="860" data-original-height="564" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="76827"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 616px; max-height: '
            '100px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '16.23%;"></div>\n'
            '<div class="image-view" data-width="616" data-height="100"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93" '
            'data-original-width="616" data-original-height="100" '
            'data-original-format="image/png" '
            'data-original-filesize="7382"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>表示先执行TextPipeline方法，再执行JsonPipeline方法，先清洗，再储存。</p><p><b>2.2储存在mysql数据库</b></p><p>首先在mysql数据库中创建一个数据库maoyanreying，创建一个表maoyan。</p><p>我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '634px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '62.46000000000001%;"></div>\n'
            '<div class="image-view" data-width="1015" data-height="634"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3" '
            'data-original-width="1015" data-original-height="634" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="102252"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 566px; max-height: '
            '218px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '38.519999999999996%;"></div>\n'
            '<div class="image-view" data-width="566" data-height="218"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198" '
            'data-original-width="566" data-original-height="218" '
            'data-original-format="image/png" '
            'data-original-filesize="5142"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>即可</p><p>完。</p></article><div></div><div class="_19DgIp" '
            'style="margin-top:24px;margin-bottom:24px"></div></section>',
 'image_srcs': ['//upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7',
                '//upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2',
                '//upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852',
                '//upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2',
                '//upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f',
                '//upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602',
                '//upload-images.jianshu.io/upload_images/9489193-23021209da207b71',
                '//upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c',
                '//upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c',
                '//upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96',
                '//upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93',
                '//upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3',
                '//upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198'],
 'image_urls': ['https://upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7',
                'https://upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2',
                'https://upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852',
                'https://upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2',
                'https://upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f',
                'https://upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602',
                'https://upload-images.jianshu.io/upload_images/9489193-23021209da207b71',
                'https://upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c',
                'https://upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c',
                'https://upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96',
                'https://upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93',
                'https://upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3',
                'https://upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198'],
 'title': '爬虫框架Scrapy的安装与基本使用',
 'url': 'https://www.jianshu.com/p/6bc5a4641629'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\PycharmProjects\MyScrapy_jianshu\MyScrapy_jianshu\pipelines.py", line 37, in process_item
    with codecs.open(filename, 'w', 'utf-8') as f:
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\codecs.py", line 898, in open
    file = builtins.open(filename, mode, buffering)
PermissionError: [Errno 13] Permission denied: 'outputs/爬虫框架Scrapy的安装与基本使用.html'
2019-10-30 10:52:29 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-30 10:52:29 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 600,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 39386,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.728611,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 30, 2, 52, 29, 135953),
 'log_count/DEBUG': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 10, 30, 2, 52, 28, 407342)}
2019-10-30 10:52:29 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-30 10:53:35 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: MyScrapy_jianshu)
2019-10-30 10:53:35 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.17763-SP0
2019-10-30 10:53:35 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'MyScrapy_jianshu', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'MyScrapy_jianshu.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['MyScrapy_jianshu.spiders']}
2019-10-30 10:53:35 [scrapy.extensions.telnet] INFO: Telnet Password: 34eeac320167ec3d
2019-10-30 10:53:35 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-30 10:53:35 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-30 10:53:35 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-30 10:53:35 [scrapy.middleware] INFO: Enabled item pipelines:
['MyScrapy_jianshu.pipelines.MyscrapyJianshuPipeline',
 'MyScrapy_jianshu.pipelines.MyImagesPipeline']
2019-10-30 10:53:35 [scrapy.core.engine] INFO: Spider opened
2019-10-30 10:53:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-30 10:53:35 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-30 10:53:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/robots.txt> (referer: None)
2019-10-30 10:53:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/p/6bc5a4641629> (referer: None)
2019-10-30 10:53:36 [scrapy.core.scraper] ERROR: Error processing {'content': '<section class="ouvJEz"><h1 '
            'class="_1RuRku">爬虫框架Scrapy的安装与基本使用</h1><div class="rEsl9f"><div '
            'class="s-dsoj"><span class="_3tCVn5"><i aria-label="ic-diamond" '
            'class="anticon"><svg width="1em" height="1em" fill="currentColor" '
            'aria-hidden="true" focusable="false" class=""><use '
            'xlink:href="#ic-diamond"></use></svg></i><span>1</span></span><time '
            'datetime="2018-07-20T04:25:56.000Z">2018.07.20 '
            '12:25:56</time><span>字数 2216</span><span>阅读 '
            '38653</span></div></div><article '
            'class="_2rhmJa"><blockquote><p><b>概括：上一节学习了pyspider框架，这一节我们来看一下Scrapy的强大之处。</b><b>他应该是目前python使用的最广泛的爬虫框架。</b></p></blockquote><p><b>一、简单实例，了解基本。</b></p><p><b>1、安装Scrapy框架</b></p><p>这里如果直接pip3 '
            'install scrapy可能会出错。</p><p>所以你可以先安装lxml：pip3 install '
            'lxml(已安装请忽略)。</p><p>安装pyOpenSSL：在官网下载wheel文件。</p><p>安装Twisted：在官网下载wheel文件。</p><p>安装PyWin32：在官网下载wheel文件。</p><p>下载地址：https://www.lfd.uci.edu/~gohlke/pythonlibs/</p><p>配置环境变量：将scrapy所在目录添加到系统环境变量即可。</p><p>ctrl+f搜索即可。</p><p>最后安装scrapy，pip3 '
            'install '
            'scrapy</p><p><b>2、创建一个scrapy项目</b></p><p>新创建一个目录，按住shift-右键-在此处打开命令窗口</p><p>输入：scrapy '
            'startproject '
            'tutorial即可创建一个tutorial文件夹</p><p>文件夹目录如下：</p><blockquote>\n'
            '<p>|-tutorial</p>\n'
            '<p>|-scrapy.cfg</p>\n'
            '<p>\xa0 |-__init__.py</p>\n'
            '<p>\xa0 |-items.py</p>\n'
            '<p>\xa0 |-middlewares.py</p>\n'
            '<p>\xa0 |-pipelines.py</p>\n'
            '<p>\xa0 |-settings.py</p>\n'
            '<p>\xa0 |-spiders</p>\n'
            '<p>\xa0 \xa0 |-__init__.py</p>\n'
            '</blockquote><p><b>文件的功能：</b></p><p>scrapy.cfg：配置文件</p><p>spiders：存放你Spider文件，也就是你爬取的py文件</p><p>items.py：相当于一个容器，和字典较像</p><p>middlewares.py：定义Downloader '
            'Middlewares(下载器中间件)和Spider '
            'Middlewares(蜘蛛中间件)的实现</p><p>pipelines.py:定义Item '
            'Pipeline的实现，实现数据的清洗，储存，验证。</p><p>settings.py：全局配置</p><p><b>3、创建一个spider（自己定义的爬虫文件）</b></p><p>例如以爬取猫眼热映口碑榜为例子来了解一下：</p><p>在spiders文件夹下创建一个maoyan.py文件，你也可以按住shift-右键-在此处打开命令窗口，输入：scrapy '
            'genspider 文件名 '
            '要爬取的网址。</p><p>自己创建的需要自己写，使用命令创建的包含最基本的东西。</p><p>我们来看一下使用命令创建的有什么。</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 573px; max-height: '
            '264px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '46.07%;"></div>\n'
            '<div class="image-view" data-width="573" data-height="264"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7" '
            'data-original-width="573" data-original-height="264" '
            'data-original-format="image/png" '
            'data-original-filesize="10489"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>介绍一下这些是干嘛的：</p><p>name：是项目的名字</p><p>allowed_domains：是允许爬取的域名，比如一些网站有相关链接，域名就和本网站不同，这些就会忽略。</p><p>atart_urls：是Spider爬取的网站，定义初始的请求url，可以多个。</p><p>parse方法：是Spider的一个方法，在请求start_url后，之后的方法，这个方法是对网页的解析，与提取自己想要的东西。</p><p>response参数：是请求网页后返回的内容，也就是你需要解析的网页。</p><p>还有其他参数有兴趣可以去查查。</p><p><b>4、定义Item</b></p><p>item是保存爬取数据的容器，使用的方法和字典差不多。</p><p>我们打开items.py，之后我们想要提取的信息有：</p><p>index(排名)、title(电影名)、star(主演)、releasetime(上映时间)、score(评分)</p><p>于是我们将items.py文件修改成这样。</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '226px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '29.82%;"></div>\n'
            '<div class="image-view" data-width="758" data-height="226"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2" '
            'data-original-width="758" data-original-height="226" '
            'data-original-format="image/png" '
            'data-original-filesize="12301"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>即可。</p><p><b>5、再次打开spider来提取我们想要的信息</b></p><p>修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '399px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '36.94%;"></div>\n'
            '<div class="image-view" data-width="1080" data-height="399"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852" '
            'data-original-width="1080" data-original-height="399" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="53873"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>好了，一个简单的爬虫就写完了。</p><p><b>6、运行</b></p><p>在该文件夹下，按住shift-右键-在此处打开命令窗口，输入：scrapy '
            'crawl maoyan(项目的名字)</p><p>即可看到：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '580px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '53.7%;"></div>\n'
            '<div class="image-view" data-width="1080" data-height="580"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2" '
            'data-original-width="1080" data-original-height="580" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="107723"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p><b>7、保存</b></p><p>我们只运行了代码，看看有没有报错，并没有保存。</p><p>如果我们想保存为csv、xml、json格式，可以直接使用命令：</p><p>在该文件夹下，按住shift-右键-在此处打开命令窗口，输入：</p><p>scrapy '
            'crawl maoyan -o maoyan.csv</p><p>scrapy crawl maoyan -o '
            'maoyan.xml</p><p>scrapy crawl maoyan -o '
            "maoyan.json</p><p>选择其中一个即可。当然如果想要保存为其他格式也是可以的，这里只说常见的。这里选择json格式，运行后会发现，在文件夹下多出来一个maoyan.json的文件。打开之后发现，中文都是一串乱码，这里需要修改编码方式，当然也可以在配置里修改</p><p>（在settings.py文件中添加FEED_EXPORT_ENCODING='UTF8'即可），</p><p>如果想直接在命令行中修改：</p><p>scrapy "
            'crawl maoyan -o maoyan.json -s '
            'FEED_EXPORT_ENCODING=UTF8</p><p>即可。</p><p>这里自己试试效果吧。</p><p>当然我们保存也可以在运行的时候自动保存，不需要自己写命令。后面介绍（我们还有还多文件没有用到呦）。</p><p><b>二、scrapy如何解析？</b></p><p>之前写过一篇文章：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483766&amp;idx=1&amp;sn=86d61115ebb7a4083e17a54f1acffdf1&amp;chksm=fb74c947cc03405122d74b1172a9a96ef35af65753666f3f7900a9318290c4326736c2d6e69e&amp;scene=21#wechat_redirect" '
            'target="_blank" '
            'rel="nofollow">三大解析库的使用</a></p><p>但是scrapy也提供了自己的解析方式（Selector），和上面的也很相似，我们来看一下：</p><p><b>1、css</b></p><p>首先需要导入模块：from '
            'scrapy import '
            "Selector</p><p>例如有这样一段html代码：</p><p>html='&lt;html&gt;&lt;head&gt;&lt;title&gt;Demo&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;div "
            "class='cla'&gt;This is "
            'Demo&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</p><p><b>1.1、首先需要构建一个Selector对象</b></p><p>sel '
            '= Selector(html)</p><p>text = '
            "sel.css('.cla::text').extract_first()</p><p>.cla表示选中上面的div节点，::text表示获取文本，这里和以前的有所不同。</p><p>extract_first()表示返回第一个元素，因为上述 "
            'sel.css(\'.cla::text\')返回的是一个列表，你也可以写成sel.css(\'.cla::text\')[0]来获取第一个元素，但是如果为空，就会报出超出最大索引的错误，不建议这样写，而使用extract_first()就不会报错，同时如果写成extract_first(\'123\')这样，如果为空就返回123</p><p><b>1.2、</b>有了选取第一个，就有选取所有：extract()表示选取所有，如果返回的是多个值，就可以是这样写。</p><p><b>1.3、</b>获取属性就是sel.css(\'.cla::sttr(\'class\')\').extract_first()表示获取class</p><p><b>1.4、</b>获取指定属性的文本：sel.css(\'div[class="cla"]::text\')</p><p><b>1.5、</b>其他写法和css的写法如出一辙。</p><p><b>1.6、</b>在scrapy中为我们提供了一个简便的写法，在上述的简单实例中，我们知道了response为请求网页的返回值。</p><p>我们可以直接写成：response.css()来解析，提取我们想要的信息。同样，下面要说的XPath也可以直接写成：</p><p>response.xpath()来解析。</p><p><b>2、Xpath</b></p><p>Xpath的使用可以看上面的文章：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483766&amp;idx=1&amp;sn=86d61115ebb7a4083e17a54f1acffdf1&amp;chksm=fb74c947cc03405122d74b1172a9a96ef35af65753666f3f7900a9318290c4326736c2d6e69e&amp;scene=21#wechat_redirect" '
            'target="_blank" '
            'rel="nofollow">三大解析库的使用</a></p><p>注意：获取的还是列表，所以还是要加上extract_first()或者extract()</p><p><b>3、正则匹配(这里用response操作)</b></p><p>例如：response.css(\'a::text\').re(\'写正则\')</p><p>这里如果response.css(\'a::text\')匹配的是多个对象，那么加上正则也是匹配符合要求的多个对象。</p><p>这里如果想要匹配第一个对象，可以把re()修改成re_first()即可。</p><p><b>注意：response不可以直接调用re(),response.xpath(\'.\').re()可以相当于达到直接使用正则的效果</b>。</p><p>正则的使用：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483727&amp;idx=1&amp;sn=1fc19af2291177ac3c1a832c47392a3f&amp;chksm=fb74c97ecc0340681bc54c97ceb7bba413da3f1ca6fc3a107bbdabe7eb9be144c261a6de347d&amp;scene=21#wechat_redirect" '
            'target="_blank" rel="nofollow">万能的正则表达式</a></p><p><b>三、Dowmloader '
            'Middleware的使用</b></p><p>本身scrapy就提供了很多Dowmloader '
            "Middleware，但是有时候我们要修改，</p><p>比如修改User-Agent，使用代理ip等。</p><p>以修改User-Agent为例（设置代理ip大同小异）：</p><p>第一种方法，可以在settings.py中直接添加USER-AGENT='xxx'</p><p>但是我们想要添加多个User-Agent，每次随机获取一个可以利用Dowmloader "
            'Middleware来设置。</p><p>第一步将settings中的USER-AGENT=\'xxx\'修改成USER-AGENT=["xxx","xxxxx","xxxxxxx"]</p><p>第二步在middlewares.py中添加：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '287px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '27.02%;"></div>\n'
            '<div class="image-view" data-width="1062" data-height="287"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f" '
            'data-original-width="1062" data-original-height="287" '
            'data-original-format="image/png" '
            'data-original-filesize="19133"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>from_crawler():通过参数crawler可以拿到配置的信息，我们的User-Agent在配置文件里，所以我们需要获取到。</p><p>方法名不可以修改。</p><p>第三步在settings.py中添加：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '110px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '12.04%;"></div>\n'
            '<div class="image-view" data-width="914" data-height="110"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602" '
            'data-original-width="914" data-original-height="110" '
            'data-original-format="image/png" '
            'data-original-filesize="5150"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>将scrapy自带的UserAgentmiddleware的键值设置为None,</p><p>自定义的设置为400，这个键值越小表示优先调用的意思。</p><p><b>四、Item '
            'Pipeline的使用。</b></p><p><b>1、进行数据的清洗</b></p><p>在一的实例中我们把评分小于等于8.5分的score修改为（不好看！），我们认为是不好看的电影，我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 678px; max-height: '
            '306px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '45.129999999999995%;"></div>\n'
            '<div class="image-view" data-width="678" data-height="306"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-23021209da207b71" '
            'data-original-width="678" data-original-height="306" '
            'data-original-format="image/png" '
            'data-original-filesize="7034"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 580px; max-height: '
            '78px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '13.450000000000001%;"></div>\n'
            '<div class="image-view" data-width="580" data-height="78"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c" '
            'data-original-width="580" data-original-height="78" '
            'data-original-format="image/png" '
            'data-original-filesize="5871"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>我们执行一下：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '460px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '44.7%;"></div>\n'
            '<div class="image-view" data-width="1029" data-height="460"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c" '
            'data-original-width="1029" data-original-height="460" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="91210"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p><b>2、储存</b></p><p><b>2.1储存为json格式</b></p><p>我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '564px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '65.58%;"></div>\n'
            '<div class="image-view" data-width="860" data-height="564"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96" '
            'data-original-width="860" data-original-height="564" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="76827"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 616px; max-height: '
            '100px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '16.23%;"></div>\n'
            '<div class="image-view" data-width="616" data-height="100"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93" '
            'data-original-width="616" data-original-height="100" '
            'data-original-format="image/png" '
            'data-original-filesize="7382"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>表示先执行TextPipeline方法，再执行JsonPipeline方法，先清洗，再储存。</p><p><b>2.2储存在mysql数据库</b></p><p>首先在mysql数据库中创建一个数据库maoyanreying，创建一个表maoyan。</p><p>我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '634px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '62.46000000000001%;"></div>\n'
            '<div class="image-view" data-width="1015" data-height="634"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3" '
            'data-original-width="1015" data-original-height="634" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="102252"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 566px; max-height: '
            '218px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '38.519999999999996%;"></div>\n'
            '<div class="image-view" data-width="566" data-height="218"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198" '
            'data-original-width="566" data-original-height="218" '
            'data-original-format="image/png" '
            'data-original-filesize="5142"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>即可</p><p>完。</p></article><div></div><div class="_19DgIp" '
            'style="margin-top:24px;margin-bottom:24px"></div></section>',
 'image_srcs': ['//upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7',
                '//upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2',
                '//upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852',
                '//upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2',
                '//upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f',
                '//upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602',
                '//upload-images.jianshu.io/upload_images/9489193-23021209da207b71',
                '//upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c',
                '//upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c',
                '//upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96',
                '//upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93',
                '//upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3',
                '//upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198'],
 'image_urls': ['https://upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7',
                'https://upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2',
                'https://upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852',
                'https://upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2',
                'https://upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f',
                'https://upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602',
                'https://upload-images.jianshu.io/upload_images/9489193-23021209da207b71',
                'https://upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c',
                'https://upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c',
                'https://upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96',
                'https://upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93',
                'https://upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3',
                'https://upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198'],
 'title': '爬虫框架Scrapy的安装与基本使用',
 'url': 'https://www.jianshu.com/p/6bc5a4641629'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\PycharmProjects\MyScrapy_jianshu\MyScrapy_jianshu\pipelines.py", line 38, in process_item
    with codecs.open(filename, 'w', 'utf-8') as f:
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\codecs.py", line 898, in open
    file = builtins.open(filename, mode, buffering)
PermissionError: [Errno 13] Permission denied: 'outputs/爬虫框架Scrapy的安装与基本使用.html'
2019-10-30 10:53:36 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-30 10:53:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 600,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 39363,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.554367,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 30, 2, 53, 36, 236918),
 'log_count/DEBUG': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 10, 30, 2, 53, 35, 682551)}
2019-10-30 10:53:36 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-30 10:54:19 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: MyScrapy_jianshu)
2019-10-30 10:54:19 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.17763-SP0
2019-10-30 10:54:19 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'MyScrapy_jianshu', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'MyScrapy_jianshu.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['MyScrapy_jianshu.spiders']}
2019-10-30 10:54:19 [scrapy.extensions.telnet] INFO: Telnet Password: 4f62c3d388ae5fc0
2019-10-30 10:54:19 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-30 10:54:20 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-30 10:54:20 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-30 10:54:20 [scrapy.middleware] INFO: Enabled item pipelines:
['MyScrapy_jianshu.pipelines.MyscrapyJianshuPipeline',
 'MyScrapy_jianshu.pipelines.MyImagesPipeline']
2019-10-30 10:54:20 [scrapy.core.engine] INFO: Spider opened
2019-10-30 10:54:20 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-30 10:54:20 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-30 10:54:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/robots.txt> (referer: None)
2019-10-30 10:54:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/p/6bc5a4641629> (referer: None)
2019-10-30 10:54:20 [scrapy.core.scraper] ERROR: Error processing {'content': '<section class="ouvJEz"><h1 '
            'class="_1RuRku">爬虫框架Scrapy的安装与基本使用</h1><div class="rEsl9f"><div '
            'class="s-dsoj"><span class="_3tCVn5"><i aria-label="ic-diamond" '
            'class="anticon"><svg width="1em" height="1em" fill="currentColor" '
            'aria-hidden="true" focusable="false" class=""><use '
            'xlink:href="#ic-diamond"></use></svg></i><span>1</span></span><time '
            'datetime="2018-07-20T04:25:56.000Z">2018.07.20 '
            '12:25:56</time><span>字数 2216</span><span>阅读 '
            '38653</span></div></div><article '
            'class="_2rhmJa"><blockquote><p><b>概括：上一节学习了pyspider框架，这一节我们来看一下Scrapy的强大之处。</b><b>他应该是目前python使用的最广泛的爬虫框架。</b></p></blockquote><p><b>一、简单实例，了解基本。</b></p><p><b>1、安装Scrapy框架</b></p><p>这里如果直接pip3 '
            'install scrapy可能会出错。</p><p>所以你可以先安装lxml：pip3 install '
            'lxml(已安装请忽略)。</p><p>安装pyOpenSSL：在官网下载wheel文件。</p><p>安装Twisted：在官网下载wheel文件。</p><p>安装PyWin32：在官网下载wheel文件。</p><p>下载地址：https://www.lfd.uci.edu/~gohlke/pythonlibs/</p><p>配置环境变量：将scrapy所在目录添加到系统环境变量即可。</p><p>ctrl+f搜索即可。</p><p>最后安装scrapy，pip3 '
            'install '
            'scrapy</p><p><b>2、创建一个scrapy项目</b></p><p>新创建一个目录，按住shift-右键-在此处打开命令窗口</p><p>输入：scrapy '
            'startproject '
            'tutorial即可创建一个tutorial文件夹</p><p>文件夹目录如下：</p><blockquote>\n'
            '<p>|-tutorial</p>\n'
            '<p>|-scrapy.cfg</p>\n'
            '<p>\xa0 |-__init__.py</p>\n'
            '<p>\xa0 |-items.py</p>\n'
            '<p>\xa0 |-middlewares.py</p>\n'
            '<p>\xa0 |-pipelines.py</p>\n'
            '<p>\xa0 |-settings.py</p>\n'
            '<p>\xa0 |-spiders</p>\n'
            '<p>\xa0 \xa0 |-__init__.py</p>\n'
            '</blockquote><p><b>文件的功能：</b></p><p>scrapy.cfg：配置文件</p><p>spiders：存放你Spider文件，也就是你爬取的py文件</p><p>items.py：相当于一个容器，和字典较像</p><p>middlewares.py：定义Downloader '
            'Middlewares(下载器中间件)和Spider '
            'Middlewares(蜘蛛中间件)的实现</p><p>pipelines.py:定义Item '
            'Pipeline的实现，实现数据的清洗，储存，验证。</p><p>settings.py：全局配置</p><p><b>3、创建一个spider（自己定义的爬虫文件）</b></p><p>例如以爬取猫眼热映口碑榜为例子来了解一下：</p><p>在spiders文件夹下创建一个maoyan.py文件，你也可以按住shift-右键-在此处打开命令窗口，输入：scrapy '
            'genspider 文件名 '
            '要爬取的网址。</p><p>自己创建的需要自己写，使用命令创建的包含最基本的东西。</p><p>我们来看一下使用命令创建的有什么。</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 573px; max-height: '
            '264px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '46.07%;"></div>\n'
            '<div class="image-view" data-width="573" data-height="264"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7" '
            'data-original-width="573" data-original-height="264" '
            'data-original-format="image/png" '
            'data-original-filesize="10489"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>介绍一下这些是干嘛的：</p><p>name：是项目的名字</p><p>allowed_domains：是允许爬取的域名，比如一些网站有相关链接，域名就和本网站不同，这些就会忽略。</p><p>atart_urls：是Spider爬取的网站，定义初始的请求url，可以多个。</p><p>parse方法：是Spider的一个方法，在请求start_url后，之后的方法，这个方法是对网页的解析，与提取自己想要的东西。</p><p>response参数：是请求网页后返回的内容，也就是你需要解析的网页。</p><p>还有其他参数有兴趣可以去查查。</p><p><b>4、定义Item</b></p><p>item是保存爬取数据的容器，使用的方法和字典差不多。</p><p>我们打开items.py，之后我们想要提取的信息有：</p><p>index(排名)、title(电影名)、star(主演)、releasetime(上映时间)、score(评分)</p><p>于是我们将items.py文件修改成这样。</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '226px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '29.82%;"></div>\n'
            '<div class="image-view" data-width="758" data-height="226"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2" '
            'data-original-width="758" data-original-height="226" '
            'data-original-format="image/png" '
            'data-original-filesize="12301"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>即可。</p><p><b>5、再次打开spider来提取我们想要的信息</b></p><p>修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '399px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '36.94%;"></div>\n'
            '<div class="image-view" data-width="1080" data-height="399"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852" '
            'data-original-width="1080" data-original-height="399" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="53873"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>好了，一个简单的爬虫就写完了。</p><p><b>6、运行</b></p><p>在该文件夹下，按住shift-右键-在此处打开命令窗口，输入：scrapy '
            'crawl maoyan(项目的名字)</p><p>即可看到：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '580px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '53.7%;"></div>\n'
            '<div class="image-view" data-width="1080" data-height="580"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2" '
            'data-original-width="1080" data-original-height="580" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="107723"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p><b>7、保存</b></p><p>我们只运行了代码，看看有没有报错，并没有保存。</p><p>如果我们想保存为csv、xml、json格式，可以直接使用命令：</p><p>在该文件夹下，按住shift-右键-在此处打开命令窗口，输入：</p><p>scrapy '
            'crawl maoyan -o maoyan.csv</p><p>scrapy crawl maoyan -o '
            'maoyan.xml</p><p>scrapy crawl maoyan -o '
            "maoyan.json</p><p>选择其中一个即可。当然如果想要保存为其他格式也是可以的，这里只说常见的。这里选择json格式，运行后会发现，在文件夹下多出来一个maoyan.json的文件。打开之后发现，中文都是一串乱码，这里需要修改编码方式，当然也可以在配置里修改</p><p>（在settings.py文件中添加FEED_EXPORT_ENCODING='UTF8'即可），</p><p>如果想直接在命令行中修改：</p><p>scrapy "
            'crawl maoyan -o maoyan.json -s '
            'FEED_EXPORT_ENCODING=UTF8</p><p>即可。</p><p>这里自己试试效果吧。</p><p>当然我们保存也可以在运行的时候自动保存，不需要自己写命令。后面介绍（我们还有还多文件没有用到呦）。</p><p><b>二、scrapy如何解析？</b></p><p>之前写过一篇文章：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483766&amp;idx=1&amp;sn=86d61115ebb7a4083e17a54f1acffdf1&amp;chksm=fb74c947cc03405122d74b1172a9a96ef35af65753666f3f7900a9318290c4326736c2d6e69e&amp;scene=21#wechat_redirect" '
            'target="_blank" '
            'rel="nofollow">三大解析库的使用</a></p><p>但是scrapy也提供了自己的解析方式（Selector），和上面的也很相似，我们来看一下：</p><p><b>1、css</b></p><p>首先需要导入模块：from '
            'scrapy import '
            "Selector</p><p>例如有这样一段html代码：</p><p>html='&lt;html&gt;&lt;head&gt;&lt;title&gt;Demo&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;div "
            "class='cla'&gt;This is "
            'Demo&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</p><p><b>1.1、首先需要构建一个Selector对象</b></p><p>sel '
            '= Selector(html)</p><p>text = '
            "sel.css('.cla::text').extract_first()</p><p>.cla表示选中上面的div节点，::text表示获取文本，这里和以前的有所不同。</p><p>extract_first()表示返回第一个元素，因为上述 "
            'sel.css(\'.cla::text\')返回的是一个列表，你也可以写成sel.css(\'.cla::text\')[0]来获取第一个元素，但是如果为空，就会报出超出最大索引的错误，不建议这样写，而使用extract_first()就不会报错，同时如果写成extract_first(\'123\')这样，如果为空就返回123</p><p><b>1.2、</b>有了选取第一个，就有选取所有：extract()表示选取所有，如果返回的是多个值，就可以是这样写。</p><p><b>1.3、</b>获取属性就是sel.css(\'.cla::sttr(\'class\')\').extract_first()表示获取class</p><p><b>1.4、</b>获取指定属性的文本：sel.css(\'div[class="cla"]::text\')</p><p><b>1.5、</b>其他写法和css的写法如出一辙。</p><p><b>1.6、</b>在scrapy中为我们提供了一个简便的写法，在上述的简单实例中，我们知道了response为请求网页的返回值。</p><p>我们可以直接写成：response.css()来解析，提取我们想要的信息。同样，下面要说的XPath也可以直接写成：</p><p>response.xpath()来解析。</p><p><b>2、Xpath</b></p><p>Xpath的使用可以看上面的文章：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483766&amp;idx=1&amp;sn=86d61115ebb7a4083e17a54f1acffdf1&amp;chksm=fb74c947cc03405122d74b1172a9a96ef35af65753666f3f7900a9318290c4326736c2d6e69e&amp;scene=21#wechat_redirect" '
            'target="_blank" '
            'rel="nofollow">三大解析库的使用</a></p><p>注意：获取的还是列表，所以还是要加上extract_first()或者extract()</p><p><b>3、正则匹配(这里用response操作)</b></p><p>例如：response.css(\'a::text\').re(\'写正则\')</p><p>这里如果response.css(\'a::text\')匹配的是多个对象，那么加上正则也是匹配符合要求的多个对象。</p><p>这里如果想要匹配第一个对象，可以把re()修改成re_first()即可。</p><p><b>注意：response不可以直接调用re(),response.xpath(\'.\').re()可以相当于达到直接使用正则的效果</b>。</p><p>正则的使用：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483727&amp;idx=1&amp;sn=1fc19af2291177ac3c1a832c47392a3f&amp;chksm=fb74c97ecc0340681bc54c97ceb7bba413da3f1ca6fc3a107bbdabe7eb9be144c261a6de347d&amp;scene=21#wechat_redirect" '
            'target="_blank" rel="nofollow">万能的正则表达式</a></p><p><b>三、Dowmloader '
            'Middleware的使用</b></p><p>本身scrapy就提供了很多Dowmloader '
            "Middleware，但是有时候我们要修改，</p><p>比如修改User-Agent，使用代理ip等。</p><p>以修改User-Agent为例（设置代理ip大同小异）：</p><p>第一种方法，可以在settings.py中直接添加USER-AGENT='xxx'</p><p>但是我们想要添加多个User-Agent，每次随机获取一个可以利用Dowmloader "
            'Middleware来设置。</p><p>第一步将settings中的USER-AGENT=\'xxx\'修改成USER-AGENT=["xxx","xxxxx","xxxxxxx"]</p><p>第二步在middlewares.py中添加：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '287px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '27.02%;"></div>\n'
            '<div class="image-view" data-width="1062" data-height="287"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f" '
            'data-original-width="1062" data-original-height="287" '
            'data-original-format="image/png" '
            'data-original-filesize="19133"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>from_crawler():通过参数crawler可以拿到配置的信息，我们的User-Agent在配置文件里，所以我们需要获取到。</p><p>方法名不可以修改。</p><p>第三步在settings.py中添加：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '110px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '12.04%;"></div>\n'
            '<div class="image-view" data-width="914" data-height="110"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602" '
            'data-original-width="914" data-original-height="110" '
            'data-original-format="image/png" '
            'data-original-filesize="5150"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>将scrapy自带的UserAgentmiddleware的键值设置为None,</p><p>自定义的设置为400，这个键值越小表示优先调用的意思。</p><p><b>四、Item '
            'Pipeline的使用。</b></p><p><b>1、进行数据的清洗</b></p><p>在一的实例中我们把评分小于等于8.5分的score修改为（不好看！），我们认为是不好看的电影，我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 678px; max-height: '
            '306px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '45.129999999999995%;"></div>\n'
            '<div class="image-view" data-width="678" data-height="306"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-23021209da207b71" '
            'data-original-width="678" data-original-height="306" '
            'data-original-format="image/png" '
            'data-original-filesize="7034"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 580px; max-height: '
            '78px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '13.450000000000001%;"></div>\n'
            '<div class="image-view" data-width="580" data-height="78"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c" '
            'data-original-width="580" data-original-height="78" '
            'data-original-format="image/png" '
            'data-original-filesize="5871"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>我们执行一下：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '460px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '44.7%;"></div>\n'
            '<div class="image-view" data-width="1029" data-height="460"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c" '
            'data-original-width="1029" data-original-height="460" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="91210"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p><b>2、储存</b></p><p><b>2.1储存为json格式</b></p><p>我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '564px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '65.58%;"></div>\n'
            '<div class="image-view" data-width="860" data-height="564"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96" '
            'data-original-width="860" data-original-height="564" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="76827"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 616px; max-height: '
            '100px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '16.23%;"></div>\n'
            '<div class="image-view" data-width="616" data-height="100"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93" '
            'data-original-width="616" data-original-height="100" '
            'data-original-format="image/png" '
            'data-original-filesize="7382"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>表示先执行TextPipeline方法，再执行JsonPipeline方法，先清洗，再储存。</p><p><b>2.2储存在mysql数据库</b></p><p>首先在mysql数据库中创建一个数据库maoyanreying，创建一个表maoyan。</p><p>我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '634px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '62.46000000000001%;"></div>\n'
            '<div class="image-view" data-width="1015" data-height="634"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3" '
            'data-original-width="1015" data-original-height="634" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="102252"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 566px; max-height: '
            '218px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '38.519999999999996%;"></div>\n'
            '<div class="image-view" data-width="566" data-height="218"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198" '
            'data-original-width="566" data-original-height="218" '
            'data-original-format="image/png" '
            'data-original-filesize="5142"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>即可</p><p>完。</p></article><div></div><div class="_19DgIp" '
            'style="margin-top:24px;margin-bottom:24px"></div></section>',
 'image_srcs': ['//upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7',
                '//upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2',
                '//upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852',
                '//upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2',
                '//upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f',
                '//upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602',
                '//upload-images.jianshu.io/upload_images/9489193-23021209da207b71',
                '//upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c',
                '//upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c',
                '//upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96',
                '//upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93',
                '//upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3',
                '//upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198'],
 'image_urls': ['https://upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7',
                'https://upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2',
                'https://upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852',
                'https://upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2',
                'https://upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f',
                'https://upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602',
                'https://upload-images.jianshu.io/upload_images/9489193-23021209da207b71',
                'https://upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c',
                'https://upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c',
                'https://upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96',
                'https://upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93',
                'https://upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3',
                'https://upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198'],
 'title': '爬虫框架Scrapy的安装与基本使用',
 'url': 'https://www.jianshu.com/p/6bc5a4641629'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\PycharmProjects\MyScrapy_jianshu\MyScrapy_jianshu\pipelines.py", line 39, in process_item
    with codecs.open(filename, 'w', 'utf-8') as f:
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\codecs.py", line 898, in open
    file = builtins.open(filename, mode, buffering)
PermissionError: [Errno 13] Permission denied: 'outputs/爬虫框架Scrapy的安装与基本使用.html'
2019-10-30 10:54:20 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-30 10:54:20 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 600,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 39362,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.714352,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 30, 2, 54, 20, 854178),
 'log_count/DEBUG': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 10, 30, 2, 54, 20, 139826)}
2019-10-30 10:54:20 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-30 10:55:14 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: MyScrapy_jianshu)
2019-10-30 10:55:14 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.17763-SP0
2019-10-30 10:55:14 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'MyScrapy_jianshu', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'MyScrapy_jianshu.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['MyScrapy_jianshu.spiders']}
2019-10-30 10:55:14 [scrapy.extensions.telnet] INFO: Telnet Password: fe7224ff01defd33
2019-10-30 10:55:14 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-30 10:55:14 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-30 10:55:14 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-30 10:55:14 [scrapy.middleware] INFO: Enabled item pipelines:
['MyScrapy_jianshu.pipelines.MyscrapyJianshuPipeline',
 'MyScrapy_jianshu.pipelines.MyImagesPipeline']
2019-10-30 10:55:14 [scrapy.core.engine] INFO: Spider opened
2019-10-30 10:55:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-30 10:55:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-30 10:55:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/robots.txt> (referer: None)
2019-10-30 10:55:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/p/6bc5a4641629> (referer: None)
2019-10-30 10:55:15 [scrapy.core.scraper] ERROR: Error processing {'content': '<section class="ouvJEz"><h1 '
            'class="_1RuRku">爬虫框架Scrapy的安装与基本使用</h1><div class="rEsl9f"><div '
            'class="s-dsoj"><span class="_3tCVn5"><i aria-label="ic-diamond" '
            'class="anticon"><svg width="1em" height="1em" fill="currentColor" '
            'aria-hidden="true" focusable="false" class=""><use '
            'xlink:href="#ic-diamond"></use></svg></i><span>1</span></span><time '
            'datetime="2018-07-20T04:25:56.000Z">2018.07.20 '
            '12:25:56</time><span>字数 2216</span><span>阅读 '
            '38655</span></div></div><article '
            'class="_2rhmJa"><blockquote><p><b>概括：上一节学习了pyspider框架，这一节我们来看一下Scrapy的强大之处。</b><b>他应该是目前python使用的最广泛的爬虫框架。</b></p></blockquote><p><b>一、简单实例，了解基本。</b></p><p><b>1、安装Scrapy框架</b></p><p>这里如果直接pip3 '
            'install scrapy可能会出错。</p><p>所以你可以先安装lxml：pip3 install '
            'lxml(已安装请忽略)。</p><p>安装pyOpenSSL：在官网下载wheel文件。</p><p>安装Twisted：在官网下载wheel文件。</p><p>安装PyWin32：在官网下载wheel文件。</p><p>下载地址：https://www.lfd.uci.edu/~gohlke/pythonlibs/</p><p>配置环境变量：将scrapy所在目录添加到系统环境变量即可。</p><p>ctrl+f搜索即可。</p><p>最后安装scrapy，pip3 '
            'install '
            'scrapy</p><p><b>2、创建一个scrapy项目</b></p><p>新创建一个目录，按住shift-右键-在此处打开命令窗口</p><p>输入：scrapy '
            'startproject '
            'tutorial即可创建一个tutorial文件夹</p><p>文件夹目录如下：</p><blockquote>\n'
            '<p>|-tutorial</p>\n'
            '<p>|-scrapy.cfg</p>\n'
            '<p>\xa0 |-__init__.py</p>\n'
            '<p>\xa0 |-items.py</p>\n'
            '<p>\xa0 |-middlewares.py</p>\n'
            '<p>\xa0 |-pipelines.py</p>\n'
            '<p>\xa0 |-settings.py</p>\n'
            '<p>\xa0 |-spiders</p>\n'
            '<p>\xa0 \xa0 |-__init__.py</p>\n'
            '</blockquote><p><b>文件的功能：</b></p><p>scrapy.cfg：配置文件</p><p>spiders：存放你Spider文件，也就是你爬取的py文件</p><p>items.py：相当于一个容器，和字典较像</p><p>middlewares.py：定义Downloader '
            'Middlewares(下载器中间件)和Spider '
            'Middlewares(蜘蛛中间件)的实现</p><p>pipelines.py:定义Item '
            'Pipeline的实现，实现数据的清洗，储存，验证。</p><p>settings.py：全局配置</p><p><b>3、创建一个spider（自己定义的爬虫文件）</b></p><p>例如以爬取猫眼热映口碑榜为例子来了解一下：</p><p>在spiders文件夹下创建一个maoyan.py文件，你也可以按住shift-右键-在此处打开命令窗口，输入：scrapy '
            'genspider 文件名 '
            '要爬取的网址。</p><p>自己创建的需要自己写，使用命令创建的包含最基本的东西。</p><p>我们来看一下使用命令创建的有什么。</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 573px; max-height: '
            '264px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '46.07%;"></div>\n'
            '<div class="image-view" data-width="573" data-height="264"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7" '
            'data-original-width="573" data-original-height="264" '
            'data-original-format="image/png" '
            'data-original-filesize="10489"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>介绍一下这些是干嘛的：</p><p>name：是项目的名字</p><p>allowed_domains：是允许爬取的域名，比如一些网站有相关链接，域名就和本网站不同，这些就会忽略。</p><p>atart_urls：是Spider爬取的网站，定义初始的请求url，可以多个。</p><p>parse方法：是Spider的一个方法，在请求start_url后，之后的方法，这个方法是对网页的解析，与提取自己想要的东西。</p><p>response参数：是请求网页后返回的内容，也就是你需要解析的网页。</p><p>还有其他参数有兴趣可以去查查。</p><p><b>4、定义Item</b></p><p>item是保存爬取数据的容器，使用的方法和字典差不多。</p><p>我们打开items.py，之后我们想要提取的信息有：</p><p>index(排名)、title(电影名)、star(主演)、releasetime(上映时间)、score(评分)</p><p>于是我们将items.py文件修改成这样。</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '226px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '29.82%;"></div>\n'
            '<div class="image-view" data-width="758" data-height="226"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2" '
            'data-original-width="758" data-original-height="226" '
            'data-original-format="image/png" '
            'data-original-filesize="12301"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>即可。</p><p><b>5、再次打开spider来提取我们想要的信息</b></p><p>修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '399px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '36.94%;"></div>\n'
            '<div class="image-view" data-width="1080" data-height="399"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852" '
            'data-original-width="1080" data-original-height="399" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="53873"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>好了，一个简单的爬虫就写完了。</p><p><b>6、运行</b></p><p>在该文件夹下，按住shift-右键-在此处打开命令窗口，输入：scrapy '
            'crawl maoyan(项目的名字)</p><p>即可看到：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '580px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '53.7%;"></div>\n'
            '<div class="image-view" data-width="1080" data-height="580"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2" '
            'data-original-width="1080" data-original-height="580" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="107723"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p><b>7、保存</b></p><p>我们只运行了代码，看看有没有报错，并没有保存。</p><p>如果我们想保存为csv、xml、json格式，可以直接使用命令：</p><p>在该文件夹下，按住shift-右键-在此处打开命令窗口，输入：</p><p>scrapy '
            'crawl maoyan -o maoyan.csv</p><p>scrapy crawl maoyan -o '
            'maoyan.xml</p><p>scrapy crawl maoyan -o '
            "maoyan.json</p><p>选择其中一个即可。当然如果想要保存为其他格式也是可以的，这里只说常见的。这里选择json格式，运行后会发现，在文件夹下多出来一个maoyan.json的文件。打开之后发现，中文都是一串乱码，这里需要修改编码方式，当然也可以在配置里修改</p><p>（在settings.py文件中添加FEED_EXPORT_ENCODING='UTF8'即可），</p><p>如果想直接在命令行中修改：</p><p>scrapy "
            'crawl maoyan -o maoyan.json -s '
            'FEED_EXPORT_ENCODING=UTF8</p><p>即可。</p><p>这里自己试试效果吧。</p><p>当然我们保存也可以在运行的时候自动保存，不需要自己写命令。后面介绍（我们还有还多文件没有用到呦）。</p><p><b>二、scrapy如何解析？</b></p><p>之前写过一篇文章：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483766&amp;idx=1&amp;sn=86d61115ebb7a4083e17a54f1acffdf1&amp;chksm=fb74c947cc03405122d74b1172a9a96ef35af65753666f3f7900a9318290c4326736c2d6e69e&amp;scene=21#wechat_redirect" '
            'target="_blank" '
            'rel="nofollow">三大解析库的使用</a></p><p>但是scrapy也提供了自己的解析方式（Selector），和上面的也很相似，我们来看一下：</p><p><b>1、css</b></p><p>首先需要导入模块：from '
            'scrapy import '
            "Selector</p><p>例如有这样一段html代码：</p><p>html='&lt;html&gt;&lt;head&gt;&lt;title&gt;Demo&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;div "
            "class='cla'&gt;This is "
            'Demo&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</p><p><b>1.1、首先需要构建一个Selector对象</b></p><p>sel '
            '= Selector(html)</p><p>text = '
            "sel.css('.cla::text').extract_first()</p><p>.cla表示选中上面的div节点，::text表示获取文本，这里和以前的有所不同。</p><p>extract_first()表示返回第一个元素，因为上述 "
            'sel.css(\'.cla::text\')返回的是一个列表，你也可以写成sel.css(\'.cla::text\')[0]来获取第一个元素，但是如果为空，就会报出超出最大索引的错误，不建议这样写，而使用extract_first()就不会报错，同时如果写成extract_first(\'123\')这样，如果为空就返回123</p><p><b>1.2、</b>有了选取第一个，就有选取所有：extract()表示选取所有，如果返回的是多个值，就可以是这样写。</p><p><b>1.3、</b>获取属性就是sel.css(\'.cla::sttr(\'class\')\').extract_first()表示获取class</p><p><b>1.4、</b>获取指定属性的文本：sel.css(\'div[class="cla"]::text\')</p><p><b>1.5、</b>其他写法和css的写法如出一辙。</p><p><b>1.6、</b>在scrapy中为我们提供了一个简便的写法，在上述的简单实例中，我们知道了response为请求网页的返回值。</p><p>我们可以直接写成：response.css()来解析，提取我们想要的信息。同样，下面要说的XPath也可以直接写成：</p><p>response.xpath()来解析。</p><p><b>2、Xpath</b></p><p>Xpath的使用可以看上面的文章：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483766&amp;idx=1&amp;sn=86d61115ebb7a4083e17a54f1acffdf1&amp;chksm=fb74c947cc03405122d74b1172a9a96ef35af65753666f3f7900a9318290c4326736c2d6e69e&amp;scene=21#wechat_redirect" '
            'target="_blank" '
            'rel="nofollow">三大解析库的使用</a></p><p>注意：获取的还是列表，所以还是要加上extract_first()或者extract()</p><p><b>3、正则匹配(这里用response操作)</b></p><p>例如：response.css(\'a::text\').re(\'写正则\')</p><p>这里如果response.css(\'a::text\')匹配的是多个对象，那么加上正则也是匹配符合要求的多个对象。</p><p>这里如果想要匹配第一个对象，可以把re()修改成re_first()即可。</p><p><b>注意：response不可以直接调用re(),response.xpath(\'.\').re()可以相当于达到直接使用正则的效果</b>。</p><p>正则的使用：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483727&amp;idx=1&amp;sn=1fc19af2291177ac3c1a832c47392a3f&amp;chksm=fb74c97ecc0340681bc54c97ceb7bba413da3f1ca6fc3a107bbdabe7eb9be144c261a6de347d&amp;scene=21#wechat_redirect" '
            'target="_blank" rel="nofollow">万能的正则表达式</a></p><p><b>三、Dowmloader '
            'Middleware的使用</b></p><p>本身scrapy就提供了很多Dowmloader '
            "Middleware，但是有时候我们要修改，</p><p>比如修改User-Agent，使用代理ip等。</p><p>以修改User-Agent为例（设置代理ip大同小异）：</p><p>第一种方法，可以在settings.py中直接添加USER-AGENT='xxx'</p><p>但是我们想要添加多个User-Agent，每次随机获取一个可以利用Dowmloader "
            'Middleware来设置。</p><p>第一步将settings中的USER-AGENT=\'xxx\'修改成USER-AGENT=["xxx","xxxxx","xxxxxxx"]</p><p>第二步在middlewares.py中添加：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '287px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '27.02%;"></div>\n'
            '<div class="image-view" data-width="1062" data-height="287"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f" '
            'data-original-width="1062" data-original-height="287" '
            'data-original-format="image/png" '
            'data-original-filesize="19133"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>from_crawler():通过参数crawler可以拿到配置的信息，我们的User-Agent在配置文件里，所以我们需要获取到。</p><p>方法名不可以修改。</p><p>第三步在settings.py中添加：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '110px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '12.04%;"></div>\n'
            '<div class="image-view" data-width="914" data-height="110"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602" '
            'data-original-width="914" data-original-height="110" '
            'data-original-format="image/png" '
            'data-original-filesize="5150"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>将scrapy自带的UserAgentmiddleware的键值设置为None,</p><p>自定义的设置为400，这个键值越小表示优先调用的意思。</p><p><b>四、Item '
            'Pipeline的使用。</b></p><p><b>1、进行数据的清洗</b></p><p>在一的实例中我们把评分小于等于8.5分的score修改为（不好看！），我们认为是不好看的电影，我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 678px; max-height: '
            '306px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '45.129999999999995%;"></div>\n'
            '<div class="image-view" data-width="678" data-height="306"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-23021209da207b71" '
            'data-original-width="678" data-original-height="306" '
            'data-original-format="image/png" '
            'data-original-filesize="7034"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 580px; max-height: '
            '78px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '13.450000000000001%;"></div>\n'
            '<div class="image-view" data-width="580" data-height="78"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c" '
            'data-original-width="580" data-original-height="78" '
            'data-original-format="image/png" '
            'data-original-filesize="5871"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>我们执行一下：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '460px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '44.7%;"></div>\n'
            '<div class="image-view" data-width="1029" data-height="460"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c" '
            'data-original-width="1029" data-original-height="460" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="91210"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p><b>2、储存</b></p><p><b>2.1储存为json格式</b></p><p>我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '564px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '65.58%;"></div>\n'
            '<div class="image-view" data-width="860" data-height="564"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96" '
            'data-original-width="860" data-original-height="564" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="76827"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 616px; max-height: '
            '100px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '16.23%;"></div>\n'
            '<div class="image-view" data-width="616" data-height="100"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93" '
            'data-original-width="616" data-original-height="100" '
            'data-original-format="image/png" '
            'data-original-filesize="7382"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>表示先执行TextPipeline方法，再执行JsonPipeline方法，先清洗，再储存。</p><p><b>2.2储存在mysql数据库</b></p><p>首先在mysql数据库中创建一个数据库maoyanreying，创建一个表maoyan。</p><p>我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '634px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '62.46000000000001%;"></div>\n'
            '<div class="image-view" data-width="1015" data-height="634"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3" '
            'data-original-width="1015" data-original-height="634" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="102252"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 566px; max-height: '
            '218px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '38.519999999999996%;"></div>\n'
            '<div class="image-view" data-width="566" data-height="218"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198" '
            'data-original-width="566" data-original-height="218" '
            'data-original-format="image/png" '
            'data-original-filesize="5142"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>即可</p><p>完。</p></article><div></div><div class="_19DgIp" '
            'style="margin-top:24px;margin-bottom:24px"></div></section>',
 'image_srcs': ['//upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7',
                '//upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2',
                '//upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852',
                '//upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2',
                '//upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f',
                '//upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602',
                '//upload-images.jianshu.io/upload_images/9489193-23021209da207b71',
                '//upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c',
                '//upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c',
                '//upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96',
                '//upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93',
                '//upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3',
                '//upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198'],
 'image_urls': ['https://upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7',
                'https://upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2',
                'https://upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852',
                'https://upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2',
                'https://upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f',
                'https://upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602',
                'https://upload-images.jianshu.io/upload_images/9489193-23021209da207b71',
                'https://upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c',
                'https://upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c',
                'https://upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96',
                'https://upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93',
                'https://upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3',
                'https://upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198'],
 'title': '爬虫框架Scrapy的安装与基本使用',
 'url': 'https://www.jianshu.com/p/6bc5a4641629'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\PycharmProjects\MyScrapy_jianshu\MyScrapy_jianshu\pipelines.py", line 40, in process_item
    with codecs.open(filename, 'w', 'utf-8') as f:
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\codecs.py", line 898, in open
    file = builtins.open(filename, mode, buffering)
PermissionError: [Errno 13] Permission denied: 'outputs/爬虫框架Scrapy的安装与基本使用.html'
2019-10-30 10:55:15 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-30 10:55:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 600,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 39368,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.944861,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 30, 2, 55, 15, 469537),
 'log_count/DEBUG': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 10, 30, 2, 55, 14, 524676)}
2019-10-30 10:55:15 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-30 10:59:17 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: MyScrapy_jianshu)
2019-10-30 10:59:17 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.17763-SP0
2019-10-30 10:59:17 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'MyScrapy_jianshu', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'MyScrapy_jianshu.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['MyScrapy_jianshu.spiders']}
2019-10-30 10:59:17 [scrapy.extensions.telnet] INFO: Telnet Password: f440c8ec2c0de7f8
2019-10-30 10:59:17 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-30 10:59:17 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-30 10:59:17 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-30 10:59:17 [scrapy.middleware] INFO: Enabled item pipelines:
['MyScrapy_jianshu.pipelines.MyscrapyJianshuPipeline',
 'MyScrapy_jianshu.pipelines.MyImagesPipeline']
2019-10-30 10:59:17 [scrapy.core.engine] INFO: Spider opened
2019-10-30 10:59:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-30 10:59:17 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-30 10:59:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/robots.txt> (referer: None)
2019-10-30 10:59:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/p/6bc5a4641629> (referer: None)
2019-10-30 10:59:18 [scrapy.core.scraper] ERROR: Error processing {'content': '<section class="ouvJEz"><h1 '
            'class="_1RuRku">爬虫框架Scrapy的安装与基本使用</h1><div class="rEsl9f"><div '
            'class="s-dsoj"><span class="_3tCVn5"><i aria-label="ic-diamond" '
            'class="anticon"><svg width="1em" height="1em" fill="currentColor" '
            'aria-hidden="true" focusable="false" class=""><use '
            'xlink:href="#ic-diamond"></use></svg></i><span>1</span></span><time '
            'datetime="2018-07-20T04:25:56.000Z">2018.07.20 '
            '12:25:56</time><span>字数 2216</span><span>阅读 '
            '38661</span></div></div><article '
            'class="_2rhmJa"><blockquote><p><b>概括：上一节学习了pyspider框架，这一节我们来看一下Scrapy的强大之处。</b><b>他应该是目前python使用的最广泛的爬虫框架。</b></p></blockquote><p><b>一、简单实例，了解基本。</b></p><p><b>1、安装Scrapy框架</b></p><p>这里如果直接pip3 '
            'install scrapy可能会出错。</p><p>所以你可以先安装lxml：pip3 install '
            'lxml(已安装请忽略)。</p><p>安装pyOpenSSL：在官网下载wheel文件。</p><p>安装Twisted：在官网下载wheel文件。</p><p>安装PyWin32：在官网下载wheel文件。</p><p>下载地址：https://www.lfd.uci.edu/~gohlke/pythonlibs/</p><p>配置环境变量：将scrapy所在目录添加到系统环境变量即可。</p><p>ctrl+f搜索即可。</p><p>最后安装scrapy，pip3 '
            'install '
            'scrapy</p><p><b>2、创建一个scrapy项目</b></p><p>新创建一个目录，按住shift-右键-在此处打开命令窗口</p><p>输入：scrapy '
            'startproject '
            'tutorial即可创建一个tutorial文件夹</p><p>文件夹目录如下：</p><blockquote>\n'
            '<p>|-tutorial</p>\n'
            '<p>|-scrapy.cfg</p>\n'
            '<p>\xa0 |-__init__.py</p>\n'
            '<p>\xa0 |-items.py</p>\n'
            '<p>\xa0 |-middlewares.py</p>\n'
            '<p>\xa0 |-pipelines.py</p>\n'
            '<p>\xa0 |-settings.py</p>\n'
            '<p>\xa0 |-spiders</p>\n'
            '<p>\xa0 \xa0 |-__init__.py</p>\n'
            '</blockquote><p><b>文件的功能：</b></p><p>scrapy.cfg：配置文件</p><p>spiders：存放你Spider文件，也就是你爬取的py文件</p><p>items.py：相当于一个容器，和字典较像</p><p>middlewares.py：定义Downloader '
            'Middlewares(下载器中间件)和Spider '
            'Middlewares(蜘蛛中间件)的实现</p><p>pipelines.py:定义Item '
            'Pipeline的实现，实现数据的清洗，储存，验证。</p><p>settings.py：全局配置</p><p><b>3、创建一个spider（自己定义的爬虫文件）</b></p><p>例如以爬取猫眼热映口碑榜为例子来了解一下：</p><p>在spiders文件夹下创建一个maoyan.py文件，你也可以按住shift-右键-在此处打开命令窗口，输入：scrapy '
            'genspider 文件名 '
            '要爬取的网址。</p><p>自己创建的需要自己写，使用命令创建的包含最基本的东西。</p><p>我们来看一下使用命令创建的有什么。</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 573px; max-height: '
            '264px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '46.07%;"></div>\n'
            '<div class="image-view" data-width="573" data-height="264"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7" '
            'data-original-width="573" data-original-height="264" '
            'data-original-format="image/png" '
            'data-original-filesize="10489"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>介绍一下这些是干嘛的：</p><p>name：是项目的名字</p><p>allowed_domains：是允许爬取的域名，比如一些网站有相关链接，域名就和本网站不同，这些就会忽略。</p><p>atart_urls：是Spider爬取的网站，定义初始的请求url，可以多个。</p><p>parse方法：是Spider的一个方法，在请求start_url后，之后的方法，这个方法是对网页的解析，与提取自己想要的东西。</p><p>response参数：是请求网页后返回的内容，也就是你需要解析的网页。</p><p>还有其他参数有兴趣可以去查查。</p><p><b>4、定义Item</b></p><p>item是保存爬取数据的容器，使用的方法和字典差不多。</p><p>我们打开items.py，之后我们想要提取的信息有：</p><p>index(排名)、title(电影名)、star(主演)、releasetime(上映时间)、score(评分)</p><p>于是我们将items.py文件修改成这样。</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '226px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '29.82%;"></div>\n'
            '<div class="image-view" data-width="758" data-height="226"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2" '
            'data-original-width="758" data-original-height="226" '
            'data-original-format="image/png" '
            'data-original-filesize="12301"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>即可。</p><p><b>5、再次打开spider来提取我们想要的信息</b></p><p>修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '399px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '36.94%;"></div>\n'
            '<div class="image-view" data-width="1080" data-height="399"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852" '
            'data-original-width="1080" data-original-height="399" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="53873"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>好了，一个简单的爬虫就写完了。</p><p><b>6、运行</b></p><p>在该文件夹下，按住shift-右键-在此处打开命令窗口，输入：scrapy '
            'crawl maoyan(项目的名字)</p><p>即可看到：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '580px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '53.7%;"></div>\n'
            '<div class="image-view" data-width="1080" data-height="580"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2" '
            'data-original-width="1080" data-original-height="580" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="107723"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p><b>7、保存</b></p><p>我们只运行了代码，看看有没有报错，并没有保存。</p><p>如果我们想保存为csv、xml、json格式，可以直接使用命令：</p><p>在该文件夹下，按住shift-右键-在此处打开命令窗口，输入：</p><p>scrapy '
            'crawl maoyan -o maoyan.csv</p><p>scrapy crawl maoyan -o '
            'maoyan.xml</p><p>scrapy crawl maoyan -o '
            "maoyan.json</p><p>选择其中一个即可。当然如果想要保存为其他格式也是可以的，这里只说常见的。这里选择json格式，运行后会发现，在文件夹下多出来一个maoyan.json的文件。打开之后发现，中文都是一串乱码，这里需要修改编码方式，当然也可以在配置里修改</p><p>（在settings.py文件中添加FEED_EXPORT_ENCODING='UTF8'即可），</p><p>如果想直接在命令行中修改：</p><p>scrapy "
            'crawl maoyan -o maoyan.json -s '
            'FEED_EXPORT_ENCODING=UTF8</p><p>即可。</p><p>这里自己试试效果吧。</p><p>当然我们保存也可以在运行的时候自动保存，不需要自己写命令。后面介绍（我们还有还多文件没有用到呦）。</p><p><b>二、scrapy如何解析？</b></p><p>之前写过一篇文章：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483766&amp;idx=1&amp;sn=86d61115ebb7a4083e17a54f1acffdf1&amp;chksm=fb74c947cc03405122d74b1172a9a96ef35af65753666f3f7900a9318290c4326736c2d6e69e&amp;scene=21#wechat_redirect" '
            'target="_blank" '
            'rel="nofollow">三大解析库的使用</a></p><p>但是scrapy也提供了自己的解析方式（Selector），和上面的也很相似，我们来看一下：</p><p><b>1、css</b></p><p>首先需要导入模块：from '
            'scrapy import '
            "Selector</p><p>例如有这样一段html代码：</p><p>html='&lt;html&gt;&lt;head&gt;&lt;title&gt;Demo&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;div "
            "class='cla'&gt;This is "
            'Demo&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</p><p><b>1.1、首先需要构建一个Selector对象</b></p><p>sel '
            '= Selector(html)</p><p>text = '
            "sel.css('.cla::text').extract_first()</p><p>.cla表示选中上面的div节点，::text表示获取文本，这里和以前的有所不同。</p><p>extract_first()表示返回第一个元素，因为上述 "
            'sel.css(\'.cla::text\')返回的是一个列表，你也可以写成sel.css(\'.cla::text\')[0]来获取第一个元素，但是如果为空，就会报出超出最大索引的错误，不建议这样写，而使用extract_first()就不会报错，同时如果写成extract_first(\'123\')这样，如果为空就返回123</p><p><b>1.2、</b>有了选取第一个，就有选取所有：extract()表示选取所有，如果返回的是多个值，就可以是这样写。</p><p><b>1.3、</b>获取属性就是sel.css(\'.cla::sttr(\'class\')\').extract_first()表示获取class</p><p><b>1.4、</b>获取指定属性的文本：sel.css(\'div[class="cla"]::text\')</p><p><b>1.5、</b>其他写法和css的写法如出一辙。</p><p><b>1.6、</b>在scrapy中为我们提供了一个简便的写法，在上述的简单实例中，我们知道了response为请求网页的返回值。</p><p>我们可以直接写成：response.css()来解析，提取我们想要的信息。同样，下面要说的XPath也可以直接写成：</p><p>response.xpath()来解析。</p><p><b>2、Xpath</b></p><p>Xpath的使用可以看上面的文章：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483766&amp;idx=1&amp;sn=86d61115ebb7a4083e17a54f1acffdf1&amp;chksm=fb74c947cc03405122d74b1172a9a96ef35af65753666f3f7900a9318290c4326736c2d6e69e&amp;scene=21#wechat_redirect" '
            'target="_blank" '
            'rel="nofollow">三大解析库的使用</a></p><p>注意：获取的还是列表，所以还是要加上extract_first()或者extract()</p><p><b>3、正则匹配(这里用response操作)</b></p><p>例如：response.css(\'a::text\').re(\'写正则\')</p><p>这里如果response.css(\'a::text\')匹配的是多个对象，那么加上正则也是匹配符合要求的多个对象。</p><p>这里如果想要匹配第一个对象，可以把re()修改成re_first()即可。</p><p><b>注意：response不可以直接调用re(),response.xpath(\'.\').re()可以相当于达到直接使用正则的效果</b>。</p><p>正则的使用：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483727&amp;idx=1&amp;sn=1fc19af2291177ac3c1a832c47392a3f&amp;chksm=fb74c97ecc0340681bc54c97ceb7bba413da3f1ca6fc3a107bbdabe7eb9be144c261a6de347d&amp;scene=21#wechat_redirect" '
            'target="_blank" rel="nofollow">万能的正则表达式</a></p><p><b>三、Dowmloader '
            'Middleware的使用</b></p><p>本身scrapy就提供了很多Dowmloader '
            "Middleware，但是有时候我们要修改，</p><p>比如修改User-Agent，使用代理ip等。</p><p>以修改User-Agent为例（设置代理ip大同小异）：</p><p>第一种方法，可以在settings.py中直接添加USER-AGENT='xxx'</p><p>但是我们想要添加多个User-Agent，每次随机获取一个可以利用Dowmloader "
            'Middleware来设置。</p><p>第一步将settings中的USER-AGENT=\'xxx\'修改成USER-AGENT=["xxx","xxxxx","xxxxxxx"]</p><p>第二步在middlewares.py中添加：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '287px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '27.02%;"></div>\n'
            '<div class="image-view" data-width="1062" data-height="287"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f" '
            'data-original-width="1062" data-original-height="287" '
            'data-original-format="image/png" '
            'data-original-filesize="19133"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>from_crawler():通过参数crawler可以拿到配置的信息，我们的User-Agent在配置文件里，所以我们需要获取到。</p><p>方法名不可以修改。</p><p>第三步在settings.py中添加：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '110px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '12.04%;"></div>\n'
            '<div class="image-view" data-width="914" data-height="110"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602" '
            'data-original-width="914" data-original-height="110" '
            'data-original-format="image/png" '
            'data-original-filesize="5150"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>将scrapy自带的UserAgentmiddleware的键值设置为None,</p><p>自定义的设置为400，这个键值越小表示优先调用的意思。</p><p><b>四、Item '
            'Pipeline的使用。</b></p><p><b>1、进行数据的清洗</b></p><p>在一的实例中我们把评分小于等于8.5分的score修改为（不好看！），我们认为是不好看的电影，我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 678px; max-height: '
            '306px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '45.129999999999995%;"></div>\n'
            '<div class="image-view" data-width="678" data-height="306"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-23021209da207b71" '
            'data-original-width="678" data-original-height="306" '
            'data-original-format="image/png" '
            'data-original-filesize="7034"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 580px; max-height: '
            '78px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '13.450000000000001%;"></div>\n'
            '<div class="image-view" data-width="580" data-height="78"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c" '
            'data-original-width="580" data-original-height="78" '
            'data-original-format="image/png" '
            'data-original-filesize="5871"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>我们执行一下：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '460px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '44.7%;"></div>\n'
            '<div class="image-view" data-width="1029" data-height="460"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c" '
            'data-original-width="1029" data-original-height="460" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="91210"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p><b>2、储存</b></p><p><b>2.1储存为json格式</b></p><p>我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '564px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '65.58%;"></div>\n'
            '<div class="image-view" data-width="860" data-height="564"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96" '
            'data-original-width="860" data-original-height="564" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="76827"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 616px; max-height: '
            '100px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '16.23%;"></div>\n'
            '<div class="image-view" data-width="616" data-height="100"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93" '
            'data-original-width="616" data-original-height="100" '
            'data-original-format="image/png" '
            'data-original-filesize="7382"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>表示先执行TextPipeline方法，再执行JsonPipeline方法，先清洗，再储存。</p><p><b>2.2储存在mysql数据库</b></p><p>首先在mysql数据库中创建一个数据库maoyanreying，创建一个表maoyan。</p><p>我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '634px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '62.46000000000001%;"></div>\n'
            '<div class="image-view" data-width="1015" data-height="634"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3" '
            'data-original-width="1015" data-original-height="634" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="102252"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 566px; max-height: '
            '218px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '38.519999999999996%;"></div>\n'
            '<div class="image-view" data-width="566" data-height="218"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198" '
            'data-original-width="566" data-original-height="218" '
            'data-original-format="image/png" '
            'data-original-filesize="5142"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>即可</p><p>完。</p></article><div></div><div class="_19DgIp" '
            'style="margin-top:24px;margin-bottom:24px"></div></section>',
 'image_srcs': ['//upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7',
                '//upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2',
                '//upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852',
                '//upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2',
                '//upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f',
                '//upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602',
                '//upload-images.jianshu.io/upload_images/9489193-23021209da207b71',
                '//upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c',
                '//upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c',
                '//upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96',
                '//upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93',
                '//upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3',
                '//upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198'],
 'image_urls': ['https://upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7',
                'https://upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2',
                'https://upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852',
                'https://upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2',
                'https://upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f',
                'https://upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602',
                'https://upload-images.jianshu.io/upload_images/9489193-23021209da207b71',
                'https://upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c',
                'https://upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c',
                'https://upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96',
                'https://upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93',
                'https://upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3',
                'https://upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198'],
 'title': '爬虫框架Scrapy的安装与基本使用',
 'url': 'https://www.jianshu.com/p/6bc5a4641629'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\PycharmProjects\MyScrapy_jianshu\MyScrapy_jianshu\pipelines.py", line 39, in process_item
    with codecs.open(filename, 'w', 'utf-8') as f:
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\codecs.py", line 898, in open
    file = builtins.open(filename, mode, buffering)
PermissionError: [Errno 13] Permission denied: 'outputs/爬虫框架Scrapy的安装与基本使用.html'
2019-10-30 10:59:18 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-30 10:59:18 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 600,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 39361,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.676162,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 30, 2, 59, 18, 625902),
 'log_count/DEBUG': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 10, 30, 2, 59, 17, 949740)}
2019-10-30 10:59:18 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-30 10:59:50 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: MyScrapy_jianshu)
2019-10-30 10:59:50 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.17763-SP0
2019-10-30 10:59:50 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'MyScrapy_jianshu', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'MyScrapy_jianshu.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['MyScrapy_jianshu.spiders']}
2019-10-30 10:59:50 [scrapy.extensions.telnet] INFO: Telnet Password: 30c0c9a2efc9dd53
2019-10-30 10:59:50 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-30 10:59:50 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-30 10:59:50 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-30 10:59:50 [scrapy.middleware] INFO: Enabled item pipelines:
['MyScrapy_jianshu.pipelines.MyscrapyJianshuPipeline',
 'MyScrapy_jianshu.pipelines.MyImagesPipeline']
2019-10-30 10:59:50 [scrapy.core.engine] INFO: Spider opened
2019-10-30 10:59:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-30 10:59:50 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-30 10:59:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/robots.txt> (referer: None)
2019-10-30 10:59:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/p/6bc5a4641629> (referer: None)
2019-10-30 10:59:51 [scrapy.core.scraper] ERROR: Error processing {'content': '<section class="ouvJEz"><h1 '
            'class="_1RuRku">爬虫框架Scrapy的安装与基本使用</h1><div class="rEsl9f"><div '
            'class="s-dsoj"><span class="_3tCVn5"><i aria-label="ic-diamond" '
            'class="anticon"><svg width="1em" height="1em" fill="currentColor" '
            'aria-hidden="true" focusable="false" class=""><use '
            'xlink:href="#ic-diamond"></use></svg></i><span>1</span></span><time '
            'datetime="2018-07-20T04:25:56.000Z">2018.07.20 '
            '12:25:56</time><span>字数 2216</span><span>阅读 '
            '38662</span></div></div><article '
            'class="_2rhmJa"><blockquote><p><b>概括：上一节学习了pyspider框架，这一节我们来看一下Scrapy的强大之处。</b><b>他应该是目前python使用的最广泛的爬虫框架。</b></p></blockquote><p><b>一、简单实例，了解基本。</b></p><p><b>1、安装Scrapy框架</b></p><p>这里如果直接pip3 '
            'install scrapy可能会出错。</p><p>所以你可以先安装lxml：pip3 install '
            'lxml(已安装请忽略)。</p><p>安装pyOpenSSL：在官网下载wheel文件。</p><p>安装Twisted：在官网下载wheel文件。</p><p>安装PyWin32：在官网下载wheel文件。</p><p>下载地址：https://www.lfd.uci.edu/~gohlke/pythonlibs/</p><p>配置环境变量：将scrapy所在目录添加到系统环境变量即可。</p><p>ctrl+f搜索即可。</p><p>最后安装scrapy，pip3 '
            'install '
            'scrapy</p><p><b>2、创建一个scrapy项目</b></p><p>新创建一个目录，按住shift-右键-在此处打开命令窗口</p><p>输入：scrapy '
            'startproject '
            'tutorial即可创建一个tutorial文件夹</p><p>文件夹目录如下：</p><blockquote>\n'
            '<p>|-tutorial</p>\n'
            '<p>|-scrapy.cfg</p>\n'
            '<p>\xa0 |-__init__.py</p>\n'
            '<p>\xa0 |-items.py</p>\n'
            '<p>\xa0 |-middlewares.py</p>\n'
            '<p>\xa0 |-pipelines.py</p>\n'
            '<p>\xa0 |-settings.py</p>\n'
            '<p>\xa0 |-spiders</p>\n'
            '<p>\xa0 \xa0 |-__init__.py</p>\n'
            '</blockquote><p><b>文件的功能：</b></p><p>scrapy.cfg：配置文件</p><p>spiders：存放你Spider文件，也就是你爬取的py文件</p><p>items.py：相当于一个容器，和字典较像</p><p>middlewares.py：定义Downloader '
            'Middlewares(下载器中间件)和Spider '
            'Middlewares(蜘蛛中间件)的实现</p><p>pipelines.py:定义Item '
            'Pipeline的实现，实现数据的清洗，储存，验证。</p><p>settings.py：全局配置</p><p><b>3、创建一个spider（自己定义的爬虫文件）</b></p><p>例如以爬取猫眼热映口碑榜为例子来了解一下：</p><p>在spiders文件夹下创建一个maoyan.py文件，你也可以按住shift-右键-在此处打开命令窗口，输入：scrapy '
            'genspider 文件名 '
            '要爬取的网址。</p><p>自己创建的需要自己写，使用命令创建的包含最基本的东西。</p><p>我们来看一下使用命令创建的有什么。</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 573px; max-height: '
            '264px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '46.07%;"></div>\n'
            '<div class="image-view" data-width="573" data-height="264"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7" '
            'data-original-width="573" data-original-height="264" '
            'data-original-format="image/png" '
            'data-original-filesize="10489"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>介绍一下这些是干嘛的：</p><p>name：是项目的名字</p><p>allowed_domains：是允许爬取的域名，比如一些网站有相关链接，域名就和本网站不同，这些就会忽略。</p><p>atart_urls：是Spider爬取的网站，定义初始的请求url，可以多个。</p><p>parse方法：是Spider的一个方法，在请求start_url后，之后的方法，这个方法是对网页的解析，与提取自己想要的东西。</p><p>response参数：是请求网页后返回的内容，也就是你需要解析的网页。</p><p>还有其他参数有兴趣可以去查查。</p><p><b>4、定义Item</b></p><p>item是保存爬取数据的容器，使用的方法和字典差不多。</p><p>我们打开items.py，之后我们想要提取的信息有：</p><p>index(排名)、title(电影名)、star(主演)、releasetime(上映时间)、score(评分)</p><p>于是我们将items.py文件修改成这样。</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '226px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '29.82%;"></div>\n'
            '<div class="image-view" data-width="758" data-height="226"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2" '
            'data-original-width="758" data-original-height="226" '
            'data-original-format="image/png" '
            'data-original-filesize="12301"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>即可。</p><p><b>5、再次打开spider来提取我们想要的信息</b></p><p>修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '399px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '36.94%;"></div>\n'
            '<div class="image-view" data-width="1080" data-height="399"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852" '
            'data-original-width="1080" data-original-height="399" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="53873"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>好了，一个简单的爬虫就写完了。</p><p><b>6、运行</b></p><p>在该文件夹下，按住shift-右键-在此处打开命令窗口，输入：scrapy '
            'crawl maoyan(项目的名字)</p><p>即可看到：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '580px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '53.7%;"></div>\n'
            '<div class="image-view" data-width="1080" data-height="580"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2" '
            'data-original-width="1080" data-original-height="580" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="107723"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p><b>7、保存</b></p><p>我们只运行了代码，看看有没有报错，并没有保存。</p><p>如果我们想保存为csv、xml、json格式，可以直接使用命令：</p><p>在该文件夹下，按住shift-右键-在此处打开命令窗口，输入：</p><p>scrapy '
            'crawl maoyan -o maoyan.csv</p><p>scrapy crawl maoyan -o '
            'maoyan.xml</p><p>scrapy crawl maoyan -o '
            "maoyan.json</p><p>选择其中一个即可。当然如果想要保存为其他格式也是可以的，这里只说常见的。这里选择json格式，运行后会发现，在文件夹下多出来一个maoyan.json的文件。打开之后发现，中文都是一串乱码，这里需要修改编码方式，当然也可以在配置里修改</p><p>（在settings.py文件中添加FEED_EXPORT_ENCODING='UTF8'即可），</p><p>如果想直接在命令行中修改：</p><p>scrapy "
            'crawl maoyan -o maoyan.json -s '
            'FEED_EXPORT_ENCODING=UTF8</p><p>即可。</p><p>这里自己试试效果吧。</p><p>当然我们保存也可以在运行的时候自动保存，不需要自己写命令。后面介绍（我们还有还多文件没有用到呦）。</p><p><b>二、scrapy如何解析？</b></p><p>之前写过一篇文章：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483766&amp;idx=1&amp;sn=86d61115ebb7a4083e17a54f1acffdf1&amp;chksm=fb74c947cc03405122d74b1172a9a96ef35af65753666f3f7900a9318290c4326736c2d6e69e&amp;scene=21#wechat_redirect" '
            'target="_blank" '
            'rel="nofollow">三大解析库的使用</a></p><p>但是scrapy也提供了自己的解析方式（Selector），和上面的也很相似，我们来看一下：</p><p><b>1、css</b></p><p>首先需要导入模块：from '
            'scrapy import '
            "Selector</p><p>例如有这样一段html代码：</p><p>html='&lt;html&gt;&lt;head&gt;&lt;title&gt;Demo&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;div "
            "class='cla'&gt;This is "
            'Demo&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</p><p><b>1.1、首先需要构建一个Selector对象</b></p><p>sel '
            '= Selector(html)</p><p>text = '
            "sel.css('.cla::text').extract_first()</p><p>.cla表示选中上面的div节点，::text表示获取文本，这里和以前的有所不同。</p><p>extract_first()表示返回第一个元素，因为上述 "
            'sel.css(\'.cla::text\')返回的是一个列表，你也可以写成sel.css(\'.cla::text\')[0]来获取第一个元素，但是如果为空，就会报出超出最大索引的错误，不建议这样写，而使用extract_first()就不会报错，同时如果写成extract_first(\'123\')这样，如果为空就返回123</p><p><b>1.2、</b>有了选取第一个，就有选取所有：extract()表示选取所有，如果返回的是多个值，就可以是这样写。</p><p><b>1.3、</b>获取属性就是sel.css(\'.cla::sttr(\'class\')\').extract_first()表示获取class</p><p><b>1.4、</b>获取指定属性的文本：sel.css(\'div[class="cla"]::text\')</p><p><b>1.5、</b>其他写法和css的写法如出一辙。</p><p><b>1.6、</b>在scrapy中为我们提供了一个简便的写法，在上述的简单实例中，我们知道了response为请求网页的返回值。</p><p>我们可以直接写成：response.css()来解析，提取我们想要的信息。同样，下面要说的XPath也可以直接写成：</p><p>response.xpath()来解析。</p><p><b>2、Xpath</b></p><p>Xpath的使用可以看上面的文章：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483766&amp;idx=1&amp;sn=86d61115ebb7a4083e17a54f1acffdf1&amp;chksm=fb74c947cc03405122d74b1172a9a96ef35af65753666f3f7900a9318290c4326736c2d6e69e&amp;scene=21#wechat_redirect" '
            'target="_blank" '
            'rel="nofollow">三大解析库的使用</a></p><p>注意：获取的还是列表，所以还是要加上extract_first()或者extract()</p><p><b>3、正则匹配(这里用response操作)</b></p><p>例如：response.css(\'a::text\').re(\'写正则\')</p><p>这里如果response.css(\'a::text\')匹配的是多个对象，那么加上正则也是匹配符合要求的多个对象。</p><p>这里如果想要匹配第一个对象，可以把re()修改成re_first()即可。</p><p><b>注意：response不可以直接调用re(),response.xpath(\'.\').re()可以相当于达到直接使用正则的效果</b>。</p><p>正则的使用：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483727&amp;idx=1&amp;sn=1fc19af2291177ac3c1a832c47392a3f&amp;chksm=fb74c97ecc0340681bc54c97ceb7bba413da3f1ca6fc3a107bbdabe7eb9be144c261a6de347d&amp;scene=21#wechat_redirect" '
            'target="_blank" rel="nofollow">万能的正则表达式</a></p><p><b>三、Dowmloader '
            'Middleware的使用</b></p><p>本身scrapy就提供了很多Dowmloader '
            "Middleware，但是有时候我们要修改，</p><p>比如修改User-Agent，使用代理ip等。</p><p>以修改User-Agent为例（设置代理ip大同小异）：</p><p>第一种方法，可以在settings.py中直接添加USER-AGENT='xxx'</p><p>但是我们想要添加多个User-Agent，每次随机获取一个可以利用Dowmloader "
            'Middleware来设置。</p><p>第一步将settings中的USER-AGENT=\'xxx\'修改成USER-AGENT=["xxx","xxxxx","xxxxxxx"]</p><p>第二步在middlewares.py中添加：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '287px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '27.02%;"></div>\n'
            '<div class="image-view" data-width="1062" data-height="287"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f" '
            'data-original-width="1062" data-original-height="287" '
            'data-original-format="image/png" '
            'data-original-filesize="19133"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>from_crawler():通过参数crawler可以拿到配置的信息，我们的User-Agent在配置文件里，所以我们需要获取到。</p><p>方法名不可以修改。</p><p>第三步在settings.py中添加：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '110px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '12.04%;"></div>\n'
            '<div class="image-view" data-width="914" data-height="110"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602" '
            'data-original-width="914" data-original-height="110" '
            'data-original-format="image/png" '
            'data-original-filesize="5150"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>将scrapy自带的UserAgentmiddleware的键值设置为None,</p><p>自定义的设置为400，这个键值越小表示优先调用的意思。</p><p><b>四、Item '
            'Pipeline的使用。</b></p><p><b>1、进行数据的清洗</b></p><p>在一的实例中我们把评分小于等于8.5分的score修改为（不好看！），我们认为是不好看的电影，我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 678px; max-height: '
            '306px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '45.129999999999995%;"></div>\n'
            '<div class="image-view" data-width="678" data-height="306"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-23021209da207b71" '
            'data-original-width="678" data-original-height="306" '
            'data-original-format="image/png" '
            'data-original-filesize="7034"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 580px; max-height: '
            '78px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '13.450000000000001%;"></div>\n'
            '<div class="image-view" data-width="580" data-height="78"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c" '
            'data-original-width="580" data-original-height="78" '
            'data-original-format="image/png" '
            'data-original-filesize="5871"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>我们执行一下：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '460px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '44.7%;"></div>\n'
            '<div class="image-view" data-width="1029" data-height="460"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c" '
            'data-original-width="1029" data-original-height="460" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="91210"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p><b>2、储存</b></p><p><b>2.1储存为json格式</b></p><p>我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '564px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '65.58%;"></div>\n'
            '<div class="image-view" data-width="860" data-height="564"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96" '
            'data-original-width="860" data-original-height="564" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="76827"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 616px; max-height: '
            '100px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '16.23%;"></div>\n'
            '<div class="image-view" data-width="616" data-height="100"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93" '
            'data-original-width="616" data-original-height="100" '
            'data-original-format="image/png" '
            'data-original-filesize="7382"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>表示先执行TextPipeline方法，再执行JsonPipeline方法，先清洗，再储存。</p><p><b>2.2储存在mysql数据库</b></p><p>首先在mysql数据库中创建一个数据库maoyanreying，创建一个表maoyan。</p><p>我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '634px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '62.46000000000001%;"></div>\n'
            '<div class="image-view" data-width="1015" data-height="634"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3" '
            'data-original-width="1015" data-original-height="634" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="102252"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 566px; max-height: '
            '218px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '38.519999999999996%;"></div>\n'
            '<div class="image-view" data-width="566" data-height="218"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198" '
            'data-original-width="566" data-original-height="218" '
            'data-original-format="image/png" '
            'data-original-filesize="5142"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>即可</p><p>完。</p></article><div></div><div class="_19DgIp" '
            'style="margin-top:24px;margin-bottom:24px"></div></section>',
 'image_srcs': ['//upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7',
                '//upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2',
                '//upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852',
                '//upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2',
                '//upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f',
                '//upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602',
                '//upload-images.jianshu.io/upload_images/9489193-23021209da207b71',
                '//upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c',
                '//upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c',
                '//upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96',
                '//upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93',
                '//upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3',
                '//upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198'],
 'image_urls': ['https://upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7',
                'https://upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2',
                'https://upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852',
                'https://upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2',
                'https://upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f',
                'https://upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602',
                'https://upload-images.jianshu.io/upload_images/9489193-23021209da207b71',
                'https://upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c',
                'https://upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c',
                'https://upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96',
                'https://upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93',
                'https://upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3',
                'https://upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198'],
 'title': '爬虫框架Scrapy的安装与基本使用',
 'url': 'https://www.jianshu.com/p/6bc5a4641629'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\PycharmProjects\MyScrapy_jianshu\MyScrapy_jianshu\pipelines.py", line 39, in process_item
    with codecs.open(filename, 'w', 'utf-8') as f:
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\codecs.py", line 898, in open
    file = builtins.open(filename, mode, buffering)
PermissionError: [Errno 13] Permission denied: 'outputs/爬虫框架Scrapy的安装与基本使用.html'
2019-10-30 10:59:51 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-30 10:59:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 600,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 39363,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.644471,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 30, 2, 59, 51, 486387),
 'log_count/DEBUG': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 10, 30, 2, 59, 50, 841916)}
2019-10-30 10:59:51 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-30 11:01:51 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: MyScrapy_jianshu)
2019-10-30 11:01:51 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.17763-SP0
2019-10-30 11:01:51 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'MyScrapy_jianshu', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'MyScrapy_jianshu.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['MyScrapy_jianshu.spiders']}
2019-10-30 11:01:51 [scrapy.extensions.telnet] INFO: Telnet Password: 56277b9f100127fa
2019-10-30 11:01:51 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-30 11:01:52 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-30 11:01:52 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-30 11:01:52 [scrapy.middleware] INFO: Enabled item pipelines:
['MyScrapy_jianshu.pipelines.MyscrapyJianshuPipeline',
 'MyScrapy_jianshu.pipelines.MyImagesPipeline']
2019-10-30 11:01:52 [scrapy.core.engine] INFO: Spider opened
2019-10-30 11:01:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-30 11:01:52 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-30 11:01:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/robots.txt> (referer: None)
2019-10-30 11:01:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/p/6bc5a4641629> (referer: None)
2019-10-30 11:01:54 [scrapy.core.scraper] ERROR: Error processing {'content': '<section class="ouvJEz"><h1 '
            'class="_1RuRku">爬虫框架Scrapy的安装与基本使用</h1><div class="rEsl9f"><div '
            'class="s-dsoj"><span class="_3tCVn5"><i aria-label="ic-diamond" '
            'class="anticon"><svg width="1em" height="1em" fill="currentColor" '
            'aria-hidden="true" focusable="false" class=""><use '
            'xlink:href="#ic-diamond"></use></svg></i><span>1</span></span><time '
            'datetime="2018-07-20T04:25:56.000Z">2018.07.20 '
            '12:25:56</time><span>字数 2216</span><span>阅读 '
            '38664</span></div></div><article '
            'class="_2rhmJa"><blockquote><p><b>概括：上一节学习了pyspider框架，这一节我们来看一下Scrapy的强大之处。</b><b>他应该是目前python使用的最广泛的爬虫框架。</b></p></blockquote><p><b>一、简单实例，了解基本。</b></p><p><b>1、安装Scrapy框架</b></p><p>这里如果直接pip3 '
            'install scrapy可能会出错。</p><p>所以你可以先安装lxml：pip3 install '
            'lxml(已安装请忽略)。</p><p>安装pyOpenSSL：在官网下载wheel文件。</p><p>安装Twisted：在官网下载wheel文件。</p><p>安装PyWin32：在官网下载wheel文件。</p><p>下载地址：https://www.lfd.uci.edu/~gohlke/pythonlibs/</p><p>配置环境变量：将scrapy所在目录添加到系统环境变量即可。</p><p>ctrl+f搜索即可。</p><p>最后安装scrapy，pip3 '
            'install '
            'scrapy</p><p><b>2、创建一个scrapy项目</b></p><p>新创建一个目录，按住shift-右键-在此处打开命令窗口</p><p>输入：scrapy '
            'startproject '
            'tutorial即可创建一个tutorial文件夹</p><p>文件夹目录如下：</p><blockquote>\n'
            '<p>|-tutorial</p>\n'
            '<p>|-scrapy.cfg</p>\n'
            '<p>\xa0 |-__init__.py</p>\n'
            '<p>\xa0 |-items.py</p>\n'
            '<p>\xa0 |-middlewares.py</p>\n'
            '<p>\xa0 |-pipelines.py</p>\n'
            '<p>\xa0 |-settings.py</p>\n'
            '<p>\xa0 |-spiders</p>\n'
            '<p>\xa0 \xa0 |-__init__.py</p>\n'
            '</blockquote><p><b>文件的功能：</b></p><p>scrapy.cfg：配置文件</p><p>spiders：存放你Spider文件，也就是你爬取的py文件</p><p>items.py：相当于一个容器，和字典较像</p><p>middlewares.py：定义Downloader '
            'Middlewares(下载器中间件)和Spider '
            'Middlewares(蜘蛛中间件)的实现</p><p>pipelines.py:定义Item '
            'Pipeline的实现，实现数据的清洗，储存，验证。</p><p>settings.py：全局配置</p><p><b>3、创建一个spider（自己定义的爬虫文件）</b></p><p>例如以爬取猫眼热映口碑榜为例子来了解一下：</p><p>在spiders文件夹下创建一个maoyan.py文件，你也可以按住shift-右键-在此处打开命令窗口，输入：scrapy '
            'genspider 文件名 '
            '要爬取的网址。</p><p>自己创建的需要自己写，使用命令创建的包含最基本的东西。</p><p>我们来看一下使用命令创建的有什么。</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 573px; max-height: '
            '264px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '46.07%;"></div>\n'
            '<div class="image-view" data-width="573" data-height="264"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7" '
            'data-original-width="573" data-original-height="264" '
            'data-original-format="image/png" '
            'data-original-filesize="10489"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>介绍一下这些是干嘛的：</p><p>name：是项目的名字</p><p>allowed_domains：是允许爬取的域名，比如一些网站有相关链接，域名就和本网站不同，这些就会忽略。</p><p>atart_urls：是Spider爬取的网站，定义初始的请求url，可以多个。</p><p>parse方法：是Spider的一个方法，在请求start_url后，之后的方法，这个方法是对网页的解析，与提取自己想要的东西。</p><p>response参数：是请求网页后返回的内容，也就是你需要解析的网页。</p><p>还有其他参数有兴趣可以去查查。</p><p><b>4、定义Item</b></p><p>item是保存爬取数据的容器，使用的方法和字典差不多。</p><p>我们打开items.py，之后我们想要提取的信息有：</p><p>index(排名)、title(电影名)、star(主演)、releasetime(上映时间)、score(评分)</p><p>于是我们将items.py文件修改成这样。</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '226px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '29.82%;"></div>\n'
            '<div class="image-view" data-width="758" data-height="226"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2" '
            'data-original-width="758" data-original-height="226" '
            'data-original-format="image/png" '
            'data-original-filesize="12301"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>即可。</p><p><b>5、再次打开spider来提取我们想要的信息</b></p><p>修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '399px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '36.94%;"></div>\n'
            '<div class="image-view" data-width="1080" data-height="399"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852" '
            'data-original-width="1080" data-original-height="399" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="53873"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>好了，一个简单的爬虫就写完了。</p><p><b>6、运行</b></p><p>在该文件夹下，按住shift-右键-在此处打开命令窗口，输入：scrapy '
            'crawl maoyan(项目的名字)</p><p>即可看到：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '580px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '53.7%;"></div>\n'
            '<div class="image-view" data-width="1080" data-height="580"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2" '
            'data-original-width="1080" data-original-height="580" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="107723"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p><b>7、保存</b></p><p>我们只运行了代码，看看有没有报错，并没有保存。</p><p>如果我们想保存为csv、xml、json格式，可以直接使用命令：</p><p>在该文件夹下，按住shift-右键-在此处打开命令窗口，输入：</p><p>scrapy '
            'crawl maoyan -o maoyan.csv</p><p>scrapy crawl maoyan -o '
            'maoyan.xml</p><p>scrapy crawl maoyan -o '
            "maoyan.json</p><p>选择其中一个即可。当然如果想要保存为其他格式也是可以的，这里只说常见的。这里选择json格式，运行后会发现，在文件夹下多出来一个maoyan.json的文件。打开之后发现，中文都是一串乱码，这里需要修改编码方式，当然也可以在配置里修改</p><p>（在settings.py文件中添加FEED_EXPORT_ENCODING='UTF8'即可），</p><p>如果想直接在命令行中修改：</p><p>scrapy "
            'crawl maoyan -o maoyan.json -s '
            'FEED_EXPORT_ENCODING=UTF8</p><p>即可。</p><p>这里自己试试效果吧。</p><p>当然我们保存也可以在运行的时候自动保存，不需要自己写命令。后面介绍（我们还有还多文件没有用到呦）。</p><p><b>二、scrapy如何解析？</b></p><p>之前写过一篇文章：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483766&amp;idx=1&amp;sn=86d61115ebb7a4083e17a54f1acffdf1&amp;chksm=fb74c947cc03405122d74b1172a9a96ef35af65753666f3f7900a9318290c4326736c2d6e69e&amp;scene=21#wechat_redirect" '
            'target="_blank" '
            'rel="nofollow">三大解析库的使用</a></p><p>但是scrapy也提供了自己的解析方式（Selector），和上面的也很相似，我们来看一下：</p><p><b>1、css</b></p><p>首先需要导入模块：from '
            'scrapy import '
            "Selector</p><p>例如有这样一段html代码：</p><p>html='&lt;html&gt;&lt;head&gt;&lt;title&gt;Demo&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;div "
            "class='cla'&gt;This is "
            'Demo&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</p><p><b>1.1、首先需要构建一个Selector对象</b></p><p>sel '
            '= Selector(html)</p><p>text = '
            "sel.css('.cla::text').extract_first()</p><p>.cla表示选中上面的div节点，::text表示获取文本，这里和以前的有所不同。</p><p>extract_first()表示返回第一个元素，因为上述 "
            'sel.css(\'.cla::text\')返回的是一个列表，你也可以写成sel.css(\'.cla::text\')[0]来获取第一个元素，但是如果为空，就会报出超出最大索引的错误，不建议这样写，而使用extract_first()就不会报错，同时如果写成extract_first(\'123\')这样，如果为空就返回123</p><p><b>1.2、</b>有了选取第一个，就有选取所有：extract()表示选取所有，如果返回的是多个值，就可以是这样写。</p><p><b>1.3、</b>获取属性就是sel.css(\'.cla::sttr(\'class\')\').extract_first()表示获取class</p><p><b>1.4、</b>获取指定属性的文本：sel.css(\'div[class="cla"]::text\')</p><p><b>1.5、</b>其他写法和css的写法如出一辙。</p><p><b>1.6、</b>在scrapy中为我们提供了一个简便的写法，在上述的简单实例中，我们知道了response为请求网页的返回值。</p><p>我们可以直接写成：response.css()来解析，提取我们想要的信息。同样，下面要说的XPath也可以直接写成：</p><p>response.xpath()来解析。</p><p><b>2、Xpath</b></p><p>Xpath的使用可以看上面的文章：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483766&amp;idx=1&amp;sn=86d61115ebb7a4083e17a54f1acffdf1&amp;chksm=fb74c947cc03405122d74b1172a9a96ef35af65753666f3f7900a9318290c4326736c2d6e69e&amp;scene=21#wechat_redirect" '
            'target="_blank" '
            'rel="nofollow">三大解析库的使用</a></p><p>注意：获取的还是列表，所以还是要加上extract_first()或者extract()</p><p><b>3、正则匹配(这里用response操作)</b></p><p>例如：response.css(\'a::text\').re(\'写正则\')</p><p>这里如果response.css(\'a::text\')匹配的是多个对象，那么加上正则也是匹配符合要求的多个对象。</p><p>这里如果想要匹配第一个对象，可以把re()修改成re_first()即可。</p><p><b>注意：response不可以直接调用re(),response.xpath(\'.\').re()可以相当于达到直接使用正则的效果</b>。</p><p>正则的使用：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483727&amp;idx=1&amp;sn=1fc19af2291177ac3c1a832c47392a3f&amp;chksm=fb74c97ecc0340681bc54c97ceb7bba413da3f1ca6fc3a107bbdabe7eb9be144c261a6de347d&amp;scene=21#wechat_redirect" '
            'target="_blank" rel="nofollow">万能的正则表达式</a></p><p><b>三、Dowmloader '
            'Middleware的使用</b></p><p>本身scrapy就提供了很多Dowmloader '
            "Middleware，但是有时候我们要修改，</p><p>比如修改User-Agent，使用代理ip等。</p><p>以修改User-Agent为例（设置代理ip大同小异）：</p><p>第一种方法，可以在settings.py中直接添加USER-AGENT='xxx'</p><p>但是我们想要添加多个User-Agent，每次随机获取一个可以利用Dowmloader "
            'Middleware来设置。</p><p>第一步将settings中的USER-AGENT=\'xxx\'修改成USER-AGENT=["xxx","xxxxx","xxxxxxx"]</p><p>第二步在middlewares.py中添加：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '287px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '27.02%;"></div>\n'
            '<div class="image-view" data-width="1062" data-height="287"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f" '
            'data-original-width="1062" data-original-height="287" '
            'data-original-format="image/png" '
            'data-original-filesize="19133"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>from_crawler():通过参数crawler可以拿到配置的信息，我们的User-Agent在配置文件里，所以我们需要获取到。</p><p>方法名不可以修改。</p><p>第三步在settings.py中添加：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '110px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '12.04%;"></div>\n'
            '<div class="image-view" data-width="914" data-height="110"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602" '
            'data-original-width="914" data-original-height="110" '
            'data-original-format="image/png" '
            'data-original-filesize="5150"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>将scrapy自带的UserAgentmiddleware的键值设置为None,</p><p>自定义的设置为400，这个键值越小表示优先调用的意思。</p><p><b>四、Item '
            'Pipeline的使用。</b></p><p><b>1、进行数据的清洗</b></p><p>在一的实例中我们把评分小于等于8.5分的score修改为（不好看！），我们认为是不好看的电影，我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 678px; max-height: '
            '306px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '45.129999999999995%;"></div>\n'
            '<div class="image-view" data-width="678" data-height="306"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-23021209da207b71" '
            'data-original-width="678" data-original-height="306" '
            'data-original-format="image/png" '
            'data-original-filesize="7034"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 580px; max-height: '
            '78px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '13.450000000000001%;"></div>\n'
            '<div class="image-view" data-width="580" data-height="78"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c" '
            'data-original-width="580" data-original-height="78" '
            'data-original-format="image/png" '
            'data-original-filesize="5871"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>我们执行一下：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '460px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '44.7%;"></div>\n'
            '<div class="image-view" data-width="1029" data-height="460"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c" '
            'data-original-width="1029" data-original-height="460" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="91210"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p><b>2、储存</b></p><p><b>2.1储存为json格式</b></p><p>我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '564px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '65.58%;"></div>\n'
            '<div class="image-view" data-width="860" data-height="564"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96" '
            'data-original-width="860" data-original-height="564" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="76827"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 616px; max-height: '
            '100px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '16.23%;"></div>\n'
            '<div class="image-view" data-width="616" data-height="100"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93" '
            'data-original-width="616" data-original-height="100" '
            'data-original-format="image/png" '
            'data-original-filesize="7382"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>表示先执行TextPipeline方法，再执行JsonPipeline方法，先清洗，再储存。</p><p><b>2.2储存在mysql数据库</b></p><p>首先在mysql数据库中创建一个数据库maoyanreying，创建一个表maoyan。</p><p>我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '634px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '62.46000000000001%;"></div>\n'
            '<div class="image-view" data-width="1015" data-height="634"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3" '
            'data-original-width="1015" data-original-height="634" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="102252"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 566px; max-height: '
            '218px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '38.519999999999996%;"></div>\n'
            '<div class="image-view" data-width="566" data-height="218"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198" '
            'data-original-width="566" data-original-height="218" '
            'data-original-format="image/png" '
            'data-original-filesize="5142"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>即可</p><p>完。</p></article><div></div><div class="_19DgIp" '
            'style="margin-top:24px;margin-bottom:24px"></div></section>',
 'image_srcs': ['//upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7',
                '//upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2',
                '//upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852',
                '//upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2',
                '//upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f',
                '//upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602',
                '//upload-images.jianshu.io/upload_images/9489193-23021209da207b71',
                '//upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c',
                '//upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c',
                '//upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96',
                '//upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93',
                '//upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3',
                '//upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198'],
 'image_urls': ['https://upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7',
                'https://upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2',
                'https://upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852',
                'https://upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2',
                'https://upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f',
                'https://upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602',
                'https://upload-images.jianshu.io/upload_images/9489193-23021209da207b71',
                'https://upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c',
                'https://upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c',
                'https://upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96',
                'https://upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93',
                'https://upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3',
                'https://upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198'],
 'title': '爬虫框架Scrapy的安装与基本使用',
 'url': 'https://www.jianshu.com/p/6bc5a4641629'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\PycharmProjects\MyScrapy_jianshu\MyScrapy_jianshu\pipelines.py", line 40, in process_item
    with codecs.open(filename, 'w', 'utf-8') as f:
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\codecs.py", line 898, in open
    file = builtins.open(filename, mode, buffering)
PermissionError: [Errno 13] Permission denied: 'outputs/爬虫框架Scrapy的安装与基本使用.html'
2019-10-30 11:01:54 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-30 11:01:54 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 600,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 39365,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 2.132382,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 30, 3, 1, 54, 216251),
 'log_count/DEBUG': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 10, 30, 3, 1, 52, 83869)}
2019-10-30 11:01:54 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-30 11:02:59 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: MyScrapy_jianshu)
2019-10-30 11:02:59 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.17763-SP0
2019-10-30 11:02:59 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'MyScrapy_jianshu', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'MyScrapy_jianshu.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['MyScrapy_jianshu.spiders']}
2019-10-30 11:02:59 [scrapy.extensions.telnet] INFO: Telnet Password: 53d17308e12d02f9
2019-10-30 11:02:59 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-30 11:02:59 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-30 11:02:59 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-30 11:02:59 [scrapy.middleware] INFO: Enabled item pipelines:
['MyScrapy_jianshu.pipelines.MyscrapyJianshuPipeline',
 'MyScrapy_jianshu.pipelines.MyImagesPipeline']
2019-10-30 11:02:59 [scrapy.core.engine] INFO: Spider opened
2019-10-30 11:02:59 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-30 11:02:59 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-30 11:02:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/robots.txt> (referer: None)
2019-10-30 11:03:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/p/6bc5a4641629> (referer: None)
2019-10-30 11:03:00 [scrapy.core.scraper] ERROR: Error processing {'content': '<section class="ouvJEz"><h1 '
            'class="_1RuRku">爬虫框架Scrapy的安装与基本使用</h1><div class="rEsl9f"><div '
            'class="s-dsoj"><span class="_3tCVn5"><i aria-label="ic-diamond" '
            'class="anticon"><svg width="1em" height="1em" fill="currentColor" '
            'aria-hidden="true" focusable="false" class=""><use '
            'xlink:href="#ic-diamond"></use></svg></i><span>1</span></span><time '
            'datetime="2018-07-20T04:25:56.000Z">2018.07.20 '
            '12:25:56</time><span>字数 2216</span><span>阅读 '
            '38664</span></div></div><article '
            'class="_2rhmJa"><blockquote><p><b>概括：上一节学习了pyspider框架，这一节我们来看一下Scrapy的强大之处。</b><b>他应该是目前python使用的最广泛的爬虫框架。</b></p></blockquote><p><b>一、简单实例，了解基本。</b></p><p><b>1、安装Scrapy框架</b></p><p>这里如果直接pip3 '
            'install scrapy可能会出错。</p><p>所以你可以先安装lxml：pip3 install '
            'lxml(已安装请忽略)。</p><p>安装pyOpenSSL：在官网下载wheel文件。</p><p>安装Twisted：在官网下载wheel文件。</p><p>安装PyWin32：在官网下载wheel文件。</p><p>下载地址：https://www.lfd.uci.edu/~gohlke/pythonlibs/</p><p>配置环境变量：将scrapy所在目录添加到系统环境变量即可。</p><p>ctrl+f搜索即可。</p><p>最后安装scrapy，pip3 '
            'install '
            'scrapy</p><p><b>2、创建一个scrapy项目</b></p><p>新创建一个目录，按住shift-右键-在此处打开命令窗口</p><p>输入：scrapy '
            'startproject '
            'tutorial即可创建一个tutorial文件夹</p><p>文件夹目录如下：</p><blockquote>\n'
            '<p>|-tutorial</p>\n'
            '<p>|-scrapy.cfg</p>\n'
            '<p>\xa0 |-__init__.py</p>\n'
            '<p>\xa0 |-items.py</p>\n'
            '<p>\xa0 |-middlewares.py</p>\n'
            '<p>\xa0 |-pipelines.py</p>\n'
            '<p>\xa0 |-settings.py</p>\n'
            '<p>\xa0 |-spiders</p>\n'
            '<p>\xa0 \xa0 |-__init__.py</p>\n'
            '</blockquote><p><b>文件的功能：</b></p><p>scrapy.cfg：配置文件</p><p>spiders：存放你Spider文件，也就是你爬取的py文件</p><p>items.py：相当于一个容器，和字典较像</p><p>middlewares.py：定义Downloader '
            'Middlewares(下载器中间件)和Spider '
            'Middlewares(蜘蛛中间件)的实现</p><p>pipelines.py:定义Item '
            'Pipeline的实现，实现数据的清洗，储存，验证。</p><p>settings.py：全局配置</p><p><b>3、创建一个spider（自己定义的爬虫文件）</b></p><p>例如以爬取猫眼热映口碑榜为例子来了解一下：</p><p>在spiders文件夹下创建一个maoyan.py文件，你也可以按住shift-右键-在此处打开命令窗口，输入：scrapy '
            'genspider 文件名 '
            '要爬取的网址。</p><p>自己创建的需要自己写，使用命令创建的包含最基本的东西。</p><p>我们来看一下使用命令创建的有什么。</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 573px; max-height: '
            '264px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '46.07%;"></div>\n'
            '<div class="image-view" data-width="573" data-height="264"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7" '
            'data-original-width="573" data-original-height="264" '
            'data-original-format="image/png" '
            'data-original-filesize="10489"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>介绍一下这些是干嘛的：</p><p>name：是项目的名字</p><p>allowed_domains：是允许爬取的域名，比如一些网站有相关链接，域名就和本网站不同，这些就会忽略。</p><p>atart_urls：是Spider爬取的网站，定义初始的请求url，可以多个。</p><p>parse方法：是Spider的一个方法，在请求start_url后，之后的方法，这个方法是对网页的解析，与提取自己想要的东西。</p><p>response参数：是请求网页后返回的内容，也就是你需要解析的网页。</p><p>还有其他参数有兴趣可以去查查。</p><p><b>4、定义Item</b></p><p>item是保存爬取数据的容器，使用的方法和字典差不多。</p><p>我们打开items.py，之后我们想要提取的信息有：</p><p>index(排名)、title(电影名)、star(主演)、releasetime(上映时间)、score(评分)</p><p>于是我们将items.py文件修改成这样。</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '226px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '29.82%;"></div>\n'
            '<div class="image-view" data-width="758" data-height="226"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2" '
            'data-original-width="758" data-original-height="226" '
            'data-original-format="image/png" '
            'data-original-filesize="12301"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>即可。</p><p><b>5、再次打开spider来提取我们想要的信息</b></p><p>修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '399px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '36.94%;"></div>\n'
            '<div class="image-view" data-width="1080" data-height="399"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852" '
            'data-original-width="1080" data-original-height="399" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="53873"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>好了，一个简单的爬虫就写完了。</p><p><b>6、运行</b></p><p>在该文件夹下，按住shift-右键-在此处打开命令窗口，输入：scrapy '
            'crawl maoyan(项目的名字)</p><p>即可看到：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '580px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '53.7%;"></div>\n'
            '<div class="image-view" data-width="1080" data-height="580"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2" '
            'data-original-width="1080" data-original-height="580" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="107723"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p><b>7、保存</b></p><p>我们只运行了代码，看看有没有报错，并没有保存。</p><p>如果我们想保存为csv、xml、json格式，可以直接使用命令：</p><p>在该文件夹下，按住shift-右键-在此处打开命令窗口，输入：</p><p>scrapy '
            'crawl maoyan -o maoyan.csv</p><p>scrapy crawl maoyan -o '
            'maoyan.xml</p><p>scrapy crawl maoyan -o '
            "maoyan.json</p><p>选择其中一个即可。当然如果想要保存为其他格式也是可以的，这里只说常见的。这里选择json格式，运行后会发现，在文件夹下多出来一个maoyan.json的文件。打开之后发现，中文都是一串乱码，这里需要修改编码方式，当然也可以在配置里修改</p><p>（在settings.py文件中添加FEED_EXPORT_ENCODING='UTF8'即可），</p><p>如果想直接在命令行中修改：</p><p>scrapy "
            'crawl maoyan -o maoyan.json -s '
            'FEED_EXPORT_ENCODING=UTF8</p><p>即可。</p><p>这里自己试试效果吧。</p><p>当然我们保存也可以在运行的时候自动保存，不需要自己写命令。后面介绍（我们还有还多文件没有用到呦）。</p><p><b>二、scrapy如何解析？</b></p><p>之前写过一篇文章：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483766&amp;idx=1&amp;sn=86d61115ebb7a4083e17a54f1acffdf1&amp;chksm=fb74c947cc03405122d74b1172a9a96ef35af65753666f3f7900a9318290c4326736c2d6e69e&amp;scene=21#wechat_redirect" '
            'target="_blank" '
            'rel="nofollow">三大解析库的使用</a></p><p>但是scrapy也提供了自己的解析方式（Selector），和上面的也很相似，我们来看一下：</p><p><b>1、css</b></p><p>首先需要导入模块：from '
            'scrapy import '
            "Selector</p><p>例如有这样一段html代码：</p><p>html='&lt;html&gt;&lt;head&gt;&lt;title&gt;Demo&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;div "
            "class='cla'&gt;This is "
            'Demo&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</p><p><b>1.1、首先需要构建一个Selector对象</b></p><p>sel '
            '= Selector(html)</p><p>text = '
            "sel.css('.cla::text').extract_first()</p><p>.cla表示选中上面的div节点，::text表示获取文本，这里和以前的有所不同。</p><p>extract_first()表示返回第一个元素，因为上述 "
            'sel.css(\'.cla::text\')返回的是一个列表，你也可以写成sel.css(\'.cla::text\')[0]来获取第一个元素，但是如果为空，就会报出超出最大索引的错误，不建议这样写，而使用extract_first()就不会报错，同时如果写成extract_first(\'123\')这样，如果为空就返回123</p><p><b>1.2、</b>有了选取第一个，就有选取所有：extract()表示选取所有，如果返回的是多个值，就可以是这样写。</p><p><b>1.3、</b>获取属性就是sel.css(\'.cla::sttr(\'class\')\').extract_first()表示获取class</p><p><b>1.4、</b>获取指定属性的文本：sel.css(\'div[class="cla"]::text\')</p><p><b>1.5、</b>其他写法和css的写法如出一辙。</p><p><b>1.6、</b>在scrapy中为我们提供了一个简便的写法，在上述的简单实例中，我们知道了response为请求网页的返回值。</p><p>我们可以直接写成：response.css()来解析，提取我们想要的信息。同样，下面要说的XPath也可以直接写成：</p><p>response.xpath()来解析。</p><p><b>2、Xpath</b></p><p>Xpath的使用可以看上面的文章：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483766&amp;idx=1&amp;sn=86d61115ebb7a4083e17a54f1acffdf1&amp;chksm=fb74c947cc03405122d74b1172a9a96ef35af65753666f3f7900a9318290c4326736c2d6e69e&amp;scene=21#wechat_redirect" '
            'target="_blank" '
            'rel="nofollow">三大解析库的使用</a></p><p>注意：获取的还是列表，所以还是要加上extract_first()或者extract()</p><p><b>3、正则匹配(这里用response操作)</b></p><p>例如：response.css(\'a::text\').re(\'写正则\')</p><p>这里如果response.css(\'a::text\')匹配的是多个对象，那么加上正则也是匹配符合要求的多个对象。</p><p>这里如果想要匹配第一个对象，可以把re()修改成re_first()即可。</p><p><b>注意：response不可以直接调用re(),response.xpath(\'.\').re()可以相当于达到直接使用正则的效果</b>。</p><p>正则的使用：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483727&amp;idx=1&amp;sn=1fc19af2291177ac3c1a832c47392a3f&amp;chksm=fb74c97ecc0340681bc54c97ceb7bba413da3f1ca6fc3a107bbdabe7eb9be144c261a6de347d&amp;scene=21#wechat_redirect" '
            'target="_blank" rel="nofollow">万能的正则表达式</a></p><p><b>三、Dowmloader '
            'Middleware的使用</b></p><p>本身scrapy就提供了很多Dowmloader '
            "Middleware，但是有时候我们要修改，</p><p>比如修改User-Agent，使用代理ip等。</p><p>以修改User-Agent为例（设置代理ip大同小异）：</p><p>第一种方法，可以在settings.py中直接添加USER-AGENT='xxx'</p><p>但是我们想要添加多个User-Agent，每次随机获取一个可以利用Dowmloader "
            'Middleware来设置。</p><p>第一步将settings中的USER-AGENT=\'xxx\'修改成USER-AGENT=["xxx","xxxxx","xxxxxxx"]</p><p>第二步在middlewares.py中添加：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '287px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '27.02%;"></div>\n'
            '<div class="image-view" data-width="1062" data-height="287"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f" '
            'data-original-width="1062" data-original-height="287" '
            'data-original-format="image/png" '
            'data-original-filesize="19133"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>from_crawler():通过参数crawler可以拿到配置的信息，我们的User-Agent在配置文件里，所以我们需要获取到。</p><p>方法名不可以修改。</p><p>第三步在settings.py中添加：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '110px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '12.04%;"></div>\n'
            '<div class="image-view" data-width="914" data-height="110"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602" '
            'data-original-width="914" data-original-height="110" '
            'data-original-format="image/png" '
            'data-original-filesize="5150"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>将scrapy自带的UserAgentmiddleware的键值设置为None,</p><p>自定义的设置为400，这个键值越小表示优先调用的意思。</p><p><b>四、Item '
            'Pipeline的使用。</b></p><p><b>1、进行数据的清洗</b></p><p>在一的实例中我们把评分小于等于8.5分的score修改为（不好看！），我们认为是不好看的电影，我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 678px; max-height: '
            '306px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '45.129999999999995%;"></div>\n'
            '<div class="image-view" data-width="678" data-height="306"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-23021209da207b71" '
            'data-original-width="678" data-original-height="306" '
            'data-original-format="image/png" '
            'data-original-filesize="7034"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 580px; max-height: '
            '78px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '13.450000000000001%;"></div>\n'
            '<div class="image-view" data-width="580" data-height="78"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c" '
            'data-original-width="580" data-original-height="78" '
            'data-original-format="image/png" '
            'data-original-filesize="5871"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>我们执行一下：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '460px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '44.7%;"></div>\n'
            '<div class="image-view" data-width="1029" data-height="460"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c" '
            'data-original-width="1029" data-original-height="460" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="91210"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p><b>2、储存</b></p><p><b>2.1储存为json格式</b></p><p>我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '564px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '65.58%;"></div>\n'
            '<div class="image-view" data-width="860" data-height="564"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96" '
            'data-original-width="860" data-original-height="564" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="76827"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 616px; max-height: '
            '100px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '16.23%;"></div>\n'
            '<div class="image-view" data-width="616" data-height="100"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93" '
            'data-original-width="616" data-original-height="100" '
            'data-original-format="image/png" '
            'data-original-filesize="7382"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>表示先执行TextPipeline方法，再执行JsonPipeline方法，先清洗，再储存。</p><p><b>2.2储存在mysql数据库</b></p><p>首先在mysql数据库中创建一个数据库maoyanreying，创建一个表maoyan。</p><p>我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '634px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '62.46000000000001%;"></div>\n'
            '<div class="image-view" data-width="1015" data-height="634"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3" '
            'data-original-width="1015" data-original-height="634" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="102252"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 566px; max-height: '
            '218px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '38.519999999999996%;"></div>\n'
            '<div class="image-view" data-width="566" data-height="218"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198" '
            'data-original-width="566" data-original-height="218" '
            'data-original-format="image/png" '
            'data-original-filesize="5142"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>即可</p><p>完。</p></article><div></div><div class="_19DgIp" '
            'style="margin-top:24px;margin-bottom:24px"></div></section>',
 'image_srcs': ['//upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7',
                '//upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2',
                '//upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852',
                '//upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2',
                '//upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f',
                '//upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602',
                '//upload-images.jianshu.io/upload_images/9489193-23021209da207b71',
                '//upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c',
                '//upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c',
                '//upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96',
                '//upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93',
                '//upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3',
                '//upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198'],
 'image_urls': ['https://upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7',
                'https://upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2',
                'https://upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852',
                'https://upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2',
                'https://upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f',
                'https://upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602',
                'https://upload-images.jianshu.io/upload_images/9489193-23021209da207b71',
                'https://upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c',
                'https://upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c',
                'https://upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96',
                'https://upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93',
                'https://upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3',
                'https://upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198'],
 'title': '爬虫框架Scrapy的安装与基本使用',
 'url': 'https://www.jianshu.com/p/6bc5a4641629'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\PycharmProjects\MyScrapy_jianshu\MyScrapy_jianshu\pipelines.py", line 40, in process_item
    with codecs.open(filename, 'w', 'utf-8') as f:
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\codecs.py", line 898, in open
    file = builtins.open(filename, mode, buffering)
PermissionError: [Errno 13] Permission denied: 'outputs/爬虫框架Scrapy的安装与基本使用.html'
2019-10-30 11:03:00 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-30 11:03:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 600,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 39361,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 1.003727,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 30, 3, 3, 0, 553528),
 'log_count/DEBUG': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 10, 30, 3, 2, 59, 549801)}
2019-10-30 11:03:00 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-30 11:04:26 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: MyScrapy_jianshu)
2019-10-30 11:04:26 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.17763-SP0
2019-10-30 11:04:26 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'MyScrapy_jianshu', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'MyScrapy_jianshu.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['MyScrapy_jianshu.spiders']}
2019-10-30 11:04:26 [scrapy.extensions.telnet] INFO: Telnet Password: 8f95dca059a3a84a
2019-10-30 11:04:26 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-30 11:04:27 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-30 11:04:27 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-30 11:04:27 [scrapy.middleware] INFO: Enabled item pipelines:
['MyScrapy_jianshu.pipelines.MyscrapyJianshuPipeline',
 'MyScrapy_jianshu.pipelines.MyImagesPipeline']
2019-10-30 11:04:27 [scrapy.core.engine] INFO: Spider opened
2019-10-30 11:04:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-30 11:04:27 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-30 11:04:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/robots.txt> (referer: None)
2019-10-30 11:04:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/p/6bc5a4641629> (referer: None)
2019-10-30 11:04:27 [scrapy.core.scraper] ERROR: Error processing {'content': '<section class="ouvJEz"><h1 '
            'class="_1RuRku">爬虫框架Scrapy的安装与基本使用</h1><div class="rEsl9f"><div '
            'class="s-dsoj"><span class="_3tCVn5"><i aria-label="ic-diamond" '
            'class="anticon"><svg width="1em" height="1em" fill="currentColor" '
            'aria-hidden="true" focusable="false" class=""><use '
            'xlink:href="#ic-diamond"></use></svg></i><span>1</span></span><time '
            'datetime="2018-07-20T04:25:56.000Z">2018.07.20 '
            '12:25:56</time><span>字数 2216</span><span>阅读 '
            '38664</span></div></div><article '
            'class="_2rhmJa"><blockquote><p><b>概括：上一节学习了pyspider框架，这一节我们来看一下Scrapy的强大之处。</b><b>他应该是目前python使用的最广泛的爬虫框架。</b></p></blockquote><p><b>一、简单实例，了解基本。</b></p><p><b>1、安装Scrapy框架</b></p><p>这里如果直接pip3 '
            'install scrapy可能会出错。</p><p>所以你可以先安装lxml：pip3 install '
            'lxml(已安装请忽略)。</p><p>安装pyOpenSSL：在官网下载wheel文件。</p><p>安装Twisted：在官网下载wheel文件。</p><p>安装PyWin32：在官网下载wheel文件。</p><p>下载地址：https://www.lfd.uci.edu/~gohlke/pythonlibs/</p><p>配置环境变量：将scrapy所在目录添加到系统环境变量即可。</p><p>ctrl+f搜索即可。</p><p>最后安装scrapy，pip3 '
            'install '
            'scrapy</p><p><b>2、创建一个scrapy项目</b></p><p>新创建一个目录，按住shift-右键-在此处打开命令窗口</p><p>输入：scrapy '
            'startproject '
            'tutorial即可创建一个tutorial文件夹</p><p>文件夹目录如下：</p><blockquote>\n'
            '<p>|-tutorial</p>\n'
            '<p>|-scrapy.cfg</p>\n'
            '<p>\xa0 |-__init__.py</p>\n'
            '<p>\xa0 |-items.py</p>\n'
            '<p>\xa0 |-middlewares.py</p>\n'
            '<p>\xa0 |-pipelines.py</p>\n'
            '<p>\xa0 |-settings.py</p>\n'
            '<p>\xa0 |-spiders</p>\n'
            '<p>\xa0 \xa0 |-__init__.py</p>\n'
            '</blockquote><p><b>文件的功能：</b></p><p>scrapy.cfg：配置文件</p><p>spiders：存放你Spider文件，也就是你爬取的py文件</p><p>items.py：相当于一个容器，和字典较像</p><p>middlewares.py：定义Downloader '
            'Middlewares(下载器中间件)和Spider '
            'Middlewares(蜘蛛中间件)的实现</p><p>pipelines.py:定义Item '
            'Pipeline的实现，实现数据的清洗，储存，验证。</p><p>settings.py：全局配置</p><p><b>3、创建一个spider（自己定义的爬虫文件）</b></p><p>例如以爬取猫眼热映口碑榜为例子来了解一下：</p><p>在spiders文件夹下创建一个maoyan.py文件，你也可以按住shift-右键-在此处打开命令窗口，输入：scrapy '
            'genspider 文件名 '
            '要爬取的网址。</p><p>自己创建的需要自己写，使用命令创建的包含最基本的东西。</p><p>我们来看一下使用命令创建的有什么。</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 573px; max-height: '
            '264px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '46.07%;"></div>\n'
            '<div class="image-view" data-width="573" data-height="264"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7" '
            'data-original-width="573" data-original-height="264" '
            'data-original-format="image/png" '
            'data-original-filesize="10489"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>介绍一下这些是干嘛的：</p><p>name：是项目的名字</p><p>allowed_domains：是允许爬取的域名，比如一些网站有相关链接，域名就和本网站不同，这些就会忽略。</p><p>atart_urls：是Spider爬取的网站，定义初始的请求url，可以多个。</p><p>parse方法：是Spider的一个方法，在请求start_url后，之后的方法，这个方法是对网页的解析，与提取自己想要的东西。</p><p>response参数：是请求网页后返回的内容，也就是你需要解析的网页。</p><p>还有其他参数有兴趣可以去查查。</p><p><b>4、定义Item</b></p><p>item是保存爬取数据的容器，使用的方法和字典差不多。</p><p>我们打开items.py，之后我们想要提取的信息有：</p><p>index(排名)、title(电影名)、star(主演)、releasetime(上映时间)、score(评分)</p><p>于是我们将items.py文件修改成这样。</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '226px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '29.82%;"></div>\n'
            '<div class="image-view" data-width="758" data-height="226"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2" '
            'data-original-width="758" data-original-height="226" '
            'data-original-format="image/png" '
            'data-original-filesize="12301"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>即可。</p><p><b>5、再次打开spider来提取我们想要的信息</b></p><p>修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '399px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '36.94%;"></div>\n'
            '<div class="image-view" data-width="1080" data-height="399"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852" '
            'data-original-width="1080" data-original-height="399" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="53873"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>好了，一个简单的爬虫就写完了。</p><p><b>6、运行</b></p><p>在该文件夹下，按住shift-右键-在此处打开命令窗口，输入：scrapy '
            'crawl maoyan(项目的名字)</p><p>即可看到：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '580px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '53.7%;"></div>\n'
            '<div class="image-view" data-width="1080" data-height="580"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2" '
            'data-original-width="1080" data-original-height="580" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="107723"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p><b>7、保存</b></p><p>我们只运行了代码，看看有没有报错，并没有保存。</p><p>如果我们想保存为csv、xml、json格式，可以直接使用命令：</p><p>在该文件夹下，按住shift-右键-在此处打开命令窗口，输入：</p><p>scrapy '
            'crawl maoyan -o maoyan.csv</p><p>scrapy crawl maoyan -o '
            'maoyan.xml</p><p>scrapy crawl maoyan -o '
            "maoyan.json</p><p>选择其中一个即可。当然如果想要保存为其他格式也是可以的，这里只说常见的。这里选择json格式，运行后会发现，在文件夹下多出来一个maoyan.json的文件。打开之后发现，中文都是一串乱码，这里需要修改编码方式，当然也可以在配置里修改</p><p>（在settings.py文件中添加FEED_EXPORT_ENCODING='UTF8'即可），</p><p>如果想直接在命令行中修改：</p><p>scrapy "
            'crawl maoyan -o maoyan.json -s '
            'FEED_EXPORT_ENCODING=UTF8</p><p>即可。</p><p>这里自己试试效果吧。</p><p>当然我们保存也可以在运行的时候自动保存，不需要自己写命令。后面介绍（我们还有还多文件没有用到呦）。</p><p><b>二、scrapy如何解析？</b></p><p>之前写过一篇文章：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483766&amp;idx=1&amp;sn=86d61115ebb7a4083e17a54f1acffdf1&amp;chksm=fb74c947cc03405122d74b1172a9a96ef35af65753666f3f7900a9318290c4326736c2d6e69e&amp;scene=21#wechat_redirect" '
            'target="_blank" '
            'rel="nofollow">三大解析库的使用</a></p><p>但是scrapy也提供了自己的解析方式（Selector），和上面的也很相似，我们来看一下：</p><p><b>1、css</b></p><p>首先需要导入模块：from '
            'scrapy import '
            "Selector</p><p>例如有这样一段html代码：</p><p>html='&lt;html&gt;&lt;head&gt;&lt;title&gt;Demo&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;div "
            "class='cla'&gt;This is "
            'Demo&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</p><p><b>1.1、首先需要构建一个Selector对象</b></p><p>sel '
            '= Selector(html)</p><p>text = '
            "sel.css('.cla::text').extract_first()</p><p>.cla表示选中上面的div节点，::text表示获取文本，这里和以前的有所不同。</p><p>extract_first()表示返回第一个元素，因为上述 "
            'sel.css(\'.cla::text\')返回的是一个列表，你也可以写成sel.css(\'.cla::text\')[0]来获取第一个元素，但是如果为空，就会报出超出最大索引的错误，不建议这样写，而使用extract_first()就不会报错，同时如果写成extract_first(\'123\')这样，如果为空就返回123</p><p><b>1.2、</b>有了选取第一个，就有选取所有：extract()表示选取所有，如果返回的是多个值，就可以是这样写。</p><p><b>1.3、</b>获取属性就是sel.css(\'.cla::sttr(\'class\')\').extract_first()表示获取class</p><p><b>1.4、</b>获取指定属性的文本：sel.css(\'div[class="cla"]::text\')</p><p><b>1.5、</b>其他写法和css的写法如出一辙。</p><p><b>1.6、</b>在scrapy中为我们提供了一个简便的写法，在上述的简单实例中，我们知道了response为请求网页的返回值。</p><p>我们可以直接写成：response.css()来解析，提取我们想要的信息。同样，下面要说的XPath也可以直接写成：</p><p>response.xpath()来解析。</p><p><b>2、Xpath</b></p><p>Xpath的使用可以看上面的文章：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483766&amp;idx=1&amp;sn=86d61115ebb7a4083e17a54f1acffdf1&amp;chksm=fb74c947cc03405122d74b1172a9a96ef35af65753666f3f7900a9318290c4326736c2d6e69e&amp;scene=21#wechat_redirect" '
            'target="_blank" '
            'rel="nofollow">三大解析库的使用</a></p><p>注意：获取的还是列表，所以还是要加上extract_first()或者extract()</p><p><b>3、正则匹配(这里用response操作)</b></p><p>例如：response.css(\'a::text\').re(\'写正则\')</p><p>这里如果response.css(\'a::text\')匹配的是多个对象，那么加上正则也是匹配符合要求的多个对象。</p><p>这里如果想要匹配第一个对象，可以把re()修改成re_first()即可。</p><p><b>注意：response不可以直接调用re(),response.xpath(\'.\').re()可以相当于达到直接使用正则的效果</b>。</p><p>正则的使用：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483727&amp;idx=1&amp;sn=1fc19af2291177ac3c1a832c47392a3f&amp;chksm=fb74c97ecc0340681bc54c97ceb7bba413da3f1ca6fc3a107bbdabe7eb9be144c261a6de347d&amp;scene=21#wechat_redirect" '
            'target="_blank" rel="nofollow">万能的正则表达式</a></p><p><b>三、Dowmloader '
            'Middleware的使用</b></p><p>本身scrapy就提供了很多Dowmloader '
            "Middleware，但是有时候我们要修改，</p><p>比如修改User-Agent，使用代理ip等。</p><p>以修改User-Agent为例（设置代理ip大同小异）：</p><p>第一种方法，可以在settings.py中直接添加USER-AGENT='xxx'</p><p>但是我们想要添加多个User-Agent，每次随机获取一个可以利用Dowmloader "
            'Middleware来设置。</p><p>第一步将settings中的USER-AGENT=\'xxx\'修改成USER-AGENT=["xxx","xxxxx","xxxxxxx"]</p><p>第二步在middlewares.py中添加：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '287px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '27.02%;"></div>\n'
            '<div class="image-view" data-width="1062" data-height="287"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f" '
            'data-original-width="1062" data-original-height="287" '
            'data-original-format="image/png" '
            'data-original-filesize="19133"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>from_crawler():通过参数crawler可以拿到配置的信息，我们的User-Agent在配置文件里，所以我们需要获取到。</p><p>方法名不可以修改。</p><p>第三步在settings.py中添加：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '110px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '12.04%;"></div>\n'
            '<div class="image-view" data-width="914" data-height="110"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602" '
            'data-original-width="914" data-original-height="110" '
            'data-original-format="image/png" '
            'data-original-filesize="5150"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>将scrapy自带的UserAgentmiddleware的键值设置为None,</p><p>自定义的设置为400，这个键值越小表示优先调用的意思。</p><p><b>四、Item '
            'Pipeline的使用。</b></p><p><b>1、进行数据的清洗</b></p><p>在一的实例中我们把评分小于等于8.5分的score修改为（不好看！），我们认为是不好看的电影，我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 678px; max-height: '
            '306px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '45.129999999999995%;"></div>\n'
            '<div class="image-view" data-width="678" data-height="306"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-23021209da207b71" '
            'data-original-width="678" data-original-height="306" '
            'data-original-format="image/png" '
            'data-original-filesize="7034"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 580px; max-height: '
            '78px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '13.450000000000001%;"></div>\n'
            '<div class="image-view" data-width="580" data-height="78"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c" '
            'data-original-width="580" data-original-height="78" '
            'data-original-format="image/png" '
            'data-original-filesize="5871"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>我们执行一下：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '460px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '44.7%;"></div>\n'
            '<div class="image-view" data-width="1029" data-height="460"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c" '
            'data-original-width="1029" data-original-height="460" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="91210"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p><b>2、储存</b></p><p><b>2.1储存为json格式</b></p><p>我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '564px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '65.58%;"></div>\n'
            '<div class="image-view" data-width="860" data-height="564"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96" '
            'data-original-width="860" data-original-height="564" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="76827"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 616px; max-height: '
            '100px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '16.23%;"></div>\n'
            '<div class="image-view" data-width="616" data-height="100"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93" '
            'data-original-width="616" data-original-height="100" '
            'data-original-format="image/png" '
            'data-original-filesize="7382"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>表示先执行TextPipeline方法，再执行JsonPipeline方法，先清洗，再储存。</p><p><b>2.2储存在mysql数据库</b></p><p>首先在mysql数据库中创建一个数据库maoyanreying，创建一个表maoyan。</p><p>我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '634px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '62.46000000000001%;"></div>\n'
            '<div class="image-view" data-width="1015" data-height="634"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3" '
            'data-original-width="1015" data-original-height="634" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="102252"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 566px; max-height: '
            '218px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '38.519999999999996%;"></div>\n'
            '<div class="image-view" data-width="566" data-height="218"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198" '
            'data-original-width="566" data-original-height="218" '
            'data-original-format="image/png" '
            'data-original-filesize="5142"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>即可</p><p>完。</p></article><div></div><div class="_19DgIp" '
            'style="margin-top:24px;margin-bottom:24px"></div></section>',
 'image_srcs': ['//upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7',
                '//upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2',
                '//upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852',
                '//upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2',
                '//upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f',
                '//upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602',
                '//upload-images.jianshu.io/upload_images/9489193-23021209da207b71',
                '//upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c',
                '//upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c',
                '//upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96',
                '//upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93',
                '//upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3',
                '//upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198'],
 'image_urls': ['https://upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7',
                'https://upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2',
                'https://upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852',
                'https://upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2',
                'https://upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f',
                'https://upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602',
                'https://upload-images.jianshu.io/upload_images/9489193-23021209da207b71',
                'https://upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c',
                'https://upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c',
                'https://upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96',
                'https://upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93',
                'https://upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3',
                'https://upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198'],
 'title': '爬虫框架Scrapy的安装与基本使用',
 'url': 'https://www.jianshu.com/p/6bc5a4641629'}
Traceback (most recent call last):
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "C:\Users\Administrator\PycharmProjects\MyScrapy_jianshu\MyScrapy_jianshu\pipelines.py", line 39, in process_item
    with codecs.open(filename, 'w', 'utf-8') as f:
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python37\lib\codecs.py", line 898, in open
    file = builtins.open(filename, mode, buffering)
PermissionError: [Errno 13] Permission denied: 'outputs/爬虫框架Scrapy的安装与基本使用.html'
2019-10-30 11:04:27 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-30 11:04:27 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 600,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 39362,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.925864,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 30, 3, 4, 27, 985394),
 'log_count/DEBUG': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 10, 30, 3, 4, 27, 59530)}
2019-10-30 11:04:27 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-30 11:05:52 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: MyScrapy_jianshu)
2019-10-30 11:05:52 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.17763-SP0
2019-10-30 11:05:52 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'MyScrapy_jianshu', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'MyScrapy_jianshu.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['MyScrapy_jianshu.spiders']}
2019-10-30 11:05:52 [scrapy.extensions.telnet] INFO: Telnet Password: 1b50de01d0ff322f
2019-10-30 11:05:52 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-30 11:05:53 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-30 11:05:53 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-30 11:05:53 [scrapy.middleware] INFO: Enabled item pipelines:
['MyScrapy_jianshu.pipelines.MyscrapyJianshuPipeline',
 'MyScrapy_jianshu.pipelines.MyImagesPipeline']
2019-10-30 11:05:53 [scrapy.core.engine] INFO: Spider opened
2019-10-30 11:05:53 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-30 11:05:53 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-30 11:05:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/robots.txt> (referer: None)
2019-10-30 11:05:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/p/6bc5a4641629> (referer: None)
2019-10-30 11:05:53 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7> referred in <None>
2019-10-30 11:05:53 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2> referred in <None>
2019-10-30 11:05:53 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852> referred in <None>
2019-10-30 11:05:53 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2> referred in <None>
2019-10-30 11:05:53 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f> referred in <None>
2019-10-30 11:05:53 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602> referred in <None>
2019-10-30 11:05:53 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-23021209da207b71> referred in <None>
2019-10-30 11:05:53 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c> referred in <None>
2019-10-30 11:05:53 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c> referred in <None>
2019-10-30 11:05:53 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96> referred in <None>
2019-10-30 11:05:53 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93> referred in <None>
2019-10-30 11:05:53 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3> referred in <None>
2019-10-30 11:05:53 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198> referred in <None>
2019-10-30 11:05:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.jianshu.com/p/6bc5a4641629>
{'content': '<section class="ouvJEz"><h1 '
            'class="_1RuRku">爬虫框架Scrapy的安装与基本使用</h1><div class="rEsl9f"><div '
            'class="s-dsoj"><span class="_3tCVn5"><i aria-label="ic-diamond" '
            'class="anticon"><svg width="1em" height="1em" fill="currentColor" '
            'aria-hidden="true" focusable="false" class=""><use '
            'xlink:href="#ic-diamond"></use></svg></i><span>1</span></span><time '
            'datetime="2018-07-20T04:25:56.000Z">2018.07.20 '
            '12:25:56</time><span>字数 2216</span><span>阅读 '
            '38664</span></div></div><article '
            'class="_2rhmJa"><blockquote><p><b>概括：上一节学习了pyspider框架，这一节我们来看一下Scrapy的强大之处。</b><b>他应该是目前python使用的最广泛的爬虫框架。</b></p></blockquote><p><b>一、简单实例，了解基本。</b></p><p><b>1、安装Scrapy框架</b></p><p>这里如果直接pip3 '
            'install scrapy可能会出错。</p><p>所以你可以先安装lxml：pip3 install '
            'lxml(已安装请忽略)。</p><p>安装pyOpenSSL：在官网下载wheel文件。</p><p>安装Twisted：在官网下载wheel文件。</p><p>安装PyWin32：在官网下载wheel文件。</p><p>下载地址：https://www.lfd.uci.edu/~gohlke/pythonlibs/</p><p>配置环境变量：将scrapy所在目录添加到系统环境变量即可。</p><p>ctrl+f搜索即可。</p><p>最后安装scrapy，pip3 '
            'install '
            'scrapy</p><p><b>2、创建一个scrapy项目</b></p><p>新创建一个目录，按住shift-右键-在此处打开命令窗口</p><p>输入：scrapy '
            'startproject '
            'tutorial即可创建一个tutorial文件夹</p><p>文件夹目录如下：</p><blockquote>\n'
            '<p>|-tutorial</p>\n'
            '<p>|-scrapy.cfg</p>\n'
            '<p>\xa0 |-__init__.py</p>\n'
            '<p>\xa0 |-items.py</p>\n'
            '<p>\xa0 |-middlewares.py</p>\n'
            '<p>\xa0 |-pipelines.py</p>\n'
            '<p>\xa0 |-settings.py</p>\n'
            '<p>\xa0 |-spiders</p>\n'
            '<p>\xa0 \xa0 |-__init__.py</p>\n'
            '</blockquote><p><b>文件的功能：</b></p><p>scrapy.cfg：配置文件</p><p>spiders：存放你Spider文件，也就是你爬取的py文件</p><p>items.py：相当于一个容器，和字典较像</p><p>middlewares.py：定义Downloader '
            'Middlewares(下载器中间件)和Spider '
            'Middlewares(蜘蛛中间件)的实现</p><p>pipelines.py:定义Item '
            'Pipeline的实现，实现数据的清洗，储存，验证。</p><p>settings.py：全局配置</p><p><b>3、创建一个spider（自己定义的爬虫文件）</b></p><p>例如以爬取猫眼热映口碑榜为例子来了解一下：</p><p>在spiders文件夹下创建一个maoyan.py文件，你也可以按住shift-右键-在此处打开命令窗口，输入：scrapy '
            'genspider 文件名 '
            '要爬取的网址。</p><p>自己创建的需要自己写，使用命令创建的包含最基本的东西。</p><p>我们来看一下使用命令创建的有什么。</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 573px; max-height: '
            '264px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '46.07%;"></div>\n'
            '<div class="image-view" data-width="573" data-height="264"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7" '
            'data-original-width="573" data-original-height="264" '
            'data-original-format="image/png" '
            'data-original-filesize="10489"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>介绍一下这些是干嘛的：</p><p>name：是项目的名字</p><p>allowed_domains：是允许爬取的域名，比如一些网站有相关链接，域名就和本网站不同，这些就会忽略。</p><p>atart_urls：是Spider爬取的网站，定义初始的请求url，可以多个。</p><p>parse方法：是Spider的一个方法，在请求start_url后，之后的方法，这个方法是对网页的解析，与提取自己想要的东西。</p><p>response参数：是请求网页后返回的内容，也就是你需要解析的网页。</p><p>还有其他参数有兴趣可以去查查。</p><p><b>4、定义Item</b></p><p>item是保存爬取数据的容器，使用的方法和字典差不多。</p><p>我们打开items.py，之后我们想要提取的信息有：</p><p>index(排名)、title(电影名)、star(主演)、releasetime(上映时间)、score(评分)</p><p>于是我们将items.py文件修改成这样。</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '226px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '29.82%;"></div>\n'
            '<div class="image-view" data-width="758" data-height="226"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2" '
            'data-original-width="758" data-original-height="226" '
            'data-original-format="image/png" '
            'data-original-filesize="12301"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>即可。</p><p><b>5、再次打开spider来提取我们想要的信息</b></p><p>修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '399px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '36.94%;"></div>\n'
            '<div class="image-view" data-width="1080" data-height="399"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852" '
            'data-original-width="1080" data-original-height="399" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="53873"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>好了，一个简单的爬虫就写完了。</p><p><b>6、运行</b></p><p>在该文件夹下，按住shift-右键-在此处打开命令窗口，输入：scrapy '
            'crawl maoyan(项目的名字)</p><p>即可看到：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '580px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '53.7%;"></div>\n'
            '<div class="image-view" data-width="1080" data-height="580"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2" '
            'data-original-width="1080" data-original-height="580" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="107723"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p><b>7、保存</b></p><p>我们只运行了代码，看看有没有报错，并没有保存。</p><p>如果我们想保存为csv、xml、json格式，可以直接使用命令：</p><p>在该文件夹下，按住shift-右键-在此处打开命令窗口，输入：</p><p>scrapy '
            'crawl maoyan -o maoyan.csv</p><p>scrapy crawl maoyan -o '
            'maoyan.xml</p><p>scrapy crawl maoyan -o '
            "maoyan.json</p><p>选择其中一个即可。当然如果想要保存为其他格式也是可以的，这里只说常见的。这里选择json格式，运行后会发现，在文件夹下多出来一个maoyan.json的文件。打开之后发现，中文都是一串乱码，这里需要修改编码方式，当然也可以在配置里修改</p><p>（在settings.py文件中添加FEED_EXPORT_ENCODING='UTF8'即可），</p><p>如果想直接在命令行中修改：</p><p>scrapy "
            'crawl maoyan -o maoyan.json -s '
            'FEED_EXPORT_ENCODING=UTF8</p><p>即可。</p><p>这里自己试试效果吧。</p><p>当然我们保存也可以在运行的时候自动保存，不需要自己写命令。后面介绍（我们还有还多文件没有用到呦）。</p><p><b>二、scrapy如何解析？</b></p><p>之前写过一篇文章：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483766&amp;idx=1&amp;sn=86d61115ebb7a4083e17a54f1acffdf1&amp;chksm=fb74c947cc03405122d74b1172a9a96ef35af65753666f3f7900a9318290c4326736c2d6e69e&amp;scene=21#wechat_redirect" '
            'target="_blank" '
            'rel="nofollow">三大解析库的使用</a></p><p>但是scrapy也提供了自己的解析方式（Selector），和上面的也很相似，我们来看一下：</p><p><b>1、css</b></p><p>首先需要导入模块：from '
            'scrapy import '
            "Selector</p><p>例如有这样一段html代码：</p><p>html='&lt;html&gt;&lt;head&gt;&lt;title&gt;Demo&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;div "
            "class='cla'&gt;This is "
            'Demo&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</p><p><b>1.1、首先需要构建一个Selector对象</b></p><p>sel '
            '= Selector(html)</p><p>text = '
            "sel.css('.cla::text').extract_first()</p><p>.cla表示选中上面的div节点，::text表示获取文本，这里和以前的有所不同。</p><p>extract_first()表示返回第一个元素，因为上述 "
            'sel.css(\'.cla::text\')返回的是一个列表，你也可以写成sel.css(\'.cla::text\')[0]来获取第一个元素，但是如果为空，就会报出超出最大索引的错误，不建议这样写，而使用extract_first()就不会报错，同时如果写成extract_first(\'123\')这样，如果为空就返回123</p><p><b>1.2、</b>有了选取第一个，就有选取所有：extract()表示选取所有，如果返回的是多个值，就可以是这样写。</p><p><b>1.3、</b>获取属性就是sel.css(\'.cla::sttr(\'class\')\').extract_first()表示获取class</p><p><b>1.4、</b>获取指定属性的文本：sel.css(\'div[class="cla"]::text\')</p><p><b>1.5、</b>其他写法和css的写法如出一辙。</p><p><b>1.6、</b>在scrapy中为我们提供了一个简便的写法，在上述的简单实例中，我们知道了response为请求网页的返回值。</p><p>我们可以直接写成：response.css()来解析，提取我们想要的信息。同样，下面要说的XPath也可以直接写成：</p><p>response.xpath()来解析。</p><p><b>2、Xpath</b></p><p>Xpath的使用可以看上面的文章：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483766&amp;idx=1&amp;sn=86d61115ebb7a4083e17a54f1acffdf1&amp;chksm=fb74c947cc03405122d74b1172a9a96ef35af65753666f3f7900a9318290c4326736c2d6e69e&amp;scene=21#wechat_redirect" '
            'target="_blank" '
            'rel="nofollow">三大解析库的使用</a></p><p>注意：获取的还是列表，所以还是要加上extract_first()或者extract()</p><p><b>3、正则匹配(这里用response操作)</b></p><p>例如：response.css(\'a::text\').re(\'写正则\')</p><p>这里如果response.css(\'a::text\')匹配的是多个对象，那么加上正则也是匹配符合要求的多个对象。</p><p>这里如果想要匹配第一个对象，可以把re()修改成re_first()即可。</p><p><b>注意：response不可以直接调用re(),response.xpath(\'.\').re()可以相当于达到直接使用正则的效果</b>。</p><p>正则的使用：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483727&amp;idx=1&amp;sn=1fc19af2291177ac3c1a832c47392a3f&amp;chksm=fb74c97ecc0340681bc54c97ceb7bba413da3f1ca6fc3a107bbdabe7eb9be144c261a6de347d&amp;scene=21#wechat_redirect" '
            'target="_blank" rel="nofollow">万能的正则表达式</a></p><p><b>三、Dowmloader '
            'Middleware的使用</b></p><p>本身scrapy就提供了很多Dowmloader '
            "Middleware，但是有时候我们要修改，</p><p>比如修改User-Agent，使用代理ip等。</p><p>以修改User-Agent为例（设置代理ip大同小异）：</p><p>第一种方法，可以在settings.py中直接添加USER-AGENT='xxx'</p><p>但是我们想要添加多个User-Agent，每次随机获取一个可以利用Dowmloader "
            'Middleware来设置。</p><p>第一步将settings中的USER-AGENT=\'xxx\'修改成USER-AGENT=["xxx","xxxxx","xxxxxxx"]</p><p>第二步在middlewares.py中添加：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '287px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '27.02%;"></div>\n'
            '<div class="image-view" data-width="1062" data-height="287"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f" '
            'data-original-width="1062" data-original-height="287" '
            'data-original-format="image/png" '
            'data-original-filesize="19133"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>from_crawler():通过参数crawler可以拿到配置的信息，我们的User-Agent在配置文件里，所以我们需要获取到。</p><p>方法名不可以修改。</p><p>第三步在settings.py中添加：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '110px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '12.04%;"></div>\n'
            '<div class="image-view" data-width="914" data-height="110"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602" '
            'data-original-width="914" data-original-height="110" '
            'data-original-format="image/png" '
            'data-original-filesize="5150"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>将scrapy自带的UserAgentmiddleware的键值设置为None,</p><p>自定义的设置为400，这个键值越小表示优先调用的意思。</p><p><b>四、Item '
            'Pipeline的使用。</b></p><p><b>1、进行数据的清洗</b></p><p>在一的实例中我们把评分小于等于8.5分的score修改为（不好看！），我们认为是不好看的电影，我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 678px; max-height: '
            '306px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '45.129999999999995%;"></div>\n'
            '<div class="image-view" data-width="678" data-height="306"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-23021209da207b71" '
            'data-original-width="678" data-original-height="306" '
            'data-original-format="image/png" '
            'data-original-filesize="7034"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 580px; max-height: '
            '78px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '13.450000000000001%;"></div>\n'
            '<div class="image-view" data-width="580" data-height="78"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c" '
            'data-original-width="580" data-original-height="78" '
            'data-original-format="image/png" '
            'data-original-filesize="5871"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>我们执行一下：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '460px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '44.7%;"></div>\n'
            '<div class="image-view" data-width="1029" data-height="460"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c" '
            'data-original-width="1029" data-original-height="460" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="91210"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p><b>2、储存</b></p><p><b>2.1储存为json格式</b></p><p>我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '564px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '65.58%;"></div>\n'
            '<div class="image-view" data-width="860" data-height="564"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96" '
            'data-original-width="860" data-original-height="564" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="76827"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 616px; max-height: '
            '100px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '16.23%;"></div>\n'
            '<div class="image-view" data-width="616" data-height="100"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93" '
            'data-original-width="616" data-original-height="100" '
            'data-original-format="image/png" '
            'data-original-filesize="7382"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>表示先执行TextPipeline方法，再执行JsonPipeline方法，先清洗，再储存。</p><p><b>2.2储存在mysql数据库</b></p><p>首先在mysql数据库中创建一个数据库maoyanreying，创建一个表maoyan。</p><p>我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '634px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '62.46000000000001%;"></div>\n'
            '<div class="image-view" data-width="1015" data-height="634"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3" '
            'data-original-width="1015" data-original-height="634" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="102252"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 566px; max-height: '
            '218px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '38.519999999999996%;"></div>\n'
            '<div class="image-view" data-width="566" data-height="218"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198" '
            'data-original-width="566" data-original-height="218" '
            'data-original-format="image/png" '
            'data-original-filesize="5142"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>即可</p><p>完。</p></article><div></div><div class="_19DgIp" '
            'style="margin-top:24px;margin-bottom:24px"></div></section>',
 'image_srcs': ['//upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7',
                '//upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2',
                '//upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852',
                '//upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2',
                '//upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f',
                '//upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602',
                '//upload-images.jianshu.io/upload_images/9489193-23021209da207b71',
                '//upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c',
                '//upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c',
                '//upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96',
                '//upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93',
                '//upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3',
                '//upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198'],
 'image_urls': ['https://upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7',
                'https://upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2',
                'https://upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852',
                'https://upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2',
                'https://upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f',
                'https://upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602',
                'https://upload-images.jianshu.io/upload_images/9489193-23021209da207b71',
                'https://upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c',
                'https://upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c',
                'https://upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96',
                'https://upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93',
                'https://upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3',
                'https://upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198'],
 'title': '爬虫框架Scrapy的安装与基本使用',
 'url': 'https://www.jianshu.com/p/6bc5a4641629'}
2019-10-30 11:05:53 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-30 11:05:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 600,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 39361,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.850937,
 'file_count': 13,
 'file_status_count/uptodate': 13,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 30, 3, 5, 53, 880658),
 'item_scraped_count': 1,
 'log_count/DEBUG': 16,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 10, 30, 3, 5, 53, 29721)}
2019-10-30 11:05:53 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-30 11:07:23 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: MyScrapy_jianshu)
2019-10-30 11:07:23 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.17763-SP0
2019-10-30 11:07:23 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'MyScrapy_jianshu', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'MyScrapy_jianshu.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['MyScrapy_jianshu.spiders']}
2019-10-30 11:07:23 [scrapy.extensions.telnet] INFO: Telnet Password: 9ab370c78da3b769
2019-10-30 11:07:23 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-30 11:07:24 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-30 11:07:24 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-30 11:07:24 [scrapy.middleware] INFO: Enabled item pipelines:
['MyScrapy_jianshu.pipelines.MyscrapyJianshuPipeline',
 'MyScrapy_jianshu.pipelines.MyImagesPipeline']
2019-10-30 11:07:24 [scrapy.core.engine] INFO: Spider opened
2019-10-30 11:07:24 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-30 11:07:24 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-30 11:07:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/robots.txt> (referer: None)
2019-10-30 11:07:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/p/6bc5a4641629> (referer: None)
2019-10-30 11:07:25 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7> referred in <None>
2019-10-30 11:07:25 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2> referred in <None>
2019-10-30 11:07:25 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852> referred in <None>
2019-10-30 11:07:25 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2> referred in <None>
2019-10-30 11:07:25 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f> referred in <None>
2019-10-30 11:07:25 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602> referred in <None>
2019-10-30 11:07:25 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-23021209da207b71> referred in <None>
2019-10-30 11:07:25 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c> referred in <None>
2019-10-30 11:07:25 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c> referred in <None>
2019-10-30 11:07:25 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96> referred in <None>
2019-10-30 11:07:25 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93> referred in <None>
2019-10-30 11:07:25 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3> referred in <None>
2019-10-30 11:07:25 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198> referred in <None>
2019-10-30 11:07:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.jianshu.com/p/6bc5a4641629>
{'content': '<section class="ouvJEz"><h1 '
            'class="_1RuRku">爬虫框架Scrapy的安装与基本使用</h1><div class="rEsl9f"><div '
            'class="s-dsoj"><span class="_3tCVn5"><i aria-label="ic-diamond" '
            'class="anticon"><svg width="1em" height="1em" fill="currentColor" '
            'aria-hidden="true" focusable="false" class=""><use '
            'xlink:href="#ic-diamond"></use></svg></i><span>1</span></span><time '
            'datetime="2018-07-20T04:25:56.000Z">2018.07.20 '
            '12:25:56</time><span>字数 2216</span><span>阅读 '
            '38667</span></div></div><article '
            'class="_2rhmJa"><blockquote><p><b>概括：上一节学习了pyspider框架，这一节我们来看一下Scrapy的强大之处。</b><b>他应该是目前python使用的最广泛的爬虫框架。</b></p></blockquote><p><b>一、简单实例，了解基本。</b></p><p><b>1、安装Scrapy框架</b></p><p>这里如果直接pip3 '
            'install scrapy可能会出错。</p><p>所以你可以先安装lxml：pip3 install '
            'lxml(已安装请忽略)。</p><p>安装pyOpenSSL：在官网下载wheel文件。</p><p>安装Twisted：在官网下载wheel文件。</p><p>安装PyWin32：在官网下载wheel文件。</p><p>下载地址：https://www.lfd.uci.edu/~gohlke/pythonlibs/</p><p>配置环境变量：将scrapy所在目录添加到系统环境变量即可。</p><p>ctrl+f搜索即可。</p><p>最后安装scrapy，pip3 '
            'install '
            'scrapy</p><p><b>2、创建一个scrapy项目</b></p><p>新创建一个目录，按住shift-右键-在此处打开命令窗口</p><p>输入：scrapy '
            'startproject '
            'tutorial即可创建一个tutorial文件夹</p><p>文件夹目录如下：</p><blockquote>\n'
            '<p>|-tutorial</p>\n'
            '<p>|-scrapy.cfg</p>\n'
            '<p>\xa0 |-__init__.py</p>\n'
            '<p>\xa0 |-items.py</p>\n'
            '<p>\xa0 |-middlewares.py</p>\n'
            '<p>\xa0 |-pipelines.py</p>\n'
            '<p>\xa0 |-settings.py</p>\n'
            '<p>\xa0 |-spiders</p>\n'
            '<p>\xa0 \xa0 |-__init__.py</p>\n'
            '</blockquote><p><b>文件的功能：</b></p><p>scrapy.cfg：配置文件</p><p>spiders：存放你Spider文件，也就是你爬取的py文件</p><p>items.py：相当于一个容器，和字典较像</p><p>middlewares.py：定义Downloader '
            'Middlewares(下载器中间件)和Spider '
            'Middlewares(蜘蛛中间件)的实现</p><p>pipelines.py:定义Item '
            'Pipeline的实现，实现数据的清洗，储存，验证。</p><p>settings.py：全局配置</p><p><b>3、创建一个spider（自己定义的爬虫文件）</b></p><p>例如以爬取猫眼热映口碑榜为例子来了解一下：</p><p>在spiders文件夹下创建一个maoyan.py文件，你也可以按住shift-右键-在此处打开命令窗口，输入：scrapy '
            'genspider 文件名 '
            '要爬取的网址。</p><p>自己创建的需要自己写，使用命令创建的包含最基本的东西。</p><p>我们来看一下使用命令创建的有什么。</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 573px; max-height: '
            '264px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '46.07%;"></div>\n'
            '<div class="image-view" data-width="573" data-height="264"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7" '
            'data-original-width="573" data-original-height="264" '
            'data-original-format="image/png" '
            'data-original-filesize="10489"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>介绍一下这些是干嘛的：</p><p>name：是项目的名字</p><p>allowed_domains：是允许爬取的域名，比如一些网站有相关链接，域名就和本网站不同，这些就会忽略。</p><p>atart_urls：是Spider爬取的网站，定义初始的请求url，可以多个。</p><p>parse方法：是Spider的一个方法，在请求start_url后，之后的方法，这个方法是对网页的解析，与提取自己想要的东西。</p><p>response参数：是请求网页后返回的内容，也就是你需要解析的网页。</p><p>还有其他参数有兴趣可以去查查。</p><p><b>4、定义Item</b></p><p>item是保存爬取数据的容器，使用的方法和字典差不多。</p><p>我们打开items.py，之后我们想要提取的信息有：</p><p>index(排名)、title(电影名)、star(主演)、releasetime(上映时间)、score(评分)</p><p>于是我们将items.py文件修改成这样。</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '226px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '29.82%;"></div>\n'
            '<div class="image-view" data-width="758" data-height="226"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2" '
            'data-original-width="758" data-original-height="226" '
            'data-original-format="image/png" '
            'data-original-filesize="12301"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>即可。</p><p><b>5、再次打开spider来提取我们想要的信息</b></p><p>修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '399px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '36.94%;"></div>\n'
            '<div class="image-view" data-width="1080" data-height="399"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852" '
            'data-original-width="1080" data-original-height="399" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="53873"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>好了，一个简单的爬虫就写完了。</p><p><b>6、运行</b></p><p>在该文件夹下，按住shift-右键-在此处打开命令窗口，输入：scrapy '
            'crawl maoyan(项目的名字)</p><p>即可看到：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '580px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '53.7%;"></div>\n'
            '<div class="image-view" data-width="1080" data-height="580"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2" '
            'data-original-width="1080" data-original-height="580" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="107723"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p><b>7、保存</b></p><p>我们只运行了代码，看看有没有报错，并没有保存。</p><p>如果我们想保存为csv、xml、json格式，可以直接使用命令：</p><p>在该文件夹下，按住shift-右键-在此处打开命令窗口，输入：</p><p>scrapy '
            'crawl maoyan -o maoyan.csv</p><p>scrapy crawl maoyan -o '
            'maoyan.xml</p><p>scrapy crawl maoyan -o '
            "maoyan.json</p><p>选择其中一个即可。当然如果想要保存为其他格式也是可以的，这里只说常见的。这里选择json格式，运行后会发现，在文件夹下多出来一个maoyan.json的文件。打开之后发现，中文都是一串乱码，这里需要修改编码方式，当然也可以在配置里修改</p><p>（在settings.py文件中添加FEED_EXPORT_ENCODING='UTF8'即可），</p><p>如果想直接在命令行中修改：</p><p>scrapy "
            'crawl maoyan -o maoyan.json -s '
            'FEED_EXPORT_ENCODING=UTF8</p><p>即可。</p><p>这里自己试试效果吧。</p><p>当然我们保存也可以在运行的时候自动保存，不需要自己写命令。后面介绍（我们还有还多文件没有用到呦）。</p><p><b>二、scrapy如何解析？</b></p><p>之前写过一篇文章：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483766&amp;idx=1&amp;sn=86d61115ebb7a4083e17a54f1acffdf1&amp;chksm=fb74c947cc03405122d74b1172a9a96ef35af65753666f3f7900a9318290c4326736c2d6e69e&amp;scene=21#wechat_redirect" '
            'target="_blank" '
            'rel="nofollow">三大解析库的使用</a></p><p>但是scrapy也提供了自己的解析方式（Selector），和上面的也很相似，我们来看一下：</p><p><b>1、css</b></p><p>首先需要导入模块：from '
            'scrapy import '
            "Selector</p><p>例如有这样一段html代码：</p><p>html='&lt;html&gt;&lt;head&gt;&lt;title&gt;Demo&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;div "
            "class='cla'&gt;This is "
            'Demo&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</p><p><b>1.1、首先需要构建一个Selector对象</b></p><p>sel '
            '= Selector(html)</p><p>text = '
            "sel.css('.cla::text').extract_first()</p><p>.cla表示选中上面的div节点，::text表示获取文本，这里和以前的有所不同。</p><p>extract_first()表示返回第一个元素，因为上述 "
            'sel.css(\'.cla::text\')返回的是一个列表，你也可以写成sel.css(\'.cla::text\')[0]来获取第一个元素，但是如果为空，就会报出超出最大索引的错误，不建议这样写，而使用extract_first()就不会报错，同时如果写成extract_first(\'123\')这样，如果为空就返回123</p><p><b>1.2、</b>有了选取第一个，就有选取所有：extract()表示选取所有，如果返回的是多个值，就可以是这样写。</p><p><b>1.3、</b>获取属性就是sel.css(\'.cla::sttr(\'class\')\').extract_first()表示获取class</p><p><b>1.4、</b>获取指定属性的文本：sel.css(\'div[class="cla"]::text\')</p><p><b>1.5、</b>其他写法和css的写法如出一辙。</p><p><b>1.6、</b>在scrapy中为我们提供了一个简便的写法，在上述的简单实例中，我们知道了response为请求网页的返回值。</p><p>我们可以直接写成：response.css()来解析，提取我们想要的信息。同样，下面要说的XPath也可以直接写成：</p><p>response.xpath()来解析。</p><p><b>2、Xpath</b></p><p>Xpath的使用可以看上面的文章：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483766&amp;idx=1&amp;sn=86d61115ebb7a4083e17a54f1acffdf1&amp;chksm=fb74c947cc03405122d74b1172a9a96ef35af65753666f3f7900a9318290c4326736c2d6e69e&amp;scene=21#wechat_redirect" '
            'target="_blank" '
            'rel="nofollow">三大解析库的使用</a></p><p>注意：获取的还是列表，所以还是要加上extract_first()或者extract()</p><p><b>3、正则匹配(这里用response操作)</b></p><p>例如：response.css(\'a::text\').re(\'写正则\')</p><p>这里如果response.css(\'a::text\')匹配的是多个对象，那么加上正则也是匹配符合要求的多个对象。</p><p>这里如果想要匹配第一个对象，可以把re()修改成re_first()即可。</p><p><b>注意：response不可以直接调用re(),response.xpath(\'.\').re()可以相当于达到直接使用正则的效果</b>。</p><p>正则的使用：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483727&amp;idx=1&amp;sn=1fc19af2291177ac3c1a832c47392a3f&amp;chksm=fb74c97ecc0340681bc54c97ceb7bba413da3f1ca6fc3a107bbdabe7eb9be144c261a6de347d&amp;scene=21#wechat_redirect" '
            'target="_blank" rel="nofollow">万能的正则表达式</a></p><p><b>三、Dowmloader '
            'Middleware的使用</b></p><p>本身scrapy就提供了很多Dowmloader '
            "Middleware，但是有时候我们要修改，</p><p>比如修改User-Agent，使用代理ip等。</p><p>以修改User-Agent为例（设置代理ip大同小异）：</p><p>第一种方法，可以在settings.py中直接添加USER-AGENT='xxx'</p><p>但是我们想要添加多个User-Agent，每次随机获取一个可以利用Dowmloader "
            'Middleware来设置。</p><p>第一步将settings中的USER-AGENT=\'xxx\'修改成USER-AGENT=["xxx","xxxxx","xxxxxxx"]</p><p>第二步在middlewares.py中添加：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '287px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '27.02%;"></div>\n'
            '<div class="image-view" data-width="1062" data-height="287"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f" '
            'data-original-width="1062" data-original-height="287" '
            'data-original-format="image/png" '
            'data-original-filesize="19133"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>from_crawler():通过参数crawler可以拿到配置的信息，我们的User-Agent在配置文件里，所以我们需要获取到。</p><p>方法名不可以修改。</p><p>第三步在settings.py中添加：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '110px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '12.04%;"></div>\n'
            '<div class="image-view" data-width="914" data-height="110"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602" '
            'data-original-width="914" data-original-height="110" '
            'data-original-format="image/png" '
            'data-original-filesize="5150"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>将scrapy自带的UserAgentmiddleware的键值设置为None,</p><p>自定义的设置为400，这个键值越小表示优先调用的意思。</p><p><b>四、Item '
            'Pipeline的使用。</b></p><p><b>1、进行数据的清洗</b></p><p>在一的实例中我们把评分小于等于8.5分的score修改为（不好看！），我们认为是不好看的电影，我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 678px; max-height: '
            '306px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '45.129999999999995%;"></div>\n'
            '<div class="image-view" data-width="678" data-height="306"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-23021209da207b71" '
            'data-original-width="678" data-original-height="306" '
            'data-original-format="image/png" '
            'data-original-filesize="7034"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 580px; max-height: '
            '78px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '13.450000000000001%;"></div>\n'
            '<div class="image-view" data-width="580" data-height="78"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c" '
            'data-original-width="580" data-original-height="78" '
            'data-original-format="image/png" '
            'data-original-filesize="5871"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>我们执行一下：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '460px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '44.7%;"></div>\n'
            '<div class="image-view" data-width="1029" data-height="460"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c" '
            'data-original-width="1029" data-original-height="460" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="91210"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p><b>2、储存</b></p><p><b>2.1储存为json格式</b></p><p>我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '564px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '65.58%;"></div>\n'
            '<div class="image-view" data-width="860" data-height="564"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96" '
            'data-original-width="860" data-original-height="564" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="76827"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 616px; max-height: '
            '100px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '16.23%;"></div>\n'
            '<div class="image-view" data-width="616" data-height="100"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93" '
            'data-original-width="616" data-original-height="100" '
            'data-original-format="image/png" '
            'data-original-filesize="7382"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>表示先执行TextPipeline方法，再执行JsonPipeline方法，先清洗，再储存。</p><p><b>2.2储存在mysql数据库</b></p><p>首先在mysql数据库中创建一个数据库maoyanreying，创建一个表maoyan。</p><p>我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '634px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '62.46000000000001%;"></div>\n'
            '<div class="image-view" data-width="1015" data-height="634"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3" '
            'data-original-width="1015" data-original-height="634" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="102252"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 566px; max-height: '
            '218px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '38.519999999999996%;"></div>\n'
            '<div class="image-view" data-width="566" data-height="218"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198" '
            'data-original-width="566" data-original-height="218" '
            'data-original-format="image/png" '
            'data-original-filesize="5142"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>即可</p><p>完。</p></article><div></div><div class="_19DgIp" '
            'style="margin-top:24px;margin-bottom:24px"></div></section>',
 'image_srcs': ['//upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7',
                '//upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2',
                '//upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852',
                '//upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2',
                '//upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f',
                '//upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602',
                '//upload-images.jianshu.io/upload_images/9489193-23021209da207b71',
                '//upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c',
                '//upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c',
                '//upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96',
                '//upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93',
                '//upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3',
                '//upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198'],
 'image_urls': ['https://upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7',
                'https://upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2',
                'https://upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852',
                'https://upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2',
                'https://upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f',
                'https://upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602',
                'https://upload-images.jianshu.io/upload_images/9489193-23021209da207b71',
                'https://upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c',
                'https://upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c',
                'https://upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96',
                'https://upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93',
                'https://upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3',
                'https://upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198'],
 'title': '爬虫框架Scrapy的安装与基本使用',
 'url': 'https://www.jianshu.com/p/6bc5a4641629'}
2019-10-30 11:07:25 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-30 11:07:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 600,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 39366,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 1.364734,
 'file_count': 13,
 'file_status_count/uptodate': 13,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 30, 3, 7, 25, 551989),
 'item_scraped_count': 1,
 'log_count/DEBUG': 16,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 10, 30, 3, 7, 24, 187255)}
2019-10-30 11:07:25 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-30 11:12:34 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: MyScrapy_jianshu)
2019-10-30 11:12:34 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.17763-SP0
2019-10-30 11:12:34 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'MyScrapy_jianshu', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'MyScrapy_jianshu.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['MyScrapy_jianshu.spiders']}
2019-10-30 11:12:34 [scrapy.extensions.telnet] INFO: Telnet Password: eaf6a3470e4a3115
2019-10-30 11:12:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-30 11:12:34 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-30 11:12:34 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-30 11:12:34 [scrapy.middleware] INFO: Enabled item pipelines:
['MyScrapy_jianshu.pipelines.MyscrapyJianshuPipeline',
 'MyScrapy_jianshu.pipelines.MyImagesPipeline']
2019-10-30 11:12:34 [scrapy.core.engine] INFO: Spider opened
2019-10-30 11:12:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-30 11:12:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-30 11:12:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/robots.txt> (referer: None)
2019-10-30 11:12:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/p/6bc5a4641629> (referer: None)
2019-10-30 11:12:35 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7> referred in <None>
2019-10-30 11:12:35 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2> referred in <None>
2019-10-30 11:12:35 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852> referred in <None>
2019-10-30 11:12:35 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2> referred in <None>
2019-10-30 11:12:35 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f> referred in <None>
2019-10-30 11:12:35 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602> referred in <None>
2019-10-30 11:12:35 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-23021209da207b71> referred in <None>
2019-10-30 11:12:35 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c> referred in <None>
2019-10-30 11:12:35 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c> referred in <None>
2019-10-30 11:12:35 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96> referred in <None>
2019-10-30 11:12:35 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93> referred in <None>
2019-10-30 11:12:35 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3> referred in <None>
2019-10-30 11:12:35 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198> referred in <None>
2019-10-30 11:12:35 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.jianshu.com/p/6bc5a4641629>
{'content': '<section class="ouvJEz"><h1 '
            'class="_1RuRku">爬虫框架Scrapy的安装与基本使用</h1><div class="rEsl9f"><div '
            'class="s-dsoj"><span class="_3tCVn5"><i aria-label="ic-diamond" '
            'class="anticon"><svg width="1em" height="1em" fill="currentColor" '
            'aria-hidden="true" focusable="false" class=""><use '
            'xlink:href="#ic-diamond"></use></svg></i><span>1</span></span><time '
            'datetime="2018-07-20T04:25:56.000Z">2018.07.20 '
            '12:25:56</time><span>字数 2216</span><span>阅读 '
            '38671</span></div></div><article '
            'class="_2rhmJa"><blockquote><p><b>概括：上一节学习了pyspider框架，这一节我们来看一下Scrapy的强大之处。</b><b>他应该是目前python使用的最广泛的爬虫框架。</b></p></blockquote><p><b>一、简单实例，了解基本。</b></p><p><b>1、安装Scrapy框架</b></p><p>这里如果直接pip3 '
            'install scrapy可能会出错。</p><p>所以你可以先安装lxml：pip3 install '
            'lxml(已安装请忽略)。</p><p>安装pyOpenSSL：在官网下载wheel文件。</p><p>安装Twisted：在官网下载wheel文件。</p><p>安装PyWin32：在官网下载wheel文件。</p><p>下载地址：https://www.lfd.uci.edu/~gohlke/pythonlibs/</p><p>配置环境变量：将scrapy所在目录添加到系统环境变量即可。</p><p>ctrl+f搜索即可。</p><p>最后安装scrapy，pip3 '
            'install '
            'scrapy</p><p><b>2、创建一个scrapy项目</b></p><p>新创建一个目录，按住shift-右键-在此处打开命令窗口</p><p>输入：scrapy '
            'startproject '
            'tutorial即可创建一个tutorial文件夹</p><p>文件夹目录如下：</p><blockquote>\n'
            '<p>|-tutorial</p>\n'
            '<p>|-scrapy.cfg</p>\n'
            '<p>\xa0 |-__init__.py</p>\n'
            '<p>\xa0 |-items.py</p>\n'
            '<p>\xa0 |-middlewares.py</p>\n'
            '<p>\xa0 |-pipelines.py</p>\n'
            '<p>\xa0 |-settings.py</p>\n'
            '<p>\xa0 |-spiders</p>\n'
            '<p>\xa0 \xa0 |-__init__.py</p>\n'
            '</blockquote><p><b>文件的功能：</b></p><p>scrapy.cfg：配置文件</p><p>spiders：存放你Spider文件，也就是你爬取的py文件</p><p>items.py：相当于一个容器，和字典较像</p><p>middlewares.py：定义Downloader '
            'Middlewares(下载器中间件)和Spider '
            'Middlewares(蜘蛛中间件)的实现</p><p>pipelines.py:定义Item '
            'Pipeline的实现，实现数据的清洗，储存，验证。</p><p>settings.py：全局配置</p><p><b>3、创建一个spider（自己定义的爬虫文件）</b></p><p>例如以爬取猫眼热映口碑榜为例子来了解一下：</p><p>在spiders文件夹下创建一个maoyan.py文件，你也可以按住shift-右键-在此处打开命令窗口，输入：scrapy '
            'genspider 文件名 '
            '要爬取的网址。</p><p>自己创建的需要自己写，使用命令创建的包含最基本的东西。</p><p>我们来看一下使用命令创建的有什么。</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 573px; max-height: '
            '264px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '46.07%;"></div>\n'
            '<div class="image-view" data-width="573" data-height="264"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7" '
            'data-original-width="573" data-original-height="264" '
            'data-original-format="image/png" '
            'data-original-filesize="10489"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>介绍一下这些是干嘛的：</p><p>name：是项目的名字</p><p>allowed_domains：是允许爬取的域名，比如一些网站有相关链接，域名就和本网站不同，这些就会忽略。</p><p>atart_urls：是Spider爬取的网站，定义初始的请求url，可以多个。</p><p>parse方法：是Spider的一个方法，在请求start_url后，之后的方法，这个方法是对网页的解析，与提取自己想要的东西。</p><p>response参数：是请求网页后返回的内容，也就是你需要解析的网页。</p><p>还有其他参数有兴趣可以去查查。</p><p><b>4、定义Item</b></p><p>item是保存爬取数据的容器，使用的方法和字典差不多。</p><p>我们打开items.py，之后我们想要提取的信息有：</p><p>index(排名)、title(电影名)、star(主演)、releasetime(上映时间)、score(评分)</p><p>于是我们将items.py文件修改成这样。</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '226px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '29.82%;"></div>\n'
            '<div class="image-view" data-width="758" data-height="226"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2" '
            'data-original-width="758" data-original-height="226" '
            'data-original-format="image/png" '
            'data-original-filesize="12301"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>即可。</p><p><b>5、再次打开spider来提取我们想要的信息</b></p><p>修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '399px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '36.94%;"></div>\n'
            '<div class="image-view" data-width="1080" data-height="399"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852" '
            'data-original-width="1080" data-original-height="399" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="53873"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>好了，一个简单的爬虫就写完了。</p><p><b>6、运行</b></p><p>在该文件夹下，按住shift-右键-在此处打开命令窗口，输入：scrapy '
            'crawl maoyan(项目的名字)</p><p>即可看到：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '580px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '53.7%;"></div>\n'
            '<div class="image-view" data-width="1080" data-height="580"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2" '
            'data-original-width="1080" data-original-height="580" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="107723"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p><b>7、保存</b></p><p>我们只运行了代码，看看有没有报错，并没有保存。</p><p>如果我们想保存为csv、xml、json格式，可以直接使用命令：</p><p>在该文件夹下，按住shift-右键-在此处打开命令窗口，输入：</p><p>scrapy '
            'crawl maoyan -o maoyan.csv</p><p>scrapy crawl maoyan -o '
            'maoyan.xml</p><p>scrapy crawl maoyan -o '
            "maoyan.json</p><p>选择其中一个即可。当然如果想要保存为其他格式也是可以的，这里只说常见的。这里选择json格式，运行后会发现，在文件夹下多出来一个maoyan.json的文件。打开之后发现，中文都是一串乱码，这里需要修改编码方式，当然也可以在配置里修改</p><p>（在settings.py文件中添加FEED_EXPORT_ENCODING='UTF8'即可），</p><p>如果想直接在命令行中修改：</p><p>scrapy "
            'crawl maoyan -o maoyan.json -s '
            'FEED_EXPORT_ENCODING=UTF8</p><p>即可。</p><p>这里自己试试效果吧。</p><p>当然我们保存也可以在运行的时候自动保存，不需要自己写命令。后面介绍（我们还有还多文件没有用到呦）。</p><p><b>二、scrapy如何解析？</b></p><p>之前写过一篇文章：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483766&amp;idx=1&amp;sn=86d61115ebb7a4083e17a54f1acffdf1&amp;chksm=fb74c947cc03405122d74b1172a9a96ef35af65753666f3f7900a9318290c4326736c2d6e69e&amp;scene=21#wechat_redirect" '
            'target="_blank" '
            'rel="nofollow">三大解析库的使用</a></p><p>但是scrapy也提供了自己的解析方式（Selector），和上面的也很相似，我们来看一下：</p><p><b>1、css</b></p><p>首先需要导入模块：from '
            'scrapy import '
            "Selector</p><p>例如有这样一段html代码：</p><p>html='&lt;html&gt;&lt;head&gt;&lt;title&gt;Demo&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;div "
            "class='cla'&gt;This is "
            'Demo&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</p><p><b>1.1、首先需要构建一个Selector对象</b></p><p>sel '
            '= Selector(html)</p><p>text = '
            "sel.css('.cla::text').extract_first()</p><p>.cla表示选中上面的div节点，::text表示获取文本，这里和以前的有所不同。</p><p>extract_first()表示返回第一个元素，因为上述 "
            'sel.css(\'.cla::text\')返回的是一个列表，你也可以写成sel.css(\'.cla::text\')[0]来获取第一个元素，但是如果为空，就会报出超出最大索引的错误，不建议这样写，而使用extract_first()就不会报错，同时如果写成extract_first(\'123\')这样，如果为空就返回123</p><p><b>1.2、</b>有了选取第一个，就有选取所有：extract()表示选取所有，如果返回的是多个值，就可以是这样写。</p><p><b>1.3、</b>获取属性就是sel.css(\'.cla::sttr(\'class\')\').extract_first()表示获取class</p><p><b>1.4、</b>获取指定属性的文本：sel.css(\'div[class="cla"]::text\')</p><p><b>1.5、</b>其他写法和css的写法如出一辙。</p><p><b>1.6、</b>在scrapy中为我们提供了一个简便的写法，在上述的简单实例中，我们知道了response为请求网页的返回值。</p><p>我们可以直接写成：response.css()来解析，提取我们想要的信息。同样，下面要说的XPath也可以直接写成：</p><p>response.xpath()来解析。</p><p><b>2、Xpath</b></p><p>Xpath的使用可以看上面的文章：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483766&amp;idx=1&amp;sn=86d61115ebb7a4083e17a54f1acffdf1&amp;chksm=fb74c947cc03405122d74b1172a9a96ef35af65753666f3f7900a9318290c4326736c2d6e69e&amp;scene=21#wechat_redirect" '
            'target="_blank" '
            'rel="nofollow">三大解析库的使用</a></p><p>注意：获取的还是列表，所以还是要加上extract_first()或者extract()</p><p><b>3、正则匹配(这里用response操作)</b></p><p>例如：response.css(\'a::text\').re(\'写正则\')</p><p>这里如果response.css(\'a::text\')匹配的是多个对象，那么加上正则也是匹配符合要求的多个对象。</p><p>这里如果想要匹配第一个对象，可以把re()修改成re_first()即可。</p><p><b>注意：response不可以直接调用re(),response.xpath(\'.\').re()可以相当于达到直接使用正则的效果</b>。</p><p>正则的使用：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483727&amp;idx=1&amp;sn=1fc19af2291177ac3c1a832c47392a3f&amp;chksm=fb74c97ecc0340681bc54c97ceb7bba413da3f1ca6fc3a107bbdabe7eb9be144c261a6de347d&amp;scene=21#wechat_redirect" '
            'target="_blank" rel="nofollow">万能的正则表达式</a></p><p><b>三、Dowmloader '
            'Middleware的使用</b></p><p>本身scrapy就提供了很多Dowmloader '
            "Middleware，但是有时候我们要修改，</p><p>比如修改User-Agent，使用代理ip等。</p><p>以修改User-Agent为例（设置代理ip大同小异）：</p><p>第一种方法，可以在settings.py中直接添加USER-AGENT='xxx'</p><p>但是我们想要添加多个User-Agent，每次随机获取一个可以利用Dowmloader "
            'Middleware来设置。</p><p>第一步将settings中的USER-AGENT=\'xxx\'修改成USER-AGENT=["xxx","xxxxx","xxxxxxx"]</p><p>第二步在middlewares.py中添加：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '287px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '27.02%;"></div>\n'
            '<div class="image-view" data-width="1062" data-height="287"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f" '
            'data-original-width="1062" data-original-height="287" '
            'data-original-format="image/png" '
            'data-original-filesize="19133"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>from_crawler():通过参数crawler可以拿到配置的信息，我们的User-Agent在配置文件里，所以我们需要获取到。</p><p>方法名不可以修改。</p><p>第三步在settings.py中添加：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '110px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '12.04%;"></div>\n'
            '<div class="image-view" data-width="914" data-height="110"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602" '
            'data-original-width="914" data-original-height="110" '
            'data-original-format="image/png" '
            'data-original-filesize="5150"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>将scrapy自带的UserAgentmiddleware的键值设置为None,</p><p>自定义的设置为400，这个键值越小表示优先调用的意思。</p><p><b>四、Item '
            'Pipeline的使用。</b></p><p><b>1、进行数据的清洗</b></p><p>在一的实例中我们把评分小于等于8.5分的score修改为（不好看！），我们认为是不好看的电影，我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 678px; max-height: '
            '306px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '45.129999999999995%;"></div>\n'
            '<div class="image-view" data-width="678" data-height="306"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-23021209da207b71" '
            'data-original-width="678" data-original-height="306" '
            'data-original-format="image/png" '
            'data-original-filesize="7034"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 580px; max-height: '
            '78px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '13.450000000000001%;"></div>\n'
            '<div class="image-view" data-width="580" data-height="78"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c" '
            'data-original-width="580" data-original-height="78" '
            'data-original-format="image/png" '
            'data-original-filesize="5871"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>我们执行一下：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '460px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '44.7%;"></div>\n'
            '<div class="image-view" data-width="1029" data-height="460"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c" '
            'data-original-width="1029" data-original-height="460" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="91210"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p><b>2、储存</b></p><p><b>2.1储存为json格式</b></p><p>我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '564px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '65.58%;"></div>\n'
            '<div class="image-view" data-width="860" data-height="564"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96" '
            'data-original-width="860" data-original-height="564" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="76827"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 616px; max-height: '
            '100px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '16.23%;"></div>\n'
            '<div class="image-view" data-width="616" data-height="100"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93" '
            'data-original-width="616" data-original-height="100" '
            'data-original-format="image/png" '
            'data-original-filesize="7382"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>表示先执行TextPipeline方法，再执行JsonPipeline方法，先清洗，再储存。</p><p><b>2.2储存在mysql数据库</b></p><p>首先在mysql数据库中创建一个数据库maoyanreying，创建一个表maoyan。</p><p>我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '634px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '62.46000000000001%;"></div>\n'
            '<div class="image-view" data-width="1015" data-height="634"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3" '
            'data-original-width="1015" data-original-height="634" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="102252"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 566px; max-height: '
            '218px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '38.519999999999996%;"></div>\n'
            '<div class="image-view" data-width="566" data-height="218"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198" '
            'data-original-width="566" data-original-height="218" '
            'data-original-format="image/png" '
            'data-original-filesize="5142"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>即可</p><p>完。</p></article><div></div><div class="_19DgIp" '
            'style="margin-top:24px;margin-bottom:24px"></div></section>',
 'image_srcs': ['//upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7',
                '//upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2',
                '//upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852',
                '//upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2',
                '//upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f',
                '//upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602',
                '//upload-images.jianshu.io/upload_images/9489193-23021209da207b71',
                '//upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c',
                '//upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c',
                '//upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96',
                '//upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93',
                '//upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3',
                '//upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198'],
 'image_urls': ['https://upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7',
                'https://upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2',
                'https://upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852',
                'https://upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2',
                'https://upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f',
                'https://upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602',
                'https://upload-images.jianshu.io/upload_images/9489193-23021209da207b71',
                'https://upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c',
                'https://upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c',
                'https://upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96',
                'https://upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93',
                'https://upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3',
                'https://upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198'],
 'title': '爬虫框架Scrapy的安装与基本使用',
 'url': 'https://www.jianshu.com/p/6bc5a4641629'}
2019-10-30 11:12:35 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-30 11:12:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 600,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 39364,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 1.087391,
 'file_count': 13,
 'file_status_count/uptodate': 13,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 30, 3, 12, 35, 509318),
 'item_scraped_count': 1,
 'log_count/DEBUG': 16,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 10, 30, 3, 12, 34, 421927)}
2019-10-30 11:12:35 [scrapy.core.engine] INFO: Spider closed (finished)
2019-10-30 11:14:44 [scrapy.utils.log] INFO: Scrapy 1.7.4 started (bot: MyScrapy_jianshu)
2019-10-30 11:14:44 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Windows-10-10.0.17763-SP0
2019-10-30 11:14:44 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'MyScrapy_jianshu', 'LOG_FILE': 'scrapy.log', 'NEWSPIDER_MODULE': 'MyScrapy_jianshu.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['MyScrapy_jianshu.spiders']}
2019-10-30 11:14:44 [scrapy.extensions.telnet] INFO: Telnet Password: 3dcdc389b9c4c936
2019-10-30 11:14:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-10-30 11:14:45 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-10-30 11:14:45 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-10-30 11:14:45 [scrapy.middleware] INFO: Enabled item pipelines:
['MyScrapy_jianshu.pipelines.MyscrapyJianshuPipeline',
 'MyScrapy_jianshu.pipelines.MyImagesPipeline']
2019-10-30 11:14:45 [scrapy.core.engine] INFO: Spider opened
2019-10-30 11:14:45 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-10-30 11:14:45 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-10-30 11:14:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/robots.txt> (referer: None)
2019-10-30 11:14:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.jianshu.com/p/6bc5a4641629> (referer: None)
2019-10-30 11:14:45 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7> referred in <None>
2019-10-30 11:14:45 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2> referred in <None>
2019-10-30 11:14:45 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852> referred in <None>
2019-10-30 11:14:45 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2> referred in <None>
2019-10-30 11:14:45 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f> referred in <None>
2019-10-30 11:14:45 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602> referred in <None>
2019-10-30 11:14:45 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-23021209da207b71> referred in <None>
2019-10-30 11:14:45 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c> referred in <None>
2019-10-30 11:14:45 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c> referred in <None>
2019-10-30 11:14:45 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96> referred in <None>
2019-10-30 11:14:45 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93> referred in <None>
2019-10-30 11:14:45 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3> referred in <None>
2019-10-30 11:14:45 [scrapy.pipelines.files] DEBUG: File (uptodate): Downloaded image from <GET https://upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198> referred in <None>
2019-10-30 11:14:45 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.jianshu.com/p/6bc5a4641629>
{'content': '<section class="ouvJEz"><h1 '
            'class="_1RuRku">爬虫框架Scrapy的安装与基本使用</h1><div class="rEsl9f"><div '
            'class="s-dsoj"><span class="_3tCVn5"><i aria-label="ic-diamond" '
            'class="anticon"><svg width="1em" height="1em" fill="currentColor" '
            'aria-hidden="true" focusable="false" class=""><use '
            'xlink:href="#ic-diamond"></use></svg></i><span>1</span></span><time '
            'datetime="2018-07-20T04:25:56.000Z">2018.07.20 '
            '12:25:56</time><span>字数 2216</span><span>阅读 '
            '38673</span></div></div><article '
            'class="_2rhmJa"><blockquote><p><b>概括：上一节学习了pyspider框架，这一节我们来看一下Scrapy的强大之处。</b><b>他应该是目前python使用的最广泛的爬虫框架。</b></p></blockquote><p><b>一、简单实例，了解基本。</b></p><p><b>1、安装Scrapy框架</b></p><p>这里如果直接pip3 '
            'install scrapy可能会出错。</p><p>所以你可以先安装lxml：pip3 install '
            'lxml(已安装请忽略)。</p><p>安装pyOpenSSL：在官网下载wheel文件。</p><p>安装Twisted：在官网下载wheel文件。</p><p>安装PyWin32：在官网下载wheel文件。</p><p>下载地址：https://www.lfd.uci.edu/~gohlke/pythonlibs/</p><p>配置环境变量：将scrapy所在目录添加到系统环境变量即可。</p><p>ctrl+f搜索即可。</p><p>最后安装scrapy，pip3 '
            'install '
            'scrapy</p><p><b>2、创建一个scrapy项目</b></p><p>新创建一个目录，按住shift-右键-在此处打开命令窗口</p><p>输入：scrapy '
            'startproject '
            'tutorial即可创建一个tutorial文件夹</p><p>文件夹目录如下：</p><blockquote>\n'
            '<p>|-tutorial</p>\n'
            '<p>|-scrapy.cfg</p>\n'
            '<p>\xa0 |-__init__.py</p>\n'
            '<p>\xa0 |-items.py</p>\n'
            '<p>\xa0 |-middlewares.py</p>\n'
            '<p>\xa0 |-pipelines.py</p>\n'
            '<p>\xa0 |-settings.py</p>\n'
            '<p>\xa0 |-spiders</p>\n'
            '<p>\xa0 \xa0 |-__init__.py</p>\n'
            '</blockquote><p><b>文件的功能：</b></p><p>scrapy.cfg：配置文件</p><p>spiders：存放你Spider文件，也就是你爬取的py文件</p><p>items.py：相当于一个容器，和字典较像</p><p>middlewares.py：定义Downloader '
            'Middlewares(下载器中间件)和Spider '
            'Middlewares(蜘蛛中间件)的实现</p><p>pipelines.py:定义Item '
            'Pipeline的实现，实现数据的清洗，储存，验证。</p><p>settings.py：全局配置</p><p><b>3、创建一个spider（自己定义的爬虫文件）</b></p><p>例如以爬取猫眼热映口碑榜为例子来了解一下：</p><p>在spiders文件夹下创建一个maoyan.py文件，你也可以按住shift-右键-在此处打开命令窗口，输入：scrapy '
            'genspider 文件名 '
            '要爬取的网址。</p><p>自己创建的需要自己写，使用命令创建的包含最基本的东西。</p><p>我们来看一下使用命令创建的有什么。</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 573px; max-height: '
            '264px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '46.07%;"></div>\n'
            '<div class="image-view" data-width="573" data-height="264"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7" '
            'data-original-width="573" data-original-height="264" '
            'data-original-format="image/png" '
            'data-original-filesize="10489"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>介绍一下这些是干嘛的：</p><p>name：是项目的名字</p><p>allowed_domains：是允许爬取的域名，比如一些网站有相关链接，域名就和本网站不同，这些就会忽略。</p><p>atart_urls：是Spider爬取的网站，定义初始的请求url，可以多个。</p><p>parse方法：是Spider的一个方法，在请求start_url后，之后的方法，这个方法是对网页的解析，与提取自己想要的东西。</p><p>response参数：是请求网页后返回的内容，也就是你需要解析的网页。</p><p>还有其他参数有兴趣可以去查查。</p><p><b>4、定义Item</b></p><p>item是保存爬取数据的容器，使用的方法和字典差不多。</p><p>我们打开items.py，之后我们想要提取的信息有：</p><p>index(排名)、title(电影名)、star(主演)、releasetime(上映时间)、score(评分)</p><p>于是我们将items.py文件修改成这样。</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '226px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '29.82%;"></div>\n'
            '<div class="image-view" data-width="758" data-height="226"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2" '
            'data-original-width="758" data-original-height="226" '
            'data-original-format="image/png" '
            'data-original-filesize="12301"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>即可。</p><p><b>5、再次打开spider来提取我们想要的信息</b></p><p>修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '399px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '36.94%;"></div>\n'
            '<div class="image-view" data-width="1080" data-height="399"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852" '
            'data-original-width="1080" data-original-height="399" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="53873"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>好了，一个简单的爬虫就写完了。</p><p><b>6、运行</b></p><p>在该文件夹下，按住shift-右键-在此处打开命令窗口，输入：scrapy '
            'crawl maoyan(项目的名字)</p><p>即可看到：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '580px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '53.7%;"></div>\n'
            '<div class="image-view" data-width="1080" data-height="580"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2" '
            'data-original-width="1080" data-original-height="580" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="107723"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p><b>7、保存</b></p><p>我们只运行了代码，看看有没有报错，并没有保存。</p><p>如果我们想保存为csv、xml、json格式，可以直接使用命令：</p><p>在该文件夹下，按住shift-右键-在此处打开命令窗口，输入：</p><p>scrapy '
            'crawl maoyan -o maoyan.csv</p><p>scrapy crawl maoyan -o '
            'maoyan.xml</p><p>scrapy crawl maoyan -o '
            "maoyan.json</p><p>选择其中一个即可。当然如果想要保存为其他格式也是可以的，这里只说常见的。这里选择json格式，运行后会发现，在文件夹下多出来一个maoyan.json的文件。打开之后发现，中文都是一串乱码，这里需要修改编码方式，当然也可以在配置里修改</p><p>（在settings.py文件中添加FEED_EXPORT_ENCODING='UTF8'即可），</p><p>如果想直接在命令行中修改：</p><p>scrapy "
            'crawl maoyan -o maoyan.json -s '
            'FEED_EXPORT_ENCODING=UTF8</p><p>即可。</p><p>这里自己试试效果吧。</p><p>当然我们保存也可以在运行的时候自动保存，不需要自己写命令。后面介绍（我们还有还多文件没有用到呦）。</p><p><b>二、scrapy如何解析？</b></p><p>之前写过一篇文章：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483766&amp;idx=1&amp;sn=86d61115ebb7a4083e17a54f1acffdf1&amp;chksm=fb74c947cc03405122d74b1172a9a96ef35af65753666f3f7900a9318290c4326736c2d6e69e&amp;scene=21#wechat_redirect" '
            'target="_blank" '
            'rel="nofollow">三大解析库的使用</a></p><p>但是scrapy也提供了自己的解析方式（Selector），和上面的也很相似，我们来看一下：</p><p><b>1、css</b></p><p>首先需要导入模块：from '
            'scrapy import '
            "Selector</p><p>例如有这样一段html代码：</p><p>html='&lt;html&gt;&lt;head&gt;&lt;title&gt;Demo&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;div "
            "class='cla'&gt;This is "
            'Demo&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;</p><p><b>1.1、首先需要构建一个Selector对象</b></p><p>sel '
            '= Selector(html)</p><p>text = '
            "sel.css('.cla::text').extract_first()</p><p>.cla表示选中上面的div节点，::text表示获取文本，这里和以前的有所不同。</p><p>extract_first()表示返回第一个元素，因为上述 "
            'sel.css(\'.cla::text\')返回的是一个列表，你也可以写成sel.css(\'.cla::text\')[0]来获取第一个元素，但是如果为空，就会报出超出最大索引的错误，不建议这样写，而使用extract_first()就不会报错，同时如果写成extract_first(\'123\')这样，如果为空就返回123</p><p><b>1.2、</b>有了选取第一个，就有选取所有：extract()表示选取所有，如果返回的是多个值，就可以是这样写。</p><p><b>1.3、</b>获取属性就是sel.css(\'.cla::sttr(\'class\')\').extract_first()表示获取class</p><p><b>1.4、</b>获取指定属性的文本：sel.css(\'div[class="cla"]::text\')</p><p><b>1.5、</b>其他写法和css的写法如出一辙。</p><p><b>1.6、</b>在scrapy中为我们提供了一个简便的写法，在上述的简单实例中，我们知道了response为请求网页的返回值。</p><p>我们可以直接写成：response.css()来解析，提取我们想要的信息。同样，下面要说的XPath也可以直接写成：</p><p>response.xpath()来解析。</p><p><b>2、Xpath</b></p><p>Xpath的使用可以看上面的文章：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483766&amp;idx=1&amp;sn=86d61115ebb7a4083e17a54f1acffdf1&amp;chksm=fb74c947cc03405122d74b1172a9a96ef35af65753666f3f7900a9318290c4326736c2d6e69e&amp;scene=21#wechat_redirect" '
            'target="_blank" '
            'rel="nofollow">三大解析库的使用</a></p><p>注意：获取的还是列表，所以还是要加上extract_first()或者extract()</p><p><b>3、正则匹配(这里用response操作)</b></p><p>例如：response.css(\'a::text\').re(\'写正则\')</p><p>这里如果response.css(\'a::text\')匹配的是多个对象，那么加上正则也是匹配符合要求的多个对象。</p><p>这里如果想要匹配第一个对象，可以把re()修改成re_first()即可。</p><p><b>注意：response不可以直接调用re(),response.xpath(\'.\').re()可以相当于达到直接使用正则的效果</b>。</p><p>正则的使用：<a '
            'href="http://mp.weixin.qq.com/s?__biz=MzU0NDg3NDg0Ng==&amp;mid=2247483727&amp;idx=1&amp;sn=1fc19af2291177ac3c1a832c47392a3f&amp;chksm=fb74c97ecc0340681bc54c97ceb7bba413da3f1ca6fc3a107bbdabe7eb9be144c261a6de347d&amp;scene=21#wechat_redirect" '
            'target="_blank" rel="nofollow">万能的正则表达式</a></p><p><b>三、Dowmloader '
            'Middleware的使用</b></p><p>本身scrapy就提供了很多Dowmloader '
            "Middleware，但是有时候我们要修改，</p><p>比如修改User-Agent，使用代理ip等。</p><p>以修改User-Agent为例（设置代理ip大同小异）：</p><p>第一种方法，可以在settings.py中直接添加USER-AGENT='xxx'</p><p>但是我们想要添加多个User-Agent，每次随机获取一个可以利用Dowmloader "
            'Middleware来设置。</p><p>第一步将settings中的USER-AGENT=\'xxx\'修改成USER-AGENT=["xxx","xxxxx","xxxxxxx"]</p><p>第二步在middlewares.py中添加：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '287px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '27.02%;"></div>\n'
            '<div class="image-view" data-width="1062" data-height="287"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f" '
            'data-original-width="1062" data-original-height="287" '
            'data-original-format="image/png" '
            'data-original-filesize="19133"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>from_crawler():通过参数crawler可以拿到配置的信息，我们的User-Agent在配置文件里，所以我们需要获取到。</p><p>方法名不可以修改。</p><p>第三步在settings.py中添加：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '110px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '12.04%;"></div>\n'
            '<div class="image-view" data-width="914" data-height="110"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602" '
            'data-original-width="914" data-original-height="110" '
            'data-original-format="image/png" '
            'data-original-filesize="5150"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>将scrapy自带的UserAgentmiddleware的键值设置为None,</p><p>自定义的设置为400，这个键值越小表示优先调用的意思。</p><p><b>四、Item '
            'Pipeline的使用。</b></p><p><b>1、进行数据的清洗</b></p><p>在一的实例中我们把评分小于等于8.5分的score修改为（不好看！），我们认为是不好看的电影，我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 678px; max-height: '
            '306px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '45.129999999999995%;"></div>\n'
            '<div class="image-view" data-width="678" data-height="306"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-23021209da207b71" '
            'data-original-width="678" data-original-height="306" '
            'data-original-format="image/png" '
            'data-original-filesize="7034"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 580px; max-height: '
            '78px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '13.450000000000001%;"></div>\n'
            '<div class="image-view" data-width="580" data-height="78"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c" '
            'data-original-width="580" data-original-height="78" '
            'data-original-format="image/png" '
            'data-original-filesize="5871"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>我们执行一下：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '460px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '44.7%;"></div>\n'
            '<div class="image-view" data-width="1029" data-height="460"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c" '
            'data-original-width="1029" data-original-height="460" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="91210"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p><b>2、储存</b></p><p><b>2.1储存为json格式</b></p><p>我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '564px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '65.58%;"></div>\n'
            '<div class="image-view" data-width="860" data-height="564"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96" '
            'data-original-width="860" data-original-height="564" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="76827"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 616px; max-height: '
            '100px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '16.23%;"></div>\n'
            '<div class="image-view" data-width="616" data-height="100"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93" '
            'data-original-width="616" data-original-height="100" '
            'data-original-format="image/png" '
            'data-original-filesize="7382"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>表示先执行TextPipeline方法，再执行JsonPipeline方法，先清洗，再储存。</p><p><b>2.2储存在mysql数据库</b></p><p>首先在mysql数据库中创建一个数据库maoyanreying，创建一个表maoyan。</p><p>我们将pipeline.py修改成这样：</p><div '
            'class="image-package">\n'
            '<div class="image-container" style="max-width: 700px; max-height: '
            '634px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '62.46000000000001%;"></div>\n'
            '<div class="image-view" data-width="1015" data-height="634"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3" '
            'data-original-width="1015" data-original-height="634" '
            'data-original-format="image/jpeg" '
            'data-original-filesize="102252"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>在setting.py中添加：</p><div class="image-package">\n'
            '<div class="image-container" style="max-width: 566px; max-height: '
            '218px;">\n'
            '<div class="image-container-fill" style="padding-bottom: '
            '38.519999999999996%;"></div>\n'
            '<div class="image-view" data-width="566" data-height="218"><img '
            'data-original-src="//upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198" '
            'data-original-width="566" data-original-height="218" '
            'data-original-format="image/png" '
            'data-original-filesize="5142"></div>\n'
            '</div>\n'
            '<div class="image-caption"></div>\n'
            '</div><p>即可</p><p>完。</p></article><div></div><div class="_19DgIp" '
            'style="margin-top:24px;margin-bottom:24px"></div></section>',
 'image_srcs': ['//upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7',
                '//upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2',
                '//upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852',
                '//upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2',
                '//upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f',
                '//upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602',
                '//upload-images.jianshu.io/upload_images/9489193-23021209da207b71',
                '//upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c',
                '//upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c',
                '//upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96',
                '//upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93',
                '//upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3',
                '//upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198'],
 'image_urls': ['https://upload-images.jianshu.io/upload_images/9489193-ae44463d0089e8f7',
                'https://upload-images.jianshu.io/upload_images/9489193-a1274105bfabadc2',
                'https://upload-images.jianshu.io/upload_images/9489193-928f83ef108b1852',
                'https://upload-images.jianshu.io/upload_images/9489193-3f35f7c23e305aa2',
                'https://upload-images.jianshu.io/upload_images/9489193-5a794618db65a28f',
                'https://upload-images.jianshu.io/upload_images/9489193-3d5124dfd31e7602',
                'https://upload-images.jianshu.io/upload_images/9489193-23021209da207b71',
                'https://upload-images.jianshu.io/upload_images/9489193-0cf21c13cd24635c',
                'https://upload-images.jianshu.io/upload_images/9489193-dbefa5bdb28f345c',
                'https://upload-images.jianshu.io/upload_images/9489193-292fe00da4028b96',
                'https://upload-images.jianshu.io/upload_images/9489193-f1b5f7a2ecd95f93',
                'https://upload-images.jianshu.io/upload_images/9489193-cc05d60fcd6dfab3',
                'https://upload-images.jianshu.io/upload_images/9489193-eaa620dfc9bd8198'],
 'title': '爬虫框架Scrapy的安装与基本使用',
 'url': 'https://www.jianshu.com/p/6bc5a4641629'}
2019-10-30 11:14:45 [scrapy.core.engine] INFO: Closing spider (finished)
2019-10-30 11:14:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 600,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 39364,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 0.817233,
 'file_count': 13,
 'file_status_count/uptodate': 13,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 10, 30, 3, 14, 45, 890430),
 'item_scraped_count': 1,
 'log_count/DEBUG': 16,
 'log_count/INFO': 10,
 'response_received_count': 2,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 10, 30, 3, 14, 45, 73197)}
2019-10-30 11:14:45 [scrapy.core.engine] INFO: Spider closed (finished)
